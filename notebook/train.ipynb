{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\software\\Anaconda3\\envs\\tf2.0\\lib\\site-packages\\ipykernel\\parentpoller.py:116: UserWarning: Parent poll failed.  If the frontend dies,\n",
      "                the kernel may be left running.  Please let us know\n",
      "                about your system (bitness, Python, etc.) at\n",
      "                ipython-dev@scipy.org\n",
      "  ipython-dev@scipy.org\"\"\")\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import tensorflow as tf\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from pgn import PGN\n",
    "from loss import coverage_loss\n",
    "from batcher import batcher\n",
    "from utils.config import CKPT_DIR\n",
    "from utils.saveLoader import Vocab\n",
    "from utils.params import get_default_params\n",
    "from utils.config_gpu import config_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "config_gpu()\n",
    "params = get_default_params()\n",
    "model = PGN(params)\n",
    "vocab = Vocab(params[\"vocab_path\"], params[\"vocab_size\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "配置保存点管理器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restored from E:\\GitHub\\QA-abstract-and-reasoning\\data\\checkpoints\\training_checkpoints\\test_model-1\n"
     ]
    }
   ],
   "source": [
    "checkpoint = tf.train.Checkpoint(Seq2Seq=model)\n",
    "checkpoint_manager = tf.train.CheckpointManager(checkpoint, CKPT_DIR, max_to_keep=5)\n",
    "checkpoint.restore(checkpoint_manager.latest_checkpoint)\n",
    "if checkpoint_manager.latest_checkpoint:\n",
    "    print(\"Restored from {}\".format(checkpoint_manager.latest_checkpoint))\n",
    "else:\n",
    "    print(\"Initializing from scratch.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.train_helper import train_model\n",
    "# train_model(model, vocab, params, checkpoint_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n",
      "Building vocab ...\n",
      "Building the model ...\n",
      "Creating the checkpoint manager\n",
      "Restored from E:\\GitHub\\QA-abstract-and-reasoning\\data\\checkpoints\\training_checkpoints\\test_model-1\n",
      "Starting the training ...\n",
      "Epoch 1 Batch 1 Loss 235.3908\n",
      "Epoch 1 Batch 2 Loss 217.1442\n",
      "Epoch 1 Batch 3 Loss 297.5134\n",
      "Epoch 1 Batch 4 Loss 310.4384\n",
      "Epoch 1 Batch 5 Loss 354.3774\n",
      "Epoch 1 Batch 6 Loss 235.2203\n",
      "Epoch 1 Batch 7 Loss 271.4305\n",
      "Epoch 1 Batch 8 Loss 343.9371\n",
      "Epoch 1 Batch 9 Loss 235.1501\n",
      "Epoch 1 Batch 10 Loss 299.7896\n",
      "Epoch 1 Batch 11 Loss 307.5605\n",
      "my break\n"
     ]
    }
   ],
   "source": [
    "%run ../utils/train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20736"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params['max_train_steps']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:\\\\GitHub\\\\QA-abstract-and-reasoning\\\\data\\\\checkpoints\\\\training_checkpoints\\\\test_model-1'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_prefix = os.path.join(CKPT_DIR, \"test_model\")\n",
    "checkpoint.save(file_prefix=checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一探train_model究竟\n",
    "\n",
    "- 摒弃`@tf.funtion`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = params['epochs']\n",
    "optimizer = tf.keras.optimizers.Adagrad(params['learning_rate'],\n",
    "                                            initial_accumulator_value=params['adagrad_init_acc'],\n",
    "                                            clipnorm=params['max_grad_norm'])\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "# optimizer.get_config(), loss_object.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred, padding_mask):\n",
    "    loss = 0\n",
    "    for t in range(real.shape[1]):\n",
    "        # if padding_mask:\n",
    "        try:\n",
    "            loss_ = loss_object(real[:, t], pred[:, t, :])\n",
    "            mask = tf.cast(padding_mask[:, t], dtype=loss_.dtype)\n",
    "            loss_ *= mask\n",
    "            loss_ = tf.reduce_mean(loss_, axis=0)  # batch-wise\n",
    "            loss += loss_\n",
    "        # else:\n",
    "        except:\n",
    "            loss_ = loss_object(real[:, t], pred[:, t, :])\n",
    "            loss_ = tf.reduce_mean(loss_, axis=0)  # batch-wise\n",
    "            loss += loss_\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.funtion\n",
    "def train_step(enc_inp, extended_enc_input, max_oov_len,\n",
    "               dec_input, dec_target, cov_loss_wt,\n",
    "               enc_pad_mask, padding_mask=None):\n",
    "    batch_loss = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = model.call_encoder(enc_inp)\n",
    "        # 第一个隐藏层输入\n",
    "        dec_hidden = enc_hidden\n",
    "        # 逐个预测序列\n",
    "        predictions, _, attentions, coverages = model(dec_input,\n",
    "                                                      dec_hidden,\n",
    "                                                      enc_output,\n",
    "                                                      dec_target,\n",
    "                                                      extended_enc_input,\n",
    "                                                      max_oov_len,\n",
    "                                                      enc_pad_mask=enc_pad_mask,\n",
    "                                                      use_coverage=True,\n",
    "                                                      prev_coverage=None)\n",
    "    \n",
    "        batch_loss = loss_function(dec_target, predictions, padding_mask) + \\\n",
    "                     cov_loss_wt * coverage_loss(attentions, coverages, padding_mask)\n",
    "        \n",
    "#     variables = model.encoder.trainable_variables + model.decoder.trainable_variables + \\\n",
    "#                 model.attention.trainable_variables + model.pointer.trainable_variables\n",
    "    variables = model.trainable_variables\n",
    "    gradients = tape.gradient(batch_loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "dataset = batcher(vocab, params)\n",
    "total_loss = 0\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进入for 循环\n",
    "# for encoder_batch_data, decoder_batch_data in dataset:\n",
    "encoder_batch_data, decoder_batch_data = next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_loss = train_step(enc_inp = encoder_batch_data[\"enc_input\"],\n",
    "                        extended_enc_input = encoder_batch_data[\"extended_enc_input\"],\n",
    "                        max_oov_len = encoder_batch_data[\"max_oov_len\"],\n",
    "                        dec_input = decoder_batch_data[\"dec_input\"],\n",
    "                        dec_target = decoder_batch_data[\"dec_target\"],\n",
    "                        cov_loss_wt=0.5,\n",
    "                        enc_pad_mask=encoder_batch_data[\"sample_encoder_pad_mask\"],\n",
    "                        padding_mask=decoder_batch_data[\"sample_decoder_pad_mask\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "进入train_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf2.0]",
   "language": "python",
   "name": "conda-env-tf2.0-py"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
