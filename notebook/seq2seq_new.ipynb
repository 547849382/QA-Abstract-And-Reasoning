{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import sys\n",
    "sys.path.append('../')  # 返回notebook的上一级目录\n",
    "# sys.path.append('E:\\GitHub\\QA-abstract-and-reasoning')  # 效果同上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no colab\n"
     ]
    }
   ],
   "source": [
    "# 在google colab运行则执行以下代码\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive_path = '/content/drive'\n",
    "    working_path = drive_path + \"/My Drive/QA\" # 工作路径\n",
    "    drive.mount(drive_path)\n",
    "    os.chdir(working_path)\n",
    "    sys.path.append(working_path)  # 环境变量\n",
    "    print(\"current working directory: \", os.getcwd())\n",
    "    \n",
    "    # %tensorflow_version 仅存在于 Colab\n",
    "    %tensorflow_version 2.x\n",
    "    print(\"run notebook in colab\")\n",
    "except:\n",
    "    print(\"no colab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "np.set_printoptions(suppress=True)\n",
    "from utils.plot import plot_attention\n",
    "from utils.saveLoader import *\n",
    "from utils.config import *\n",
    "from layers import *\n",
    "from preprocess import Preprocess\n",
    "from gensim.models.word2vec import LineSentence, Word2Vec\n",
    "import tensorflow as tf\n",
    "# from model_layer import seq2seq_model\n",
    "import time\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[限制gpu内存增长](https://tensorflow.google.cn/guide/gpu#limiting_gpu_memory_growth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "from utils.config_gpu import config_gpu\n",
    "config_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x,train_y,test_x = load_train_dataset()  # 数据集\n",
    "vocab,vocab_reversed = load_vocab(VOCAB_PAD)  # vocab\n",
    "embedding_matrix = np.loadtxt(EMBEDDING_MATRIX_PAD)  # 预训练层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 设置参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'vocab_size': 32566, 'max_enc_len': 260, 'max_dec_len': 33, 'embed_size': 300, 'enc_units': 256, 'attn_units': 10, 'dec_units': 256, 'batch_size': 32, 'epochs': 2}\n"
     ]
    }
   ],
   "source": [
    "params = {}\n",
    "params[\"vocab_size\"] = len(vocab)\n",
    "params[\"max_enc_len\"] = train_x.shape[1]  # 260\n",
    "params[\"max_dec_len\"] = train_y.shape[1]  # 33\n",
    "params[\"embed_size\"] = embedding_matrix.shape[1]\n",
    "params[\"enc_units\"] = 256\n",
    "params[\"attn_units\"] = 10\n",
    "params[\"dec_units\"] = params[\"enc_units\"]\n",
    "params[\"batch_size\"] = 32\n",
    "params[\"epochs\"] = 2\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取部分数据进行训练\n",
    "# sample_num=64\n",
    "sample_num = train_x.shape[0]\n",
    "dataset = tf.data.Dataset.from_tensor_slices((train_x[:sample_num], train_y[:sample_num])).shuffle(params[\"batch_size\"]*2+1)\n",
    "dataset = dataset.batch(params[\"batch_size\"], drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = sample_num//params[\"batch_size\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seq2seq import *\n",
    "model=Seq2Seq(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存点设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.config import CKPT_DIR, CKPT_PREFIX\n",
    "from utils.saveLoader import del_all_files_of_dir\n",
    "ckpt = tf.train.Checkpoint(Seq2Seq=model)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, CKPT_DIR, max_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there no files in this path\n",
      "Model restored\n"
     ]
    }
   ],
   "source": [
    "del_all_files_of_dir(CKPT_DIR)\n",
    "ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "print(\"Model restored\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[SparseCategoricalCrossentropy](https://tensorflow.google.cn/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(name='Adam',learning_rate=0.001)\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "pad_index=vocab['<PAD>']\n",
    "unk_index=vocab['<UNK>']\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    pad_mask = tf.math.equal(real, pad_index)\n",
    "    unk_mask = tf.math.equal(real, unk_index)\n",
    "    # <PAD> 和 <UNK> 的损失都不算\n",
    "    mask = tf.math.logical_not(tf.math.logical_or(pad_mask,unk_mask))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_mean(loss_)\n",
    "    # return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 调试train_step()\n",
    "# inp, targ = next(iter(dataset))\n",
    "# pad_index=vocab['<PAD>']\n",
    "# unk_index=vocab['<UNK>']\n",
    "# enc_output, enc_hidden = model.call_encoder(inp)\n",
    "# dec_hidden = enc_hidden\n",
    "# dec_input = tf.expand_dims([vocab['<START>']] * params[\"batch_size\"], 1)\n",
    "# predictions, _ = model(dec_input, dec_hidden, enc_output, targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ):\n",
    "    pad_index=vocab['<PAD>']\n",
    "    unk_index=vocab['<UNK>']\n",
    "    loss = 0\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        # 1. 构建encoder\n",
    "        enc_output, enc_hidden = model.call_encoder(inp)\n",
    "        # 2. 复制\n",
    "        dec_hidden = enc_hidden\n",
    "        # 3. <START> * BATCH_SIZE \n",
    "        dec_input = tf.expand_dims([vocab['<START>']] * params[\"batch_size\"], 1)\n",
    "        \n",
    "        # 逐个预测序列\n",
    "        predictions, _ = model(dec_input, dec_hidden, enc_output, targ)\n",
    "        \n",
    "        batch_loss = loss_function(targ[:, 1:], predictions)\n",
    "\n",
    "        variables = model.encoder.trainable_variables + model.decoder.trainable_variables+ model.attention.trainable_variables\n",
    "    \n",
    "        gradients = tape.gradient(batch_loss, variables)\n",
    "\n",
    "        optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "        return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 5.3902\n",
      "Epoch 1 Batch 1 Loss 5.6638\n",
      "Epoch 1 Batch 2 Loss 5.2197\n",
      "Epoch 1 Batch 3 Loss 5.5543\n",
      "Epoch 1 Batch 4 Loss 5.8169\n",
      "Epoch 1 Batch 5 Loss 4.5870\n",
      "Epoch 1 Batch 6 Loss 5.0880\n",
      "Epoch 1 Batch 7 Loss 4.8601\n",
      "Epoch 1 Batch 8 Loss 4.3980\n",
      "Epoch 1 Batch 9 Loss 3.2799\n",
      "Epoch 1 Batch 10 Loss 5.2775\n",
      "Epoch 1 Batch 11 Loss 3.6353\n",
      "Epoch 1 Batch 12 Loss 2.7170\n",
      "Epoch 1 Batch 13 Loss 2.6421\n",
      "Epoch 1 Batch 14 Loss 2.1167\n",
      "Epoch 1 Batch 15 Loss 2.9356\n",
      "Epoch 1 Batch 16 Loss 2.4914\n",
      "Epoch 1 Batch 17 Loss 2.3527\n",
      "Epoch 1 Batch 18 Loss 2.4934\n",
      "Epoch 1 Batch 19 Loss 3.1829\n",
      "Epoch 1 Batch 20 Loss 3.3698\n",
      "Epoch 1 Batch 21 Loss 3.2792\n",
      "Epoch 1 Batch 22 Loss 3.1758\n",
      "Epoch 1 Batch 23 Loss 3.3565\n",
      "Epoch 1 Batch 24 Loss 2.8799\n",
      "Epoch 1 Batch 25 Loss 2.5533\n",
      "Epoch 1 Batch 26 Loss 2.5327\n",
      "Epoch 1 Batch 27 Loss 3.1751\n",
      "Epoch 1 Batch 28 Loss 3.1140\n",
      "Epoch 1 Batch 29 Loss 2.9614\n",
      "Epoch 1 Batch 30 Loss 3.0480\n",
      "Epoch 1 Batch 31 Loss 3.4621\n",
      "Epoch 1 Batch 32 Loss 3.3694\n",
      "Epoch 1 Batch 33 Loss 2.5887\n",
      "Epoch 1 Batch 34 Loss 1.7413\n",
      "Epoch 1 Batch 35 Loss 2.1052\n",
      "Epoch 1 Batch 36 Loss 2.1231\n",
      "Epoch 1 Batch 37 Loss 1.6203\n",
      "Epoch 1 Batch 38 Loss 1.9480\n",
      "Epoch 1 Batch 39 Loss 2.0578\n",
      "Epoch 1 Batch 40 Loss 2.0394\n",
      "Epoch 1 Batch 41 Loss 2.6192\n",
      "Epoch 1 Batch 42 Loss 2.0071\n",
      "Epoch 1 Batch 43 Loss 2.1149\n",
      "Epoch 1 Batch 44 Loss 2.5986\n",
      "Epoch 1 Batch 45 Loss 2.8199\n",
      "Epoch 1 Batch 46 Loss 3.2689\n",
      "Epoch 1 Batch 47 Loss 2.7101\n",
      "Epoch 1 Batch 48 Loss 2.4153\n",
      "Epoch 1 Batch 49 Loss 2.4914\n",
      "Epoch 1 Batch 50 Loss 2.6334\n",
      "Epoch 1 Batch 51 Loss 2.6871\n",
      "Epoch 1 Batch 52 Loss 3.0354\n",
      "Epoch 1 Batch 53 Loss 2.7590\n",
      "Epoch 1 Batch 54 Loss 3.3114\n",
      "Epoch 1 Batch 55 Loss 3.1814\n",
      "Epoch 1 Batch 56 Loss 3.7608\n",
      "Epoch 1 Batch 57 Loss 3.8574\n",
      "Epoch 1 Batch 58 Loss 4.1828\n",
      "Epoch 1 Batch 59 Loss 3.0734\n",
      "Epoch 1 Batch 60 Loss 2.7548\n",
      "Epoch 1 Batch 61 Loss 2.6958\n",
      "Epoch 1 Batch 62 Loss 2.7401\n",
      "Epoch 1 Batch 63 Loss 2.9483\n",
      "Epoch 1 Batch 64 Loss 2.3175\n",
      "Epoch 1 Batch 65 Loss 2.4642\n",
      "Epoch 1 Batch 66 Loss 2.5425\n",
      "Epoch 1 Batch 67 Loss 2.9211\n",
      "Epoch 1 Batch 68 Loss 2.5065\n",
      "Epoch 1 Batch 69 Loss 2.9689\n",
      "Epoch 1 Batch 70 Loss 2.7883\n",
      "Epoch 1 Batch 71 Loss 3.0603\n",
      "Epoch 1 Batch 72 Loss 2.8202\n",
      "Epoch 1 Batch 73 Loss 3.4948\n",
      "Epoch 1 Batch 74 Loss 3.5883\n",
      "Epoch 1 Batch 75 Loss 3.4207\n",
      "Epoch 1 Batch 76 Loss 3.3605\n",
      "Epoch 1 Batch 77 Loss 2.8247\n",
      "Epoch 1 Batch 78 Loss 3.2950\n",
      "Epoch 1 Batch 79 Loss 3.2665\n",
      "Epoch 1 Batch 80 Loss 3.0373\n",
      "Epoch 1 Batch 81 Loss 2.8295\n",
      "Epoch 1 Batch 82 Loss 2.5969\n",
      "Epoch 1 Batch 83 Loss 2.5897\n",
      "Epoch 1 Batch 84 Loss 3.2455\n",
      "Epoch 1 Batch 85 Loss 3.2307\n",
      "Epoch 1 Batch 86 Loss 3.4268\n",
      "Epoch 1 Batch 87 Loss 3.2165\n",
      "Epoch 1 Batch 88 Loss 2.7619\n",
      "Epoch 1 Batch 89 Loss 3.0768\n",
      "Epoch 1 Batch 90 Loss 3.0649\n",
      "Epoch 1 Batch 91 Loss 2.4568\n",
      "Epoch 1 Batch 92 Loss 3.0979\n",
      "Epoch 1 Batch 93 Loss 2.1221\n",
      "Epoch 1 Batch 94 Loss 2.0194\n",
      "Epoch 1 Batch 95 Loss 1.7367\n",
      "Epoch 1 Batch 96 Loss 2.3849\n",
      "Epoch 1 Batch 97 Loss 2.8051\n",
      "Epoch 1 Batch 98 Loss 3.2872\n",
      "Epoch 1 Batch 99 Loss 3.8254\n",
      "Epoch 1 Batch 100 Loss 4.2381\n",
      "Epoch 1 Batch 101 Loss 3.7809\n",
      "Epoch 1 Batch 102 Loss 2.9281\n",
      "Epoch 1 Batch 103 Loss 2.8255\n",
      "Epoch 1 Batch 104 Loss 2.8410\n",
      "Epoch 1 Batch 105 Loss 3.3247\n",
      "Epoch 1 Batch 106 Loss 2.5788\n",
      "Epoch 1 Batch 107 Loss 3.4830\n",
      "Epoch 1 Batch 108 Loss 3.5723\n",
      "Epoch 1 Batch 109 Loss 2.7296\n",
      "Epoch 1 Batch 110 Loss 2.8703\n",
      "Epoch 1 Batch 111 Loss 2.0184\n",
      "Epoch 1 Batch 112 Loss 2.4584\n",
      "Epoch 1 Batch 113 Loss 2.6152\n",
      "Epoch 1 Batch 114 Loss 2.6472\n",
      "Epoch 1 Batch 115 Loss 2.7472\n",
      "Epoch 1 Batch 116 Loss 2.8306\n",
      "Epoch 1 Batch 117 Loss 2.8870\n",
      "Epoch 1 Batch 118 Loss 2.8602\n",
      "Epoch 1 Batch 119 Loss 2.3996\n",
      "Epoch 1 Batch 120 Loss 2.5895\n",
      "Epoch 1 Batch 121 Loss 3.1986\n",
      "Epoch 1 Batch 122 Loss 2.5711\n",
      "Epoch 1 Batch 123 Loss 2.5145\n",
      "Epoch 1 Batch 124 Loss 2.2236\n",
      "Epoch 1 Batch 125 Loss 1.9991\n",
      "Epoch 1 Batch 126 Loss 2.2327\n",
      "Epoch 1 Batch 127 Loss 2.4315\n",
      "Epoch 1 Batch 128 Loss 1.8755\n",
      "Epoch 1 Batch 129 Loss 2.4874\n",
      "Epoch 1 Batch 130 Loss 2.9522\n",
      "Epoch 1 Batch 131 Loss 2.9418\n",
      "Epoch 1 Batch 132 Loss 2.6236\n",
      "Epoch 1 Batch 133 Loss 3.2565\n",
      "Epoch 1 Batch 134 Loss 3.2114\n",
      "Epoch 1 Batch 135 Loss 2.7034\n",
      "Epoch 1 Batch 136 Loss 2.5936\n",
      "Epoch 1 Batch 137 Loss 2.8020\n",
      "Epoch 1 Batch 138 Loss 2.9648\n",
      "Epoch 1 Batch 139 Loss 2.8406\n",
      "Epoch 1 Batch 140 Loss 2.5036\n",
      "Epoch 1 Batch 141 Loss 2.2758\n",
      "Epoch 1 Batch 142 Loss 2.7403\n",
      "Epoch 1 Batch 143 Loss 2.3130\n",
      "Epoch 1 Batch 144 Loss 2.7424\n",
      "Epoch 1 Batch 145 Loss 2.8741\n",
      "Epoch 1 Batch 146 Loss 2.7752\n",
      "Epoch 1 Batch 147 Loss 3.1695\n",
      "Epoch 1 Batch 148 Loss 3.2862\n",
      "Epoch 1 Batch 149 Loss 3.5620\n",
      "Epoch 1 Batch 150 Loss 2.2938\n",
      "Epoch 1 Batch 151 Loss 2.7315\n",
      "Epoch 1 Batch 152 Loss 3.2000\n",
      "Epoch 1 Batch 153 Loss 3.0762\n",
      "Epoch 1 Batch 154 Loss 2.1902\n",
      "Epoch 1 Batch 155 Loss 2.8025\n",
      "Epoch 1 Batch 156 Loss 3.3217\n",
      "Epoch 1 Batch 157 Loss 3.7658\n",
      "Epoch 1 Batch 158 Loss 3.3751\n",
      "Epoch 1 Batch 159 Loss 3.3136\n",
      "Epoch 1 Batch 160 Loss 2.9501\n",
      "Epoch 1 Batch 161 Loss 2.4670\n",
      "Epoch 1 Batch 162 Loss 2.4228\n",
      "Epoch 1 Batch 163 Loss 2.0666\n",
      "Epoch 1 Batch 164 Loss 1.7592\n",
      "Epoch 1 Batch 165 Loss 2.3776\n",
      "Epoch 1 Batch 166 Loss 3.1320\n",
      "Epoch 1 Batch 167 Loss 3.0078\n",
      "Epoch 1 Batch 168 Loss 2.6889\n",
      "Epoch 1 Batch 169 Loss 2.7223\n",
      "Epoch 1 Batch 170 Loss 2.6799\n",
      "Epoch 1 Batch 171 Loss 2.4379\n",
      "Epoch 1 Batch 172 Loss 2.3113\n",
      "Epoch 1 Batch 173 Loss 2.8215\n",
      "Epoch 1 Batch 174 Loss 2.6059\n",
      "Epoch 1 Batch 175 Loss 2.6432\n",
      "Epoch 1 Batch 176 Loss 2.3513\n",
      "Epoch 1 Batch 177 Loss 2.8453\n",
      "Epoch 1 Batch 178 Loss 2.5837\n",
      "Epoch 1 Batch 179 Loss 2.2726\n",
      "Epoch 1 Batch 180 Loss 2.1194\n",
      "Epoch 1 Batch 181 Loss 2.1744\n",
      "Epoch 1 Batch 182 Loss 2.0413\n",
      "Epoch 1 Batch 183 Loss 1.8558\n",
      "Epoch 1 Batch 184 Loss 1.8125\n",
      "Epoch 1 Batch 185 Loss 1.9466\n",
      "Epoch 1 Batch 186 Loss 2.1311\n",
      "Epoch 1 Batch 187 Loss 2.1136\n",
      "Epoch 1 Batch 188 Loss 1.9491\n",
      "Epoch 1 Batch 189 Loss 2.4922\n",
      "Epoch 1 Batch 190 Loss 2.7133\n",
      "Epoch 1 Batch 191 Loss 2.8314\n",
      "Epoch 1 Batch 192 Loss 3.0526\n",
      "Epoch 1 Batch 193 Loss 3.0522\n",
      "Epoch 1 Batch 194 Loss 2.5966\n",
      "Epoch 1 Batch 195 Loss 2.5867\n",
      "Epoch 1 Batch 196 Loss 3.3329\n",
      "Epoch 1 Batch 197 Loss 3.0309\n",
      "Epoch 1 Batch 198 Loss 2.3152\n",
      "Epoch 1 Batch 199 Loss 2.5482\n",
      "Epoch 1 Batch 200 Loss 2.8602\n",
      "Epoch 1 Batch 201 Loss 2.1062\n",
      "Epoch 1 Batch 202 Loss 2.1994\n",
      "Epoch 1 Batch 203 Loss 2.0502\n",
      "Epoch 1 Batch 204 Loss 2.1043\n",
      "Epoch 1 Batch 205 Loss 2.3741\n",
      "Epoch 1 Batch 206 Loss 2.8702\n",
      "Epoch 1 Batch 207 Loss 2.9256\n",
      "Epoch 1 Batch 208 Loss 2.5158\n",
      "Epoch 1 Batch 209 Loss 2.1614\n",
      "Epoch 1 Batch 210 Loss 2.7359\n",
      "Epoch 1 Batch 211 Loss 2.4392\n",
      "Epoch 1 Batch 212 Loss 2.5633\n",
      "Epoch 1 Batch 213 Loss 2.4807\n",
      "Epoch 1 Batch 214 Loss 2.5585\n",
      "Epoch 1 Batch 215 Loss 2.9571\n",
      "Epoch 1 Batch 216 Loss 2.4981\n",
      "Epoch 1 Batch 217 Loss 2.4663\n",
      "Epoch 1 Batch 218 Loss 1.4804\n",
      "Epoch 1 Batch 219 Loss 2.1263\n",
      "Epoch 1 Batch 220 Loss 2.9785\n",
      "Epoch 1 Batch 221 Loss 2.1023\n",
      "Epoch 1 Batch 222 Loss 2.3787\n",
      "Epoch 1 Batch 223 Loss 2.2947\n",
      "Epoch 1 Batch 224 Loss 2.4435\n",
      "Epoch 1 Batch 225 Loss 2.3161\n",
      "Epoch 1 Batch 226 Loss 2.6469\n",
      "Epoch 1 Batch 227 Loss 2.3697\n",
      "Epoch 1 Batch 228 Loss 2.8273\n",
      "Epoch 1 Batch 229 Loss 2.2675\n",
      "Epoch 1 Batch 230 Loss 2.0508\n",
      "Epoch 1 Batch 231 Loss 2.3235\n",
      "Epoch 1 Batch 232 Loss 1.9769\n",
      "Epoch 1 Batch 233 Loss 2.2207\n",
      "Epoch 1 Batch 234 Loss 1.5929\n",
      "Epoch 1 Batch 235 Loss 2.0676\n",
      "Epoch 1 Batch 236 Loss 2.3832\n",
      "Epoch 1 Batch 237 Loss 2.4098\n",
      "Epoch 1 Batch 238 Loss 2.0383\n",
      "Epoch 1 Batch 239 Loss 2.2007\n",
      "Epoch 1 Batch 240 Loss 2.0768\n",
      "Epoch 1 Batch 241 Loss 2.5313\n",
      "Epoch 1 Batch 242 Loss 2.9232\n",
      "Epoch 1 Batch 243 Loss 2.3542\n",
      "Epoch 1 Batch 244 Loss 2.5897\n",
      "Epoch 1 Batch 245 Loss 2.6997\n",
      "Epoch 1 Batch 246 Loss 2.5944\n",
      "Epoch 1 Batch 247 Loss 2.0906\n",
      "Epoch 1 Batch 248 Loss 2.5644\n",
      "Epoch 1 Batch 249 Loss 2.7564\n",
      "Epoch 1 Batch 250 Loss 2.0965\n",
      "Epoch 1 Batch 251 Loss 2.4813\n",
      "Epoch 1 Batch 252 Loss 3.0839\n",
      "Epoch 1 Batch 253 Loss 3.1102\n",
      "Epoch 1 Batch 254 Loss 2.2369\n",
      "Epoch 1 Batch 255 Loss 2.5319\n",
      "Epoch 1 Batch 256 Loss 2.3615\n",
      "Epoch 1 Batch 257 Loss 2.7021\n",
      "Epoch 1 Batch 258 Loss 2.0691\n",
      "Epoch 1 Batch 259 Loss 2.2562\n",
      "Epoch 1 Batch 260 Loss 2.1806\n",
      "Epoch 1 Batch 261 Loss 2.1964\n",
      "Epoch 1 Batch 262 Loss 1.8498\n",
      "Epoch 1 Batch 263 Loss 2.0302\n",
      "Epoch 1 Batch 264 Loss 2.0898\n",
      "Epoch 1 Batch 265 Loss 2.1634\n",
      "Epoch 1 Batch 266 Loss 2.2413\n",
      "Epoch 1 Batch 267 Loss 2.4801\n",
      "Epoch 1 Batch 268 Loss 2.4181\n",
      "Epoch 1 Batch 269 Loss 2.8004\n",
      "Epoch 1 Batch 270 Loss 3.2501\n",
      "Epoch 1 Batch 271 Loss 3.0315\n",
      "Epoch 1 Batch 272 Loss 3.3526\n",
      "Epoch 1 Batch 273 Loss 2.7694\n",
      "Epoch 1 Batch 274 Loss 2.7485\n",
      "Epoch 1 Batch 275 Loss 2.6867\n",
      "Epoch 1 Batch 276 Loss 2.5919\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 277 Loss 2.5332\n",
      "Epoch 1 Batch 278 Loss 2.7266\n",
      "Epoch 1 Batch 279 Loss 2.2328\n",
      "Epoch 1 Batch 280 Loss 2.3626\n",
      "Epoch 1 Batch 281 Loss 2.8816\n",
      "Epoch 1 Batch 282 Loss 2.6482\n",
      "Epoch 1 Batch 283 Loss 2.3508\n",
      "Epoch 1 Batch 284 Loss 2.4148\n",
      "Epoch 1 Batch 285 Loss 2.1364\n",
      "Epoch 1 Batch 286 Loss 1.7620\n",
      "Epoch 1 Batch 287 Loss 2.5446\n",
      "Epoch 1 Batch 288 Loss 1.9034\n",
      "Epoch 1 Batch 289 Loss 2.3294\n",
      "Epoch 1 Batch 290 Loss 2.4867\n",
      "Epoch 1 Batch 291 Loss 2.5074\n",
      "Epoch 1 Batch 292 Loss 1.7173\n",
      "Epoch 1 Batch 293 Loss 2.1615\n",
      "Epoch 1 Batch 294 Loss 2.1000\n",
      "Epoch 1 Batch 295 Loss 2.4031\n",
      "Epoch 1 Batch 296 Loss 1.9260\n",
      "Epoch 1 Batch 297 Loss 2.8261\n",
      "Epoch 1 Batch 298 Loss 2.3688\n",
      "Epoch 1 Batch 299 Loss 2.1287\n",
      "Epoch 1 Batch 300 Loss 2.3126\n",
      "Epoch 1 Batch 301 Loss 2.2815\n",
      "Epoch 1 Batch 302 Loss 1.7520\n",
      "Epoch 1 Batch 303 Loss 2.2485\n",
      "Epoch 1 Batch 304 Loss 1.6329\n",
      "Epoch 1 Batch 305 Loss 2.2861\n",
      "Epoch 1 Batch 306 Loss 2.0162\n",
      "Epoch 1 Batch 307 Loss 2.9728\n",
      "Epoch 1 Batch 308 Loss 2.4058\n",
      "Epoch 1 Batch 309 Loss 2.5699\n",
      "Epoch 1 Batch 310 Loss 2.0918\n",
      "Epoch 1 Batch 311 Loss 2.7916\n",
      "Epoch 1 Batch 312 Loss 1.9978\n",
      "Epoch 1 Batch 313 Loss 2.4355\n",
      "Epoch 1 Batch 314 Loss 2.5204\n",
      "Epoch 1 Batch 315 Loss 2.4591\n",
      "Epoch 1 Batch 316 Loss 2.1635\n",
      "Epoch 1 Batch 317 Loss 2.4706\n",
      "Epoch 1 Batch 318 Loss 1.8999\n",
      "Epoch 1 Batch 319 Loss 1.5219\n",
      "Epoch 1 Batch 320 Loss 2.2400\n",
      "Epoch 1 Batch 321 Loss 2.4123\n",
      "Epoch 1 Batch 322 Loss 3.3645\n",
      "Epoch 1 Batch 323 Loss 2.4655\n",
      "Epoch 1 Batch 324 Loss 2.0875\n",
      "Epoch 1 Batch 325 Loss 2.0768\n",
      "Epoch 1 Batch 326 Loss 2.4470\n",
      "Epoch 1 Batch 327 Loss 2.1219\n",
      "Epoch 1 Batch 328 Loss 3.0059\n",
      "Epoch 1 Batch 329 Loss 2.2662\n",
      "Epoch 1 Batch 330 Loss 2.1487\n",
      "Epoch 1 Batch 331 Loss 2.3571\n",
      "Epoch 1 Batch 332 Loss 2.0877\n",
      "Epoch 1 Batch 333 Loss 2.0407\n",
      "Epoch 1 Batch 334 Loss 2.1134\n",
      "Epoch 1 Batch 335 Loss 1.8264\n",
      "Epoch 1 Batch 336 Loss 2.2049\n",
      "Epoch 1 Batch 337 Loss 2.3071\n",
      "Epoch 1 Batch 338 Loss 2.5702\n",
      "Epoch 1 Batch 339 Loss 2.7576\n",
      "Epoch 1 Batch 340 Loss 2.5724\n",
      "Epoch 1 Batch 341 Loss 2.1694\n",
      "Epoch 1 Batch 342 Loss 2.4148\n",
      "Epoch 1 Batch 343 Loss 2.4045\n",
      "Epoch 1 Batch 344 Loss 2.1458\n",
      "Epoch 1 Batch 345 Loss 2.2302\n",
      "Epoch 1 Batch 346 Loss 2.3773\n",
      "Epoch 1 Batch 347 Loss 2.3829\n",
      "Epoch 1 Batch 348 Loss 2.5015\n",
      "Epoch 1 Batch 349 Loss 2.5560\n",
      "Epoch 1 Batch 350 Loss 2.4687\n",
      "Epoch 1 Batch 351 Loss 1.7655\n",
      "Epoch 1 Batch 352 Loss 1.7364\n",
      "Epoch 1 Batch 353 Loss 2.0336\n",
      "Epoch 1 Batch 354 Loss 1.7124\n",
      "Epoch 1 Batch 355 Loss 2.2998\n",
      "Epoch 1 Batch 356 Loss 2.2625\n",
      "Epoch 1 Batch 357 Loss 2.5436\n",
      "Epoch 1 Batch 358 Loss 2.4013\n",
      "Epoch 1 Batch 359 Loss 2.6664\n",
      "Epoch 1 Batch 360 Loss 2.3057\n",
      "Epoch 1 Batch 361 Loss 2.2291\n",
      "Epoch 1 Batch 362 Loss 2.3264\n",
      "Epoch 1 Batch 363 Loss 2.5398\n",
      "Epoch 1 Batch 364 Loss 2.3479\n",
      "Epoch 1 Batch 365 Loss 2.9126\n",
      "Epoch 1 Batch 366 Loss 2.4842\n",
      "Epoch 1 Batch 367 Loss 2.1619\n",
      "Epoch 1 Batch 368 Loss 2.3842\n",
      "Epoch 1 Batch 369 Loss 2.4876\n",
      "Epoch 1 Batch 370 Loss 3.0875\n",
      "Epoch 1 Batch 371 Loss 2.5535\n",
      "Epoch 1 Batch 372 Loss 2.1986\n",
      "Epoch 1 Batch 373 Loss 2.3750\n",
      "Epoch 1 Batch 374 Loss 2.0278\n",
      "Epoch 1 Batch 375 Loss 1.6727\n",
      "Epoch 1 Batch 376 Loss 1.5079\n",
      "Epoch 1 Batch 377 Loss 2.1954\n",
      "Epoch 1 Batch 378 Loss 2.0253\n",
      "Epoch 1 Batch 379 Loss 2.1142\n",
      "Epoch 1 Batch 380 Loss 1.8532\n",
      "Epoch 1 Batch 381 Loss 2.3771\n",
      "Epoch 1 Batch 382 Loss 1.9553\n",
      "Epoch 1 Batch 383 Loss 2.1341\n",
      "Epoch 1 Batch 384 Loss 1.8893\n",
      "Epoch 1 Batch 385 Loss 2.3114\n",
      "Epoch 1 Batch 386 Loss 1.9859\n",
      "Epoch 1 Batch 387 Loss 2.1746\n",
      "Epoch 1 Batch 388 Loss 2.2136\n",
      "Epoch 1 Batch 389 Loss 2.4818\n",
      "Epoch 1 Batch 390 Loss 2.3266\n",
      "Epoch 1 Batch 391 Loss 2.5229\n",
      "Epoch 1 Batch 392 Loss 1.9732\n",
      "Epoch 1 Batch 393 Loss 2.2386\n",
      "Epoch 1 Batch 394 Loss 2.2073\n",
      "Epoch 1 Batch 395 Loss 1.9445\n",
      "Epoch 1 Batch 396 Loss 1.5764\n",
      "Epoch 1 Batch 397 Loss 1.6278\n",
      "Epoch 1 Batch 398 Loss 2.3878\n",
      "Epoch 1 Batch 399 Loss 2.7971\n",
      "Epoch 1 Batch 400 Loss 1.9263\n",
      "Epoch 1 Batch 401 Loss 2.0033\n",
      "Epoch 1 Batch 402 Loss 1.9393\n",
      "Epoch 1 Batch 403 Loss 2.4715\n",
      "Epoch 1 Batch 404 Loss 1.4479\n",
      "Epoch 1 Batch 405 Loss 2.3966\n",
      "Epoch 1 Batch 406 Loss 2.2774\n",
      "Epoch 1 Batch 407 Loss 2.7712\n",
      "Epoch 1 Batch 408 Loss 2.2994\n",
      "Epoch 1 Batch 409 Loss 2.6066\n",
      "Epoch 1 Batch 410 Loss 1.9052\n",
      "Epoch 1 Batch 411 Loss 2.5866\n",
      "Epoch 1 Batch 412 Loss 2.6389\n",
      "Epoch 1 Batch 413 Loss 2.6033\n",
      "Epoch 1 Batch 414 Loss 1.9001\n",
      "Epoch 1 Batch 415 Loss 2.2826\n",
      "Epoch 1 Batch 416 Loss 2.6494\n",
      "Epoch 1 Batch 417 Loss 2.3308\n",
      "Epoch 1 Batch 418 Loss 2.4106\n",
      "Epoch 1 Batch 419 Loss 2.2600\n",
      "Epoch 1 Batch 420 Loss 1.8991\n",
      "Epoch 1 Batch 421 Loss 2.5408\n",
      "Epoch 1 Batch 422 Loss 2.6528\n",
      "Epoch 1 Batch 423 Loss 2.7065\n",
      "Epoch 1 Batch 424 Loss 2.5008\n",
      "Epoch 1 Batch 425 Loss 2.1180\n",
      "Epoch 1 Batch 426 Loss 3.0007\n",
      "Epoch 1 Batch 427 Loss 2.9065\n",
      "Epoch 1 Batch 428 Loss 2.8368\n",
      "Epoch 1 Batch 429 Loss 2.2959\n",
      "Epoch 1 Batch 430 Loss 2.7159\n",
      "Epoch 1 Batch 431 Loss 2.4552\n",
      "Epoch 1 Batch 432 Loss 1.8722\n",
      "Epoch 1 Batch 433 Loss 2.6489\n",
      "Epoch 1 Batch 434 Loss 1.9466\n",
      "Epoch 1 Batch 435 Loss 2.0689\n",
      "Epoch 1 Batch 436 Loss 2.2267\n",
      "Epoch 1 Batch 437 Loss 2.8142\n",
      "Epoch 1 Batch 438 Loss 1.8962\n",
      "Epoch 1 Batch 439 Loss 2.5018\n",
      "Epoch 1 Batch 440 Loss 2.6592\n",
      "Epoch 1 Batch 441 Loss 2.6274\n",
      "Epoch 1 Batch 442 Loss 2.7549\n",
      "Epoch 1 Batch 443 Loss 1.9244\n",
      "Epoch 1 Batch 444 Loss 2.2778\n",
      "Epoch 1 Batch 445 Loss 1.9666\n",
      "Epoch 1 Batch 446 Loss 2.2378\n",
      "Epoch 1 Batch 447 Loss 2.8127\n",
      "Epoch 1 Batch 448 Loss 2.7156\n",
      "Epoch 1 Batch 449 Loss 2.2145\n",
      "Epoch 1 Batch 450 Loss 2.4448\n",
      "Epoch 1 Batch 451 Loss 2.8721\n",
      "Epoch 1 Batch 452 Loss 2.6561\n",
      "Epoch 1 Batch 453 Loss 2.3862\n",
      "Epoch 1 Batch 454 Loss 2.7931\n",
      "Epoch 1 Batch 455 Loss 2.2247\n",
      "Epoch 1 Batch 456 Loss 2.3214\n",
      "Epoch 1 Batch 457 Loss 2.3777\n",
      "Epoch 1 Batch 458 Loss 1.8196\n",
      "Epoch 1 Batch 459 Loss 2.0933\n",
      "Epoch 1 Batch 460 Loss 1.7953\n",
      "Epoch 1 Batch 461 Loss 1.6186\n",
      "Epoch 1 Batch 462 Loss 2.0236\n",
      "Epoch 1 Batch 463 Loss 2.8463\n",
      "Epoch 1 Batch 464 Loss 2.9027\n",
      "Epoch 1 Batch 465 Loss 2.5285\n",
      "Epoch 1 Batch 466 Loss 2.7888\n",
      "Epoch 1 Batch 467 Loss 2.4353\n",
      "Epoch 1 Batch 468 Loss 2.6006\n",
      "Epoch 1 Batch 469 Loss 2.3728\n",
      "Epoch 1 Batch 470 Loss 2.3264\n",
      "Epoch 1 Batch 471 Loss 2.4287\n",
      "Epoch 1 Batch 472 Loss 2.1148\n",
      "Epoch 1 Batch 473 Loss 1.9376\n",
      "Epoch 1 Batch 474 Loss 2.0490\n",
      "Epoch 1 Batch 475 Loss 1.9952\n",
      "Epoch 1 Batch 476 Loss 2.0909\n",
      "Epoch 1 Batch 477 Loss 2.1046\n",
      "Epoch 1 Batch 478 Loss 1.6882\n",
      "Epoch 1 Batch 479 Loss 1.8164\n",
      "Epoch 1 Batch 480 Loss 2.5344\n",
      "Epoch 1 Batch 481 Loss 2.1469\n",
      "Epoch 1 Batch 482 Loss 2.1726\n",
      "Epoch 1 Batch 483 Loss 2.7029\n",
      "Epoch 1 Batch 484 Loss 2.7376\n",
      "Epoch 1 Batch 485 Loss 2.5416\n",
      "Epoch 1 Batch 486 Loss 2.1781\n",
      "Epoch 1 Batch 487 Loss 1.7728\n",
      "Epoch 1 Batch 488 Loss 1.5850\n",
      "Epoch 1 Batch 489 Loss 1.7160\n",
      "Epoch 1 Batch 490 Loss 1.9257\n",
      "Epoch 1 Batch 491 Loss 1.9170\n",
      "Epoch 1 Batch 492 Loss 2.7170\n",
      "Epoch 1 Batch 493 Loss 2.0662\n",
      "Epoch 1 Batch 494 Loss 2.1597\n",
      "Epoch 1 Batch 495 Loss 2.0780\n",
      "Epoch 1 Batch 496 Loss 1.5792\n",
      "Epoch 1 Batch 497 Loss 2.2658\n",
      "Epoch 1 Batch 498 Loss 1.7912\n",
      "Epoch 1 Batch 499 Loss 1.9836\n",
      "Epoch 1 Batch 500 Loss 2.2081\n",
      "Epoch 1 Batch 501 Loss 1.9202\n",
      "Epoch 1 Batch 502 Loss 1.4794\n",
      "Epoch 1 Batch 503 Loss 1.4732\n",
      "Epoch 1 Batch 504 Loss 1.8266\n",
      "Epoch 1 Batch 505 Loss 1.6935\n",
      "Epoch 1 Batch 506 Loss 1.8583\n",
      "Epoch 1 Batch 507 Loss 2.0932\n",
      "Epoch 1 Batch 508 Loss 2.0076\n",
      "Epoch 1 Batch 509 Loss 2.9455\n",
      "Epoch 1 Batch 510 Loss 2.6306\n",
      "Epoch 1 Batch 511 Loss 2.0361\n",
      "Epoch 1 Batch 512 Loss 2.1565\n",
      "Epoch 1 Batch 513 Loss 1.9165\n",
      "Epoch 1 Batch 514 Loss 1.9627\n",
      "Epoch 1 Batch 515 Loss 2.3468\n",
      "Epoch 1 Batch 516 Loss 2.4171\n",
      "Epoch 1 Batch 517 Loss 2.4151\n",
      "Epoch 1 Batch 518 Loss 2.0979\n",
      "Epoch 1 Batch 519 Loss 2.0550\n",
      "Epoch 1 Batch 520 Loss 1.4894\n",
      "Epoch 1 Batch 521 Loss 1.8032\n",
      "Epoch 1 Batch 522 Loss 1.9968\n",
      "Epoch 1 Batch 523 Loss 1.9412\n",
      "Epoch 1 Batch 524 Loss 1.9476\n",
      "Epoch 1 Batch 525 Loss 1.8444\n",
      "Epoch 1 Batch 526 Loss 1.8580\n",
      "Epoch 1 Batch 527 Loss 1.9818\n",
      "Epoch 1 Batch 528 Loss 2.3960\n",
      "Epoch 1 Batch 529 Loss 2.0090\n",
      "Epoch 1 Batch 530 Loss 1.8717\n",
      "Epoch 1 Batch 531 Loss 2.5128\n",
      "Epoch 1 Batch 532 Loss 2.7656\n",
      "Epoch 1 Batch 533 Loss 2.4214\n",
      "Epoch 1 Batch 534 Loss 2.4146\n",
      "Epoch 1 Batch 535 Loss 2.2648\n",
      "Epoch 1 Batch 536 Loss 2.0922\n",
      "Epoch 1 Batch 537 Loss 2.3995\n",
      "Epoch 1 Batch 538 Loss 2.2854\n"
     ]
    }
   ],
   "source": [
    "epochs = params[\"epochs\"]\n",
    "# 如果检查点存在，则恢复最新的检查点。\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print ('Latest checkpoint restored!!')\n",
    "    \n",
    "for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch+1)):\n",
    "        batch_loss = train_step(inp, targ)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        if batch % 1 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                         batch,\n",
    "                                                         batch_loss.numpy()))\n",
    "    # saving (checkpoint) the model every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        ckpt_save_path = ckpt_manager.save()\n",
    "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
    "                                                             ckpt_save_path))\n",
    "\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 载入模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果检查点存在，则恢复最新的检查点。\n",
    "ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "print(\"Model restored\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model,inputs):\n",
    "    attention_plot = np.zeros((params[\"max_dec_len\"], params[\"max_enc_len\"]))\n",
    "    \n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "    result = ''\n",
    "    \n",
    "    hidden = [tf.zeros((1, params[\"enc_units\"]))]\n",
    "    enc_output, enc_hidden = model.encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    \n",
    "    dec_input = tf.expand_dims([vocab['<START>']], 0)\n",
    "    \n",
    "    context_vector, _ = model.attention(dec_hidden, enc_output)\n",
    "\n",
    "    for t in range(params[\"max_dec_len\"]):\n",
    "        \n",
    "        context_vector, attention_weights = model.attention(dec_hidden, enc_output)\n",
    "        \n",
    "        predictions, dec_hidden = model.decoder(dec_input,\n",
    "                                         dec_hidden,\n",
    "                                         enc_output,\n",
    "                                         context_vector)\n",
    "\n",
    "        # storing the attention weights to plot later on\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        \n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += vocab_reversed[predicted_id] + ' '\n",
    "        if vocab_reversed[predicted_id] == '<STOP>':\n",
    "            return result, attention_plot\n",
    "\n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    st = preproc.sentence_proc(sentence)\n",
    "    sentence = preproc.sentence_proc_eval(sentence,params[\"max_enc_len\"]-2,vocab)\n",
    "    result, attention_plot = evaluate(model,sentence)\n",
    "\n",
    "    print('Input: %s' % (st))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "\n",
    "    attention_plot = attention_plot[:len(result.split(' ')), :len(st.split(' '))]\n",
    "    plot_attention(attention_plot, st.split(' '), result.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = '方向机重，助力泵，方向机都换了还是一样'\n",
    "preproc = Preprocess()\n",
    "preproc.sentence_proc(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib as mpl\n",
    "# mpl.rcParams['font.family'] = 'STSong'  # 显示中文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "translate(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下半部分\n",
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_predict(inps):\n",
    "    # 判断输入长度\n",
    "    batch_size=len(inps)\n",
    "    # 开辟结果存储list\n",
    "    preidicts=[''] * batch_size\n",
    "    \n",
    "    inps = tf.convert_to_tensor(inps)\n",
    "    # 0. 初始化隐藏层输入\n",
    "    hidden = [tf.zeros((batch_size, params[\"enc_units\"]))]\n",
    "    # 1. 构建encoder\n",
    "    enc_output, enc_hidden = model.encoder(inps, hidden)\n",
    "    # 2. 复制\n",
    "    dec_hidden = enc_hidden\n",
    "    # 3. <START> * BATCH_SIZE \n",
    "    dec_input = tf.expand_dims([vocab['<START>']] * batch_size, 1)\n",
    "    \n",
    "    context_vector, _ = model.attention(dec_hidden, enc_output)\n",
    "    # Teacher forcing - feeding the target as the next input\n",
    "    for t in range(params[\"max_dec_len\"]):\n",
    "        # 计算上下文\n",
    "        context_vector, attention_weights = model.attention(dec_hidden, enc_output)\n",
    "        # 单步预测\n",
    "        predictions, dec_hidden = model.decoder(dec_input,\n",
    "                                         dec_hidden,\n",
    "                                         enc_output,\n",
    "                                         context_vector)\n",
    "        \n",
    "        # id转换 贪婪搜索\n",
    "        predicted_ids = tf.argmax(predictions,axis=1).numpy()\n",
    "        \n",
    "        \n",
    "        for index,predicted_id in enumerate(predicted_ids):\n",
    "            preidicts[index]+= vocab_reversed[predicted_id] + ' '\n",
    "        \n",
    "        # using teacher forcing\n",
    "        dec_input = tf.expand_dims(predicted_ids, 1)\n",
    "\n",
    "    results=[]\n",
    "    for preidict in preidicts:\n",
    "        # 去掉句子前后空格\n",
    "        preidict=preidict.strip()\n",
    "        # 句子小于max len就结束了 截断\n",
    "        if '<STOP>' in preidict:\n",
    "            # 截断stop\n",
    "            preidict=preidict[:preidict.index('<STOP>')]\n",
    "        # 保存结果\n",
    "        results.append(preidict)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试代码\n",
    "# ds = iter(dataset)\n",
    "# x,y = ds.next()\n",
    "# batch_predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict(data_X, batch_size):\n",
    "    # 存储结果\n",
    "    results=[]\n",
    "    # 样本数量\n",
    "    sample_size=len(data_X)\n",
    "    # batch 操作轮数 math.ceil向上取整 小数 +1\n",
    "    # 因为最后一个batch可能不足一个batch size 大小 ,但是依然需要计算  \n",
    "    steps_epoch = math.ceil(sample_size/batch_size)\n",
    "    # [0,steps_epoch)\n",
    "    for i in tqdm(range(steps_epoch)):\n",
    "        batch_data = data_X[i*batch_size:(i+1)*batch_size]\n",
    "        results+=batch_predict(batch_data)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "results=model_predict(train_x[:sample_num+1],batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf2.0]",
   "language": "python",
   "name": "conda-env-tf2.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
