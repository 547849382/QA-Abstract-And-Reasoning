{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import sys\n",
    "sys.path.append('../')  # 返回notebook的上一级目录\n",
    "# sys.path.append('E:\\GitHub\\QA-abstract-and-reasoning')  # 效果同上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no colab\n"
     ]
    }
   ],
   "source": [
    "# 在google colab运行则执行以下代码\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive_path = '/content/drive'\n",
    "    working_path = drive_path + \"/My Drive/QA\" # 工作路径\n",
    "    drive.mount(drive_path)\n",
    "    os.chdir(working_path)\n",
    "    sys.path.append(working_path)  # 环境变量\n",
    "    print(\"current working directory: \", os.getcwd())\n",
    "    \n",
    "    # %tensorflow_version 仅存在于 Colab\n",
    "    %tensorflow_version 2.x\n",
    "    print(\"run notebook in colab\")\n",
    "except:\n",
    "    print(\"no colab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "np.set_printoptions(suppress=True)\n",
    "from utils.plot import plot_attention\n",
    "from utils.saveLoader import *\n",
    "from utils.config import *\n",
    "from layers import *\n",
    "from preprocess import Preprocess\n",
    "from gensim.models.word2vec import LineSentence, Word2Vec\n",
    "import tensorflow as tf\n",
    "# from model_layer import seq2seq_model\n",
    "import time\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[限制gpu内存增长](https://tensorflow.google.cn/guide/gpu#limiting_gpu_memory_growth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "from utils.config_gpu import config_gpu\n",
    "config_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x,train_y,test_x = load_train_dataset()  # 数据集\n",
    "vocab,vocab_reversed = load_vocab(VOCAB_PAD)  # vocab\n",
    "embedding_matrix = np.loadtxt(EMBEDDING_MATRIX_PAD)  # 预训练层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 设置参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'vocab_size': 32566, 'max_enc_len': 260, 'max_dec_len': 33, 'embed_size': 300, 'enc_units': 256, 'attn_units': 10, 'dec_units': 256, 'batch_size': 32, 'epochs': 2}\n"
     ]
    }
   ],
   "source": [
    "params = {}\n",
    "params[\"vocab_size\"] = len(vocab)\n",
    "params[\"max_enc_len\"] = train_x.shape[1]  # 260\n",
    "params[\"max_dec_len\"] = train_y.shape[1]  # 33\n",
    "params[\"embed_size\"] = embedding_matrix.shape[1]\n",
    "params[\"enc_units\"] = 256\n",
    "params[\"attn_units\"] = 10\n",
    "params[\"dec_units\"] = params[\"enc_units\"]\n",
    "params[\"batch_size\"] = 32\n",
    "params[\"epochs\"] = 2\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取部分数据进行训练\n",
    "# sample_num=64\n",
    "sample_num = train_x.shape[0]\n",
    "dataset = tf.data.Dataset.from_tensor_slices((train_x[:sample_num], train_y[:sample_num])).shuffle(params[\"batch_size\"]*2+1)\n",
    "dataset = dataset.batch(params[\"batch_size\"], drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = sample_num//params[\"batch_size\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seq2seq import *\n",
    "model=Seq2Seq(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存点设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there no files in this path\n"
     ]
    }
   ],
   "source": [
    "from utils.config import CKPT_DIR, CKPT_PREFIX\n",
    "from utils.saveLoader import del_all_files_of_dir\n",
    "del_all_files_of_dir(CKPT_DIR)\n",
    "ckpt = tf.train.Checkpoint(Seq2Seq=model)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, CKPT_DIR, max_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model restored\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "print(\"Model restored\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[SparseCategoricalCrossentropy](https://tensorflow.google.cn/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(name='Adam',learning_rate=0.001)\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "pad_index=vocab['<PAD>']\n",
    "unk_index=vocab['<UNK>']\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    pad_mask = tf.math.equal(real, pad_index)\n",
    "    unk_mask = tf.math.equal(real, unk_index)\n",
    "    # <PAD> 和 <UNK> 的损失都不算\n",
    "    mask = tf.math.logical_not(tf.math.logical_or(pad_mask,unk_mask))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_mean(loss_)\n",
    "    # return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 调试train_step()\n",
    "# inp, targ = next(iter(dataset))\n",
    "# pad_index=vocab['<PAD>']\n",
    "# unk_index=vocab['<UNK>']\n",
    "# enc_output, enc_hidden = model.call_encoder(inp)\n",
    "# dec_hidden = enc_hidden\n",
    "# dec_input = tf.expand_dims([vocab['<START>']] * params[\"batch_size\"], 1)\n",
    "# predictions, _ = model(dec_input, dec_hidden, enc_output, targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ):\n",
    "    pad_index=vocab['<PAD>']\n",
    "    unk_index=vocab['<UNK>']\n",
    "    loss = 0\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        # 1. 构建encoder\n",
    "        enc_output, enc_hidden = model.call_encoder(inp)\n",
    "        # 2. 复制\n",
    "        dec_hidden = enc_hidden\n",
    "        # 3. <START> * BATCH_SIZE \n",
    "        dec_input = tf.expand_dims([vocab['<START>']] * params[\"batch_size\"], 1)\n",
    "        \n",
    "        # 逐个预测序列\n",
    "        predictions, _ = model(dec_input, dec_hidden, enc_output, targ)\n",
    "        \n",
    "        batch_loss = loss_function(targ[:, 1:], predictions)\n",
    "\n",
    "        variables = model.encoder.trainable_variables + model.decoder.trainable_variables+ model.attention.trainable_variables\n",
    "    \n",
    "        gradients = tape.gradient(batch_loss, variables)\n",
    "\n",
    "        optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "        return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 4.9748\n",
      "Epoch 1 Batch 1 Loss 6.3068\n",
      "Epoch 1 Batch 2 Loss 5.2856\n",
      "Epoch 1 Batch 3 Loss 5.6969\n",
      "Epoch 1 Batch 4 Loss 5.5998\n",
      "Epoch 1 Batch 5 Loss 5.7948\n",
      "Epoch 1 Batch 6 Loss 4.0068\n",
      "Epoch 1 Batch 7 Loss 4.8437\n",
      "Epoch 1 Batch 8 Loss 4.1494\n",
      "Epoch 1 Batch 9 Loss 4.8610\n",
      "Epoch 1 Batch 10 Loss 4.2249\n",
      "Epoch 1 Batch 11 Loss 3.5277\n",
      "Epoch 1 Batch 12 Loss 3.3266\n",
      "Epoch 1 Batch 13 Loss 2.4634\n",
      "Epoch 1 Batch 14 Loss 2.2345\n",
      "Epoch 1 Batch 15 Loss 3.2764\n",
      "Epoch 1 Batch 16 Loss 2.7978\n",
      "Epoch 1 Batch 17 Loss 2.6513\n",
      "Epoch 1 Batch 18 Loss 2.0814\n",
      "Epoch 1 Batch 19 Loss 3.2465\n",
      "Epoch 1 Batch 20 Loss 4.1165\n",
      "Epoch 1 Batch 21 Loss 3.5698\n",
      "Epoch 1 Batch 22 Loss 2.7225\n",
      "Epoch 1 Batch 23 Loss 3.1314\n",
      "Epoch 1 Batch 24 Loss 2.6006\n",
      "Epoch 1 Batch 25 Loss 2.4615\n",
      "Epoch 1 Batch 26 Loss 2.3426\n",
      "Epoch 1 Batch 27 Loss 3.4894\n",
      "Epoch 1 Batch 28 Loss 3.2427\n",
      "Epoch 1 Batch 29 Loss 3.5756\n",
      "Epoch 1 Batch 30 Loss 2.7058\n",
      "Epoch 1 Batch 31 Loss 3.3102\n",
      "Epoch 1 Batch 32 Loss 2.4620\n",
      "Epoch 1 Batch 33 Loss 2.8100\n",
      "Epoch 1 Batch 34 Loss 1.8753\n",
      "Epoch 1 Batch 35 Loss 1.9313\n",
      "Epoch 1 Batch 36 Loss 2.4240\n",
      "Epoch 1 Batch 37 Loss 1.8191\n",
      "Epoch 1 Batch 38 Loss 1.5824\n",
      "Epoch 1 Batch 39 Loss 2.4103\n",
      "Epoch 1 Batch 40 Loss 2.1603\n",
      "Epoch 1 Batch 41 Loss 2.4711\n",
      "Epoch 1 Batch 42 Loss 2.2758\n",
      "Epoch 1 Batch 43 Loss 2.0431\n",
      "Epoch 1 Batch 44 Loss 2.5728\n",
      "Epoch 1 Batch 45 Loss 2.2365\n",
      "Epoch 1 Batch 46 Loss 3.1773\n",
      "Epoch 1 Batch 47 Loss 2.6061\n",
      "Epoch 1 Batch 48 Loss 2.5372\n",
      "Epoch 1 Batch 49 Loss 3.0881\n",
      "Epoch 1 Batch 50 Loss 2.2333\n",
      "Epoch 1 Batch 51 Loss 2.9368\n",
      "Epoch 1 Batch 52 Loss 3.2766\n",
      "Epoch 1 Batch 53 Loss 2.6451\n",
      "Epoch 1 Batch 54 Loss 3.1382\n",
      "Epoch 1 Batch 55 Loss 3.6214\n",
      "Epoch 1 Batch 56 Loss 3.2455\n",
      "Epoch 1 Batch 57 Loss 3.7919\n",
      "Epoch 1 Batch 58 Loss 3.4883\n",
      "Epoch 1 Batch 59 Loss 3.2849\n",
      "Epoch 1 Batch 60 Loss 2.8691\n",
      "Epoch 1 Batch 61 Loss 2.6941\n",
      "Epoch 1 Batch 62 Loss 2.9188\n",
      "Epoch 1 Batch 63 Loss 2.8281\n",
      "Epoch 1 Batch 64 Loss 2.6750\n",
      "Epoch 1 Batch 65 Loss 2.4954\n",
      "Epoch 1 Batch 66 Loss 2.1321\n",
      "Epoch 1 Batch 67 Loss 2.7670\n",
      "Epoch 1 Batch 68 Loss 2.8047\n",
      "Epoch 1 Batch 69 Loss 3.1172\n",
      "Epoch 1 Batch 70 Loss 3.0768\n",
      "Epoch 1 Batch 71 Loss 3.2086\n",
      "Epoch 1 Batch 72 Loss 2.7327\n",
      "Epoch 1 Batch 73 Loss 2.9096\n",
      "Epoch 1 Batch 74 Loss 3.4863\n",
      "Epoch 1 Batch 75 Loss 3.4724\n",
      "Epoch 1 Batch 76 Loss 3.1600\n",
      "Epoch 1 Batch 77 Loss 2.6672\n",
      "Epoch 1 Batch 78 Loss 3.6007\n",
      "Epoch 1 Batch 79 Loss 3.4408\n",
      "Epoch 1 Batch 80 Loss 2.9479\n",
      "Epoch 1 Batch 81 Loss 2.8449\n",
      "Epoch 1 Batch 82 Loss 2.7683\n",
      "Epoch 1 Batch 83 Loss 2.4684\n",
      "Epoch 1 Batch 84 Loss 3.3056\n",
      "Epoch 1 Batch 85 Loss 2.7869\n",
      "Epoch 1 Batch 86 Loss 3.3586\n",
      "Epoch 1 Batch 87 Loss 2.9720\n",
      "Epoch 1 Batch 88 Loss 2.7533\n",
      "Epoch 1 Batch 89 Loss 2.9961\n",
      "Epoch 1 Batch 90 Loss 2.5514\n",
      "Epoch 1 Batch 91 Loss 2.9915\n",
      "Epoch 1 Batch 92 Loss 2.6722\n",
      "Epoch 1 Batch 93 Loss 2.2579\n",
      "Epoch 1 Batch 94 Loss 1.6432\n",
      "Epoch 1 Batch 95 Loss 2.4401\n",
      "Epoch 1 Batch 96 Loss 2.5617\n",
      "Epoch 1 Batch 97 Loss 3.2295\n",
      "Epoch 1 Batch 98 Loss 2.8404\n",
      "Epoch 1 Batch 99 Loss 2.2794\n",
      "Epoch 1 Batch 100 Loss 4.0212\n",
      "Epoch 1 Batch 101 Loss 3.9612\n",
      "Epoch 1 Batch 102 Loss 3.7635\n",
      "Epoch 1 Batch 103 Loss 3.0100\n",
      "Epoch 1 Batch 104 Loss 3.2184\n",
      "Epoch 1 Batch 105 Loss 2.7925\n",
      "Epoch 1 Batch 106 Loss 2.8591\n",
      "Epoch 1 Batch 107 Loss 3.2711\n",
      "Epoch 1 Batch 108 Loss 3.5076\n",
      "Epoch 1 Batch 109 Loss 2.7264\n",
      "Epoch 1 Batch 110 Loss 2.3785\n",
      "Epoch 1 Batch 111 Loss 1.8975\n",
      "Epoch 1 Batch 112 Loss 2.4076\n",
      "Epoch 1 Batch 113 Loss 2.6218\n",
      "Epoch 1 Batch 114 Loss 2.5074\n",
      "Epoch 1 Batch 115 Loss 2.9199\n",
      "Epoch 1 Batch 116 Loss 2.8015\n",
      "Epoch 1 Batch 117 Loss 2.7298\n",
      "Epoch 1 Batch 118 Loss 2.7011\n",
      "Epoch 1 Batch 119 Loss 2.6008\n",
      "Epoch 1 Batch 120 Loss 2.4381\n",
      "Epoch 1 Batch 121 Loss 2.6177\n",
      "Epoch 1 Batch 122 Loss 2.5607\n",
      "Epoch 1 Batch 123 Loss 2.9066\n",
      "Epoch 1 Batch 124 Loss 2.3588\n",
      "Epoch 1 Batch 125 Loss 1.8981\n",
      "Epoch 1 Batch 126 Loss 2.1079\n",
      "Epoch 1 Batch 127 Loss 2.0926\n",
      "Epoch 1 Batch 128 Loss 2.2249\n",
      "Epoch 1 Batch 129 Loss 2.9821\n",
      "Epoch 1 Batch 130 Loss 2.5348\n",
      "Epoch 1 Batch 131 Loss 2.2981\n",
      "Epoch 1 Batch 132 Loss 2.3317\n",
      "Epoch 1 Batch 133 Loss 3.3215\n",
      "Epoch 1 Batch 134 Loss 3.2147\n",
      "Epoch 1 Batch 135 Loss 2.9962\n",
      "Epoch 1 Batch 136 Loss 3.1134\n",
      "Epoch 1 Batch 137 Loss 3.0903\n",
      "Epoch 1 Batch 138 Loss 2.9405\n",
      "Epoch 1 Batch 139 Loss 2.3985\n",
      "Epoch 1 Batch 140 Loss 2.1135\n",
      "Epoch 1 Batch 141 Loss 2.2274\n",
      "Epoch 1 Batch 142 Loss 2.5269\n",
      "Epoch 1 Batch 143 Loss 2.5263\n",
      "Epoch 1 Batch 144 Loss 3.0326\n",
      "Epoch 1 Batch 145 Loss 2.3037\n",
      "Epoch 1 Batch 146 Loss 3.1532\n",
      "Epoch 1 Batch 147 Loss 2.7247\n",
      "Epoch 1 Batch 148 Loss 3.3274\n",
      "Epoch 1 Batch 149 Loss 2.9142\n",
      "Epoch 1 Batch 150 Loss 3.2785\n",
      "Epoch 1 Batch 151 Loss 2.1159\n",
      "Epoch 1 Batch 152 Loss 2.6746\n",
      "Epoch 1 Batch 153 Loss 2.9263\n",
      "Epoch 1 Batch 154 Loss 2.3668\n",
      "Epoch 1 Batch 155 Loss 2.9787\n",
      "Epoch 1 Batch 156 Loss 2.8559\n",
      "Epoch 1 Batch 157 Loss 3.4838\n",
      "Epoch 1 Batch 158 Loss 3.3696\n",
      "Epoch 1 Batch 159 Loss 2.9689\n",
      "Epoch 1 Batch 160 Loss 3.3363\n",
      "Epoch 1 Batch 161 Loss 2.7054\n",
      "Epoch 1 Batch 162 Loss 2.6929\n",
      "Epoch 1 Batch 163 Loss 1.8117\n",
      "Epoch 1 Batch 164 Loss 1.5516\n",
      "Epoch 1 Batch 165 Loss 2.3451\n",
      "Epoch 1 Batch 166 Loss 2.8734\n",
      "Epoch 1 Batch 167 Loss 2.9801\n",
      "Epoch 1 Batch 168 Loss 2.5283\n",
      "Epoch 1 Batch 169 Loss 2.3215\n",
      "Epoch 1 Batch 170 Loss 2.6978\n",
      "Epoch 1 Batch 171 Loss 2.7242\n",
      "Epoch 1 Batch 172 Loss 2.1302\n",
      "Epoch 1 Batch 173 Loss 2.3196\n",
      "Epoch 1 Batch 174 Loss 2.3760\n",
      "Epoch 1 Batch 175 Loss 2.7595\n",
      "Epoch 1 Batch 176 Loss 2.9226\n",
      "Epoch 1 Batch 177 Loss 2.4011\n",
      "Epoch 1 Batch 178 Loss 2.0171\n",
      "Epoch 1 Batch 179 Loss 2.1894\n",
      "Epoch 1 Batch 180 Loss 2.2133\n",
      "Epoch 1 Batch 181 Loss 1.5854\n",
      "Epoch 1 Batch 182 Loss 1.9204\n",
      "Epoch 1 Batch 183 Loss 2.3165\n",
      "Epoch 1 Batch 184 Loss 1.8749\n",
      "Epoch 1 Batch 185 Loss 1.8614\n",
      "Epoch 1 Batch 186 Loss 1.9486\n",
      "Epoch 1 Batch 187 Loss 2.3503\n",
      "Epoch 1 Batch 188 Loss 2.0309\n",
      "Epoch 1 Batch 189 Loss 2.2480\n",
      "Epoch 1 Batch 190 Loss 2.9523\n",
      "Epoch 1 Batch 191 Loss 2.6225\n",
      "Epoch 1 Batch 192 Loss 2.9770\n",
      "Epoch 1 Batch 193 Loss 2.5201\n",
      "Epoch 1 Batch 194 Loss 3.2290\n",
      "Epoch 1 Batch 195 Loss 2.8880\n",
      "Epoch 1 Batch 196 Loss 2.3743\n",
      "Epoch 1 Batch 197 Loss 2.7649\n",
      "Epoch 1 Batch 198 Loss 2.2364\n",
      "Epoch 1 Batch 199 Loss 2.6573\n",
      "Epoch 1 Batch 200 Loss 2.5868\n",
      "Epoch 1 Batch 201 Loss 2.0462\n",
      "Epoch 1 Batch 202 Loss 2.0848\n",
      "Epoch 1 Batch 203 Loss 2.3932\n",
      "Epoch 1 Batch 204 Loss 1.8263\n",
      "Epoch 1 Batch 205 Loss 2.0636\n",
      "Epoch 1 Batch 206 Loss 2.3950\n",
      "Epoch 1 Batch 207 Loss 2.7698\n",
      "Epoch 1 Batch 208 Loss 3.1059\n",
      "Epoch 1 Batch 209 Loss 1.8435\n",
      "Epoch 1 Batch 210 Loss 2.5723\n",
      "Epoch 1 Batch 211 Loss 2.3206\n",
      "Epoch 1 Batch 212 Loss 2.3886\n",
      "Epoch 1 Batch 213 Loss 2.2499\n",
      "Epoch 1 Batch 214 Loss 2.5201\n",
      "Epoch 1 Batch 215 Loss 2.8486\n",
      "Epoch 1 Batch 216 Loss 2.5018\n",
      "Epoch 1 Batch 217 Loss 2.3695\n",
      "Epoch 1 Batch 218 Loss 1.9881\n",
      "Epoch 1 Batch 219 Loss 2.6346\n",
      "Epoch 1 Batch 220 Loss 2.4014\n",
      "Epoch 1 Batch 221 Loss 2.4054\n",
      "Epoch 1 Batch 222 Loss 2.0828\n",
      "Epoch 1 Batch 223 Loss 2.2652\n",
      "Epoch 1 Batch 224 Loss 2.2946\n",
      "Epoch 1 Batch 225 Loss 2.1261\n",
      "Epoch 1 Batch 226 Loss 2.4316\n",
      "Epoch 1 Batch 227 Loss 2.2168\n",
      "Epoch 1 Batch 228 Loss 3.1330\n",
      "Epoch 1 Batch 229 Loss 2.5537\n",
      "Epoch 1 Batch 230 Loss 2.1464\n",
      "Epoch 1 Batch 231 Loss 2.1206\n",
      "Epoch 1 Batch 232 Loss 1.6900\n",
      "Epoch 1 Batch 233 Loss 2.0320\n",
      "Epoch 1 Batch 234 Loss 1.6436\n",
      "Epoch 1 Batch 235 Loss 1.6987\n",
      "Epoch 1 Batch 236 Loss 2.1149\n",
      "Epoch 1 Batch 237 Loss 2.2477\n",
      "Epoch 1 Batch 238 Loss 2.6301\n",
      "Epoch 1 Batch 239 Loss 2.3426\n",
      "Epoch 1 Batch 240 Loss 2.1973\n",
      "Epoch 1 Batch 241 Loss 2.5522\n",
      "Epoch 1 Batch 242 Loss 2.6629\n",
      "Epoch 1 Batch 243 Loss 2.7162\n",
      "Epoch 1 Batch 244 Loss 2.0979\n",
      "Epoch 1 Batch 245 Loss 2.0537\n",
      "Epoch 1 Batch 246 Loss 2.1903\n",
      "Epoch 1 Batch 247 Loss 2.0173\n",
      "Epoch 1 Batch 248 Loss 2.9949\n",
      "Epoch 1 Batch 249 Loss 1.9928\n",
      "Epoch 1 Batch 250 Loss 2.6649\n",
      "Epoch 1 Batch 251 Loss 2.4772\n",
      "Epoch 1 Batch 252 Loss 3.0877\n",
      "Epoch 1 Batch 253 Loss 2.3005\n",
      "Epoch 1 Batch 254 Loss 2.6793\n",
      "Epoch 1 Batch 255 Loss 2.4523\n",
      "Epoch 1 Batch 256 Loss 2.3921\n",
      "Epoch 1 Batch 257 Loss 2.4197\n",
      "Epoch 1 Batch 258 Loss 2.2500\n",
      "Epoch 1 Batch 259 Loss 1.9080\n",
      "Epoch 1 Batch 260 Loss 2.4137\n",
      "Epoch 1 Batch 261 Loss 2.1206\n",
      "Epoch 1 Batch 262 Loss 1.9682\n",
      "Epoch 1 Batch 263 Loss 1.8349\n",
      "Epoch 1 Batch 264 Loss 2.2047\n",
      "Epoch 1 Batch 265 Loss 2.3567\n",
      "Epoch 1 Batch 266 Loss 1.9600\n",
      "Epoch 1 Batch 267 Loss 2.0331\n",
      "Epoch 1 Batch 268 Loss 2.5481\n",
      "Epoch 1 Batch 269 Loss 2.0746\n",
      "Epoch 1 Batch 270 Loss 3.3079\n",
      "Epoch 1 Batch 271 Loss 2.8941\n",
      "Epoch 1 Batch 272 Loss 3.2623\n",
      "Epoch 1 Batch 273 Loss 2.7173\n",
      "Epoch 1 Batch 274 Loss 3.1331\n",
      "Epoch 1 Batch 275 Loss 2.3264\n",
      "Epoch 1 Batch 276 Loss 2.5724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 277 Loss 2.1417\n",
      "Epoch 1 Batch 278 Loss 2.4971\n",
      "Epoch 1 Batch 279 Loss 2.2793\n",
      "Epoch 1 Batch 280 Loss 2.5576\n",
      "Epoch 1 Batch 281 Loss 3.0865\n",
      "Epoch 1 Batch 282 Loss 2.5766\n",
      "Epoch 1 Batch 283 Loss 2.5527\n",
      "Epoch 1 Batch 284 Loss 2.0635\n",
      "Epoch 1 Batch 285 Loss 2.2774\n",
      "Epoch 1 Batch 286 Loss 2.1888\n",
      "Epoch 1 Batch 287 Loss 1.8481\n",
      "Epoch 1 Batch 288 Loss 2.4249\n",
      "Epoch 1 Batch 289 Loss 2.6497\n",
      "Epoch 1 Batch 290 Loss 2.2709\n",
      "Epoch 1 Batch 291 Loss 2.1527\n",
      "Epoch 1 Batch 292 Loss 1.5704\n",
      "Epoch 1 Batch 293 Loss 2.3197\n",
      "Epoch 1 Batch 294 Loss 2.1963\n",
      "Epoch 1 Batch 295 Loss 2.2006\n",
      "Epoch 1 Batch 296 Loss 1.8862\n",
      "Epoch 1 Batch 297 Loss 1.8809\n",
      "Epoch 1 Batch 298 Loss 2.1006\n",
      "Epoch 1 Batch 299 Loss 2.0901\n",
      "Epoch 1 Batch 300 Loss 2.2854\n",
      "Epoch 1 Batch 301 Loss 2.1871\n",
      "Epoch 1 Batch 302 Loss 2.1028\n",
      "Epoch 1 Batch 303 Loss 1.9281\n",
      "Epoch 1 Batch 304 Loss 2.2708\n",
      "Epoch 1 Batch 305 Loss 2.1611\n",
      "Epoch 1 Batch 306 Loss 2.3706\n",
      "Epoch 1 Batch 307 Loss 2.4295\n",
      "Epoch 1 Batch 308 Loss 2.0004\n",
      "Epoch 1 Batch 309 Loss 3.0357\n",
      "Epoch 1 Batch 310 Loss 2.4272\n",
      "Epoch 1 Batch 311 Loss 2.0943\n",
      "Epoch 1 Batch 312 Loss 2.5125\n",
      "Epoch 1 Batch 313 Loss 2.2905\n",
      "Epoch 1 Batch 314 Loss 2.1751\n",
      "Epoch 1 Batch 315 Loss 2.2308\n",
      "Epoch 1 Batch 316 Loss 2.2205\n",
      "Epoch 1 Batch 317 Loss 2.6303\n",
      "Epoch 1 Batch 318 Loss 1.6845\n",
      "Epoch 1 Batch 319 Loss 1.9168\n",
      "Epoch 1 Batch 320 Loss 2.0572\n",
      "Epoch 1 Batch 321 Loss 3.0656\n",
      "Epoch 1 Batch 322 Loss 2.1374\n",
      "Epoch 1 Batch 323 Loss 2.3014\n",
      "Epoch 1 Batch 324 Loss 1.9490\n",
      "Epoch 1 Batch 325 Loss 2.5285\n",
      "Epoch 1 Batch 326 Loss 2.3350\n",
      "Epoch 1 Batch 327 Loss 2.0285\n",
      "Epoch 1 Batch 328 Loss 3.0035\n",
      "Epoch 1 Batch 329 Loss 2.1149\n",
      "Epoch 1 Batch 330 Loss 1.9884\n",
      "Epoch 1 Batch 331 Loss 2.2250\n",
      "Epoch 1 Batch 332 Loss 1.6997\n",
      "Epoch 1 Batch 333 Loss 2.0983\n",
      "Epoch 1 Batch 334 Loss 2.0645\n",
      "Epoch 1 Batch 335 Loss 2.1583\n",
      "Epoch 1 Batch 336 Loss 2.3848\n",
      "Epoch 1 Batch 337 Loss 2.2194\n",
      "Epoch 1 Batch 338 Loss 2.1109\n",
      "Epoch 1 Batch 339 Loss 3.0619\n",
      "Epoch 1 Batch 340 Loss 2.4987\n",
      "Epoch 1 Batch 341 Loss 2.3752\n",
      "Epoch 1 Batch 342 Loss 2.0935\n",
      "Epoch 1 Batch 343 Loss 1.9421\n",
      "Epoch 1 Batch 344 Loss 1.7782\n",
      "Epoch 1 Batch 345 Loss 2.1387\n",
      "Epoch 1 Batch 346 Loss 2.7379\n",
      "Epoch 1 Batch 347 Loss 2.4324\n",
      "Epoch 1 Batch 348 Loss 2.2708\n",
      "Epoch 1 Batch 349 Loss 2.1312\n",
      "Epoch 1 Batch 350 Loss 1.9419\n",
      "Epoch 1 Batch 351 Loss 2.1752\n",
      "Epoch 1 Batch 352 Loss 1.9782\n",
      "Epoch 1 Batch 353 Loss 1.9334\n",
      "Epoch 1 Batch 354 Loss 1.8686\n",
      "Epoch 1 Batch 355 Loss 2.3961\n",
      "Epoch 1 Batch 356 Loss 2.5180\n",
      "Epoch 1 Batch 357 Loss 1.9412\n",
      "Epoch 1 Batch 358 Loss 2.4753\n",
      "Epoch 1 Batch 359 Loss 2.4215\n",
      "Epoch 1 Batch 360 Loss 2.5468\n",
      "Epoch 1 Batch 361 Loss 2.7019\n",
      "Epoch 1 Batch 362 Loss 2.4028\n",
      "Epoch 1 Batch 363 Loss 1.8604\n",
      "Epoch 1 Batch 364 Loss 2.2232\n",
      "Epoch 1 Batch 365 Loss 2.7230\n",
      "Epoch 1 Batch 366 Loss 2.5560\n",
      "Epoch 1 Batch 367 Loss 2.6441\n",
      "Epoch 1 Batch 368 Loss 2.1038\n",
      "Epoch 1 Batch 369 Loss 2.7333\n",
      "Epoch 1 Batch 370 Loss 3.1904\n",
      "Epoch 1 Batch 371 Loss 2.2890\n",
      "Epoch 1 Batch 372 Loss 1.8500\n",
      "Epoch 1 Batch 373 Loss 1.7457\n",
      "Epoch 1 Batch 374 Loss 2.1658\n",
      "Epoch 1 Batch 375 Loss 1.8799\n",
      "Epoch 1 Batch 376 Loss 1.8320\n",
      "Epoch 1 Batch 377 Loss 1.9744\n",
      "Epoch 1 Batch 378 Loss 1.9999\n",
      "Epoch 1 Batch 379 Loss 1.8846\n",
      "Epoch 1 Batch 380 Loss 2.0407\n",
      "Epoch 1 Batch 381 Loss 2.3944\n",
      "Epoch 1 Batch 382 Loss 1.8841\n",
      "Epoch 1 Batch 383 Loss 1.8472\n",
      "Epoch 1 Batch 384 Loss 2.1662\n",
      "Epoch 1 Batch 385 Loss 2.3113\n",
      "Epoch 1 Batch 386 Loss 2.1471\n",
      "Epoch 1 Batch 387 Loss 2.3736\n",
      "Epoch 1 Batch 388 Loss 2.1655\n",
      "Epoch 1 Batch 389 Loss 1.9055\n",
      "Epoch 1 Batch 390 Loss 2.2375\n",
      "Epoch 1 Batch 391 Loss 2.0370\n",
      "Epoch 1 Batch 392 Loss 2.4158\n",
      "Epoch 1 Batch 393 Loss 2.0763\n",
      "Epoch 1 Batch 394 Loss 1.9449\n",
      "Epoch 1 Batch 395 Loss 2.2120\n",
      "Epoch 1 Batch 396 Loss 1.4928\n",
      "Epoch 1 Batch 397 Loss 1.9761\n",
      "Epoch 1 Batch 398 Loss 1.9584\n",
      "Epoch 1 Batch 399 Loss 2.3875\n",
      "Epoch 1 Batch 400 Loss 2.3255\n",
      "Epoch 1 Batch 401 Loss 2.4369\n",
      "Epoch 1 Batch 402 Loss 1.7858\n",
      "Epoch 1 Batch 403 Loss 1.7152\n",
      "Epoch 1 Batch 404 Loss 2.4038\n",
      "Epoch 1 Batch 405 Loss 1.7685\n",
      "Epoch 1 Batch 406 Loss 1.9001\n",
      "Epoch 1 Batch 407 Loss 2.3665\n",
      "Epoch 1 Batch 408 Loss 2.1761\n",
      "Epoch 1 Batch 409 Loss 2.2756\n",
      "Epoch 1 Batch 410 Loss 2.2372\n",
      "Epoch 1 Batch 411 Loss 2.3903\n",
      "Epoch 1 Batch 412 Loss 3.0452\n",
      "Epoch 1 Batch 413 Loss 2.6052\n",
      "Epoch 1 Batch 414 Loss 2.5349\n",
      "Epoch 1 Batch 415 Loss 2.4599\n",
      "Epoch 1 Batch 416 Loss 2.2790\n",
      "Epoch 1 Batch 417 Loss 2.0248\n",
      "Epoch 1 Batch 418 Loss 2.0660\n",
      "Epoch 1 Batch 419 Loss 1.9377\n",
      "Epoch 1 Batch 420 Loss 2.3880\n",
      "Epoch 1 Batch 421 Loss 2.7681\n",
      "Epoch 1 Batch 422 Loss 2.3222\n",
      "Epoch 1 Batch 423 Loss 2.7391\n",
      "Epoch 1 Batch 424 Loss 2.1209\n",
      "Epoch 1 Batch 425 Loss 2.5053\n",
      "Epoch 1 Batch 426 Loss 2.5349\n",
      "Epoch 1 Batch 427 Loss 2.4935\n",
      "Epoch 1 Batch 428 Loss 2.6668\n",
      "Epoch 1 Batch 429 Loss 2.5590\n",
      "Epoch 1 Batch 430 Loss 2.1405\n",
      "Epoch 1 Batch 431 Loss 2.2926\n",
      "Epoch 1 Batch 432 Loss 2.3615\n",
      "Epoch 1 Batch 433 Loss 2.5011\n",
      "Epoch 1 Batch 434 Loss 2.1300\n",
      "Epoch 1 Batch 435 Loss 1.8791\n",
      "Epoch 1 Batch 436 Loss 2.2428\n",
      "Epoch 1 Batch 437 Loss 2.5067\n",
      "Epoch 1 Batch 438 Loss 2.2359\n",
      "Epoch 1 Batch 439 Loss 2.3350\n",
      "Epoch 1 Batch 440 Loss 2.4635\n",
      "Epoch 1 Batch 441 Loss 2.4526\n",
      "Epoch 1 Batch 442 Loss 2.5592\n",
      "Epoch 1 Batch 443 Loss 2.1792\n",
      "Epoch 1 Batch 444 Loss 2.1106\n",
      "Epoch 1 Batch 445 Loss 2.0012\n",
      "Epoch 1 Batch 446 Loss 2.7910\n",
      "Epoch 1 Batch 447 Loss 2.5213\n",
      "Epoch 1 Batch 448 Loss 3.2334\n",
      "Epoch 1 Batch 449 Loss 2.2120\n",
      "Epoch 1 Batch 450 Loss 2.0546\n",
      "Epoch 1 Batch 451 Loss 2.7081\n",
      "Epoch 1 Batch 452 Loss 2.5833\n",
      "Epoch 1 Batch 453 Loss 2.5555\n",
      "Epoch 1 Batch 454 Loss 2.3804\n",
      "Epoch 1 Batch 455 Loss 1.9595\n",
      "Epoch 1 Batch 456 Loss 1.9275\n",
      "Epoch 1 Batch 457 Loss 2.2574\n",
      "Epoch 1 Batch 458 Loss 2.2577\n",
      "Epoch 1 Batch 459 Loss 1.7327\n",
      "Epoch 1 Batch 460 Loss 1.8770\n",
      "Epoch 1 Batch 461 Loss 1.8954\n",
      "Epoch 1 Batch 462 Loss 2.4764\n",
      "Epoch 1 Batch 463 Loss 2.4006\n",
      "Epoch 1 Batch 464 Loss 2.9568\n",
      "Epoch 1 Batch 465 Loss 2.3556\n",
      "Epoch 1 Batch 466 Loss 2.3063\n",
      "Epoch 1 Batch 467 Loss 2.1708\n",
      "Epoch 1 Batch 468 Loss 2.8425\n",
      "Epoch 1 Batch 469 Loss 2.3147\n",
      "Epoch 1 Batch 470 Loss 2.5441\n",
      "Epoch 1 Batch 471 Loss 2.2875\n",
      "Epoch 1 Batch 472 Loss 2.3583\n",
      "Epoch 1 Batch 473 Loss 2.0175\n",
      "Epoch 1 Batch 474 Loss 2.1502\n",
      "Epoch 1 Batch 475 Loss 2.2345\n",
      "Epoch 1 Batch 476 Loss 1.9642\n",
      "Epoch 1 Batch 477 Loss 1.8419\n",
      "Epoch 1 Batch 478 Loss 1.8094\n",
      "Epoch 1 Batch 479 Loss 2.0667\n",
      "Epoch 1 Batch 480 Loss 2.0031\n",
      "Epoch 1 Batch 481 Loss 1.7701\n",
      "Epoch 1 Batch 482 Loss 2.3582\n",
      "Epoch 1 Batch 483 Loss 2.4900\n",
      "Epoch 1 Batch 484 Loss 2.3969\n",
      "Epoch 1 Batch 485 Loss 2.2176\n",
      "Epoch 1 Batch 486 Loss 2.0208\n",
      "Epoch 1 Batch 487 Loss 2.0092\n",
      "Epoch 1 Batch 488 Loss 1.8065\n",
      "Epoch 1 Batch 489 Loss 1.5760\n",
      "Epoch 1 Batch 490 Loss 1.8235\n",
      "Epoch 1 Batch 491 Loss 2.0373\n",
      "Epoch 1 Batch 492 Loss 2.4134\n",
      "Epoch 1 Batch 493 Loss 2.5452\n",
      "Epoch 1 Batch 494 Loss 2.1062\n",
      "Epoch 1 Batch 495 Loss 2.2461\n",
      "Epoch 1 Batch 496 Loss 1.3736\n",
      "Epoch 1 Batch 497 Loss 1.6617\n",
      "Epoch 1 Batch 498 Loss 1.7263\n",
      "Epoch 1 Batch 499 Loss 1.8966\n",
      "Epoch 1 Batch 500 Loss 2.3791\n",
      "Epoch 1 Batch 501 Loss 1.6454\n",
      "Epoch 1 Batch 502 Loss 1.5716\n",
      "Epoch 1 Batch 503 Loss 1.5353\n",
      "Epoch 1 Batch 504 Loss 1.8905\n",
      "Epoch 1 Batch 505 Loss 1.7621\n",
      "Epoch 1 Batch 506 Loss 1.8902\n",
      "Epoch 1 Batch 507 Loss 2.3730\n",
      "Epoch 1 Batch 508 Loss 2.3595\n",
      "Epoch 1 Batch 509 Loss 2.6784\n",
      "Epoch 1 Batch 510 Loss 2.2044\n",
      "Epoch 1 Batch 511 Loss 2.4715\n",
      "Epoch 1 Batch 512 Loss 1.7880\n",
      "Epoch 1 Batch 513 Loss 1.9965\n",
      "Epoch 1 Batch 514 Loss 1.5634\n",
      "Epoch 1 Batch 515 Loss 2.3137\n",
      "Epoch 1 Batch 516 Loss 2.3265\n",
      "Epoch 1 Batch 517 Loss 2.4522\n",
      "Epoch 1 Batch 518 Loss 2.0517\n",
      "Epoch 1 Batch 519 Loss 1.7234\n",
      "Epoch 1 Batch 520 Loss 1.5793\n",
      "Epoch 1 Batch 521 Loss 1.8269\n",
      "Epoch 1 Batch 522 Loss 1.9597\n",
      "Epoch 1 Batch 523 Loss 2.0225\n",
      "Epoch 1 Batch 524 Loss 1.4794\n",
      "Epoch 1 Batch 525 Loss 1.8185\n",
      "Epoch 1 Batch 526 Loss 1.8371\n",
      "Epoch 1 Batch 527 Loss 1.7830\n",
      "Epoch 1 Batch 528 Loss 2.4234\n",
      "Epoch 1 Batch 529 Loss 1.9898\n",
      "Epoch 1 Batch 530 Loss 2.1342\n",
      "Epoch 1 Batch 531 Loss 2.4091\n",
      "Epoch 1 Batch 532 Loss 2.4077\n",
      "Epoch 1 Batch 533 Loss 2.6930\n",
      "Epoch 1 Batch 534 Loss 2.4324\n",
      "Epoch 1 Batch 535 Loss 2.6154\n",
      "Epoch 1 Batch 536 Loss 2.1221\n",
      "Epoch 1 Batch 537 Loss 2.0784\n",
      "Epoch 1 Batch 538 Loss 2.0967\n",
      "Epoch 1 Batch 539 Loss 1.7323\n",
      "Epoch 1 Batch 540 Loss 1.8805\n",
      "Epoch 1 Batch 541 Loss 1.7295\n",
      "Epoch 1 Batch 542 Loss 1.8436\n",
      "Epoch 1 Batch 543 Loss 2.0298\n",
      "Epoch 1 Batch 544 Loss 2.4130\n",
      "Epoch 1 Batch 545 Loss 2.1396\n",
      "Epoch 1 Batch 546 Loss 2.4033\n",
      "Epoch 1 Batch 547 Loss 2.7422\n",
      "Epoch 1 Batch 548 Loss 2.5110\n",
      "Epoch 1 Batch 549 Loss 2.8029\n",
      "Epoch 1 Batch 550 Loss 2.3729\n",
      "Epoch 1 Batch 551 Loss 2.2840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 552 Loss 1.8525\n",
      "Epoch 1 Batch 553 Loss 2.3659\n",
      "Epoch 1 Batch 554 Loss 2.1962\n",
      "Epoch 1 Batch 555 Loss 2.3052\n",
      "Epoch 1 Batch 556 Loss 2.2576\n",
      "Epoch 1 Batch 557 Loss 1.6550\n",
      "Epoch 1 Batch 558 Loss 2.0800\n",
      "Epoch 1 Batch 559 Loss 2.0328\n",
      "Epoch 1 Batch 560 Loss 2.1066\n",
      "Epoch 1 Batch 561 Loss 2.2905\n",
      "Epoch 1 Batch 562 Loss 2.2658\n",
      "Epoch 1 Batch 563 Loss 2.0765\n",
      "Epoch 1 Batch 564 Loss 2.2018\n",
      "Epoch 1 Batch 565 Loss 2.1462\n",
      "Epoch 1 Batch 566 Loss 1.7480\n",
      "Epoch 1 Batch 567 Loss 2.2310\n",
      "Epoch 1 Batch 568 Loss 2.2628\n",
      "Epoch 1 Batch 569 Loss 1.9961\n",
      "Epoch 1 Batch 570 Loss 2.0219\n",
      "Epoch 1 Batch 571 Loss 1.6372\n",
      "Epoch 1 Batch 572 Loss 1.5511\n",
      "Epoch 1 Batch 573 Loss 2.0662\n",
      "Epoch 1 Batch 574 Loss 2.0943\n",
      "Epoch 1 Batch 575 Loss 2.4267\n",
      "Epoch 1 Batch 576 Loss 2.6431\n",
      "Epoch 1 Batch 577 Loss 2.3985\n",
      "Epoch 1 Batch 578 Loss 2.2124\n",
      "Epoch 1 Batch 579 Loss 1.8946\n",
      "Epoch 1 Batch 580 Loss 2.3972\n",
      "Epoch 1 Batch 581 Loss 2.5207\n",
      "Epoch 1 Batch 582 Loss 2.0755\n",
      "Epoch 1 Batch 583 Loss 1.9613\n",
      "Epoch 1 Batch 584 Loss 1.8972\n",
      "Epoch 1 Batch 585 Loss 1.5778\n",
      "Epoch 1 Batch 586 Loss 2.0340\n",
      "Epoch 1 Batch 587 Loss 2.2863\n",
      "Epoch 1 Batch 588 Loss 2.0245\n",
      "Epoch 1 Batch 589 Loss 1.8521\n",
      "Epoch 1 Batch 590 Loss 1.9916\n",
      "Epoch 1 Batch 591 Loss 2.4170\n",
      "Epoch 1 Batch 592 Loss 2.2637\n",
      "Epoch 1 Batch 593 Loss 2.4938\n",
      "Epoch 1 Batch 594 Loss 2.3823\n",
      "Epoch 1 Batch 595 Loss 2.3697\n",
      "Epoch 1 Batch 596 Loss 1.8976\n",
      "Epoch 1 Batch 597 Loss 2.2261\n",
      "Epoch 1 Batch 598 Loss 2.2544\n",
      "Epoch 1 Batch 599 Loss 2.5879\n",
      "Epoch 1 Batch 600 Loss 2.2624\n",
      "Epoch 1 Batch 601 Loss 1.8796\n",
      "Epoch 1 Batch 602 Loss 2.0637\n",
      "Epoch 1 Batch 603 Loss 2.2423\n",
      "Epoch 1 Batch 604 Loss 2.1814\n",
      "Epoch 1 Batch 605 Loss 2.3187\n",
      "Epoch 1 Batch 606 Loss 2.6319\n",
      "Epoch 1 Batch 607 Loss 2.2989\n",
      "Epoch 1 Batch 608 Loss 2.3196\n",
      "Epoch 1 Batch 609 Loss 2.3645\n",
      "Epoch 1 Batch 610 Loss 1.7747\n",
      "Epoch 1 Batch 611 Loss 2.3301\n",
      "Epoch 1 Batch 612 Loss 1.7979\n",
      "Epoch 1 Batch 613 Loss 2.2423\n",
      "Epoch 1 Batch 614 Loss 1.9653\n",
      "Epoch 1 Batch 615 Loss 2.3139\n",
      "Epoch 1 Batch 616 Loss 2.4982\n",
      "Epoch 1 Batch 617 Loss 2.2282\n",
      "Epoch 1 Batch 618 Loss 2.4466\n",
      "Epoch 1 Batch 619 Loss 2.0703\n",
      "Epoch 1 Batch 620 Loss 2.0022\n",
      "Epoch 1 Batch 621 Loss 1.6052\n",
      "Epoch 1 Batch 622 Loss 2.0555\n",
      "Epoch 1 Batch 623 Loss 1.8878\n",
      "Epoch 1 Batch 624 Loss 2.0595\n",
      "Epoch 1 Batch 625 Loss 1.5678\n",
      "Epoch 1 Batch 626 Loss 1.7914\n",
      "Epoch 1 Batch 627 Loss 2.3212\n",
      "Epoch 1 Batch 628 Loss 2.1929\n",
      "Epoch 1 Batch 629 Loss 2.7788\n",
      "Epoch 1 Batch 630 Loss 2.7586\n",
      "Epoch 1 Batch 631 Loss 2.4912\n",
      "Epoch 1 Batch 632 Loss 2.2440\n",
      "Epoch 1 Batch 633 Loss 2.7492\n",
      "Epoch 1 Batch 634 Loss 1.9392\n",
      "Epoch 1 Batch 635 Loss 2.1175\n",
      "Epoch 1 Batch 636 Loss 2.1713\n",
      "Epoch 1 Batch 637 Loss 2.1974\n",
      "Epoch 1 Batch 638 Loss 2.0296\n",
      "Epoch 1 Batch 639 Loss 2.3955\n",
      "Epoch 1 Batch 640 Loss 1.8684\n",
      "Epoch 1 Batch 641 Loss 2.0530\n",
      "Epoch 1 Batch 642 Loss 1.9744\n",
      "Epoch 1 Batch 643 Loss 2.0174\n",
      "Epoch 1 Batch 644 Loss 2.0409\n",
      "Epoch 1 Batch 645 Loss 1.6767\n",
      "Epoch 1 Batch 646 Loss 1.7834\n",
      "Epoch 1 Batch 647 Loss 2.3121\n",
      "Epoch 1 Batch 648 Loss 2.3603\n",
      "Epoch 1 Batch 649 Loss 2.4106\n",
      "Epoch 1 Batch 650 Loss 2.1302\n",
      "Epoch 1 Batch 651 Loss 2.0066\n",
      "Epoch 1 Batch 652 Loss 2.2808\n",
      "Epoch 1 Batch 653 Loss 2.1545\n",
      "Epoch 1 Batch 654 Loss 2.1569\n",
      "Epoch 1 Batch 655 Loss 2.5540\n",
      "Epoch 1 Batch 656 Loss 2.5932\n",
      "Epoch 1 Batch 657 Loss 2.8820\n",
      "Epoch 1 Batch 658 Loss 2.4998\n",
      "Epoch 1 Batch 659 Loss 2.4681\n",
      "Epoch 1 Batch 660 Loss 2.1532\n",
      "Epoch 1 Batch 661 Loss 2.1644\n",
      "Epoch 1 Batch 662 Loss 2.5755\n",
      "Epoch 1 Batch 663 Loss 2.5179\n",
      "Epoch 1 Batch 664 Loss 2.2668\n",
      "Epoch 1 Batch 665 Loss 1.9480\n",
      "Epoch 1 Batch 666 Loss 2.1632\n",
      "Epoch 1 Batch 667 Loss 2.5225\n",
      "Epoch 1 Batch 668 Loss 2.1242\n",
      "Epoch 1 Batch 669 Loss 1.6335\n",
      "Epoch 1 Batch 670 Loss 1.9616\n",
      "Epoch 1 Batch 671 Loss 2.3193\n",
      "Epoch 1 Batch 672 Loss 2.5357\n",
      "Epoch 1 Batch 673 Loss 2.1651\n",
      "Epoch 1 Batch 674 Loss 1.8751\n",
      "Epoch 1 Batch 675 Loss 1.9694\n",
      "Epoch 1 Batch 676 Loss 2.0450\n",
      "Epoch 1 Batch 677 Loss 2.1784\n",
      "Epoch 1 Batch 678 Loss 2.2287\n",
      "Epoch 1 Batch 679 Loss 1.9423\n",
      "Epoch 1 Batch 680 Loss 2.3828\n",
      "Epoch 1 Batch 681 Loss 2.5688\n",
      "Epoch 1 Batch 682 Loss 2.1333\n",
      "Epoch 1 Batch 683 Loss 1.7103\n",
      "Epoch 1 Batch 684 Loss 2.3152\n",
      "Epoch 1 Batch 685 Loss 2.2962\n",
      "Epoch 1 Batch 686 Loss 1.5189\n",
      "Epoch 1 Batch 687 Loss 1.8856\n",
      "Epoch 1 Batch 688 Loss 1.9087\n",
      "Epoch 1 Batch 689 Loss 2.2717\n",
      "Epoch 1 Batch 690 Loss 2.7196\n",
      "Epoch 1 Batch 691 Loss 2.6986\n",
      "Epoch 1 Batch 692 Loss 2.6450\n",
      "Epoch 1 Batch 693 Loss 2.2073\n",
      "Epoch 1 Batch 694 Loss 2.2791\n",
      "Epoch 1 Batch 695 Loss 2.7922\n",
      "Epoch 1 Batch 696 Loss 1.6978\n",
      "Epoch 1 Batch 697 Loss 1.9686\n",
      "Epoch 1 Batch 698 Loss 2.1100\n",
      "Epoch 1 Batch 699 Loss 2.1431\n",
      "Epoch 1 Batch 700 Loss 1.9452\n",
      "Epoch 1 Batch 701 Loss 2.2527\n",
      "Epoch 1 Batch 702 Loss 1.6634\n",
      "Epoch 1 Batch 703 Loss 1.8039\n",
      "Epoch 1 Batch 704 Loss 2.0199\n",
      "Epoch 1 Batch 705 Loss 1.7126\n",
      "Epoch 1 Batch 706 Loss 2.1383\n",
      "Epoch 1 Batch 707 Loss 2.4871\n",
      "Epoch 1 Batch 708 Loss 3.0657\n",
      "Epoch 1 Batch 709 Loss 2.6030\n",
      "Epoch 1 Batch 710 Loss 2.1265\n",
      "Epoch 1 Batch 711 Loss 1.8732\n",
      "Epoch 1 Batch 712 Loss 1.6048\n",
      "Epoch 1 Batch 713 Loss 1.9610\n",
      "Epoch 1 Batch 714 Loss 1.9671\n",
      "Epoch 1 Batch 715 Loss 2.1735\n",
      "Epoch 1 Batch 716 Loss 2.0480\n",
      "Epoch 1 Batch 717 Loss 2.1685\n",
      "Epoch 1 Batch 718 Loss 2.1529\n",
      "Epoch 1 Batch 719 Loss 2.1561\n",
      "Epoch 1 Batch 720 Loss 2.1280\n",
      "Epoch 1 Batch 721 Loss 2.9845\n",
      "Epoch 1 Batch 722 Loss 2.2742\n",
      "Epoch 1 Batch 723 Loss 2.3296\n",
      "Epoch 1 Batch 724 Loss 2.4709\n",
      "Epoch 1 Batch 725 Loss 2.3302\n",
      "Epoch 1 Batch 726 Loss 2.8263\n",
      "Epoch 1 Batch 727 Loss 2.0044\n",
      "Epoch 1 Batch 728 Loss 1.5533\n",
      "Epoch 1 Batch 729 Loss 1.6242\n",
      "Epoch 1 Batch 730 Loss 1.6971\n",
      "Epoch 1 Batch 731 Loss 1.7702\n",
      "Epoch 1 Batch 732 Loss 1.7629\n",
      "Epoch 1 Batch 733 Loss 1.6160\n",
      "Epoch 1 Batch 734 Loss 1.3616\n",
      "Epoch 1 Batch 735 Loss 1.7743\n",
      "Epoch 1 Batch 736 Loss 1.7997\n",
      "Epoch 1 Batch 737 Loss 1.4870\n",
      "Epoch 1 Batch 738 Loss 1.6722\n",
      "Epoch 1 Batch 739 Loss 1.0830\n",
      "Epoch 1 Batch 740 Loss 1.7724\n",
      "Epoch 1 Batch 741 Loss 1.7462\n",
      "Epoch 1 Batch 742 Loss 1.8485\n",
      "Epoch 1 Batch 743 Loss 2.1361\n",
      "Epoch 1 Batch 744 Loss 2.6437\n",
      "Epoch 1 Batch 745 Loss 2.2584\n",
      "Epoch 1 Batch 746 Loss 2.1171\n",
      "Epoch 1 Batch 747 Loss 2.3839\n",
      "Epoch 1 Batch 748 Loss 1.9959\n",
      "Epoch 1 Batch 749 Loss 1.9688\n",
      "Epoch 1 Batch 750 Loss 2.1514\n",
      "Epoch 1 Batch 751 Loss 2.0290\n",
      "Epoch 1 Batch 752 Loss 1.7521\n",
      "Epoch 1 Batch 753 Loss 1.7579\n",
      "Epoch 1 Batch 754 Loss 1.7221\n",
      "Epoch 1 Batch 755 Loss 1.9422\n",
      "Epoch 1 Batch 756 Loss 1.9735\n",
      "Epoch 1 Batch 757 Loss 1.9653\n",
      "Epoch 1 Batch 758 Loss 2.2766\n",
      "Epoch 1 Batch 759 Loss 2.3276\n",
      "Epoch 1 Batch 760 Loss 1.9686\n",
      "Epoch 1 Batch 761 Loss 2.4504\n",
      "Epoch 1 Batch 762 Loss 2.4138\n",
      "Epoch 1 Batch 763 Loss 2.3510\n",
      "Epoch 1 Batch 764 Loss 2.6051\n",
      "Epoch 1 Batch 765 Loss 2.0219\n",
      "Epoch 1 Batch 766 Loss 2.4745\n",
      "Epoch 1 Batch 767 Loss 2.4103\n",
      "Epoch 1 Batch 768 Loss 2.9425\n",
      "Epoch 1 Batch 769 Loss 2.3583\n",
      "Epoch 1 Batch 770 Loss 2.4320\n",
      "Epoch 1 Batch 771 Loss 2.3136\n",
      "Epoch 1 Batch 772 Loss 2.9305\n",
      "Epoch 1 Batch 773 Loss 2.5858\n",
      "Epoch 1 Batch 774 Loss 2.8494\n",
      "Epoch 1 Batch 775 Loss 2.2156\n",
      "Epoch 1 Batch 776 Loss 2.4435\n",
      "Epoch 1 Batch 777 Loss 1.9346\n",
      "Epoch 1 Batch 778 Loss 2.4179\n",
      "Epoch 1 Batch 779 Loss 1.9595\n",
      "Epoch 1 Batch 780 Loss 1.6706\n",
      "Epoch 1 Batch 781 Loss 2.2936\n",
      "Epoch 1 Batch 782 Loss 2.0690\n",
      "Epoch 1 Batch 783 Loss 2.3203\n",
      "Epoch 1 Batch 784 Loss 1.7964\n",
      "Epoch 1 Batch 785 Loss 2.2027\n",
      "Epoch 1 Batch 786 Loss 1.6144\n",
      "Epoch 1 Batch 787 Loss 2.3649\n",
      "Epoch 1 Batch 788 Loss 2.2832\n",
      "Epoch 1 Batch 789 Loss 2.0392\n",
      "Epoch 1 Batch 790 Loss 1.8430\n",
      "Epoch 1 Batch 791 Loss 2.2414\n",
      "Epoch 1 Batch 792 Loss 2.2327\n",
      "Epoch 1 Batch 793 Loss 1.5424\n",
      "Epoch 1 Batch 794 Loss 1.3860\n",
      "Epoch 1 Batch 795 Loss 1.6444\n",
      "Epoch 1 Batch 796 Loss 1.5289\n",
      "Epoch 1 Batch 797 Loss 2.0428\n",
      "Epoch 1 Batch 798 Loss 1.5362\n",
      "Epoch 1 Batch 799 Loss 1.8335\n",
      "Epoch 1 Batch 800 Loss 1.8685\n",
      "Epoch 1 Batch 801 Loss 2.2267\n",
      "Epoch 1 Batch 802 Loss 1.5246\n",
      "Epoch 1 Batch 803 Loss 1.7434\n",
      "Epoch 1 Batch 804 Loss 2.3070\n",
      "Epoch 1 Batch 805 Loss 1.9001\n",
      "Epoch 1 Batch 806 Loss 1.8003\n",
      "Epoch 1 Batch 807 Loss 2.4659\n",
      "Epoch 1 Batch 808 Loss 2.3136\n",
      "Epoch 1 Batch 809 Loss 2.2472\n",
      "Epoch 1 Batch 810 Loss 2.4518\n",
      "Epoch 1 Batch 811 Loss 2.1918\n",
      "Epoch 1 Batch 812 Loss 2.1082\n",
      "Epoch 1 Batch 813 Loss 2.4880\n",
      "Epoch 1 Batch 814 Loss 2.0983\n",
      "Epoch 1 Batch 815 Loss 2.3177\n",
      "Epoch 1 Batch 816 Loss 2.3491\n",
      "Epoch 1 Batch 817 Loss 2.0042\n",
      "Epoch 1 Batch 818 Loss 2.6253\n",
      "Epoch 1 Batch 819 Loss 2.3997\n",
      "Epoch 1 Batch 820 Loss 3.0318\n",
      "Epoch 1 Batch 821 Loss 2.1221\n",
      "Epoch 1 Batch 822 Loss 2.2483\n",
      "Epoch 1 Batch 823 Loss 2.0746\n",
      "Epoch 1 Batch 824 Loss 1.4314\n",
      "Epoch 1 Batch 825 Loss 1.8064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 826 Loss 2.1212\n",
      "Epoch 1 Batch 827 Loss 1.8558\n",
      "Epoch 1 Batch 828 Loss 2.0776\n",
      "Epoch 1 Batch 829 Loss 2.3757\n",
      "Epoch 1 Batch 830 Loss 2.9994\n",
      "Epoch 1 Batch 831 Loss 2.0989\n",
      "Epoch 1 Batch 832 Loss 2.4157\n",
      "Epoch 1 Batch 833 Loss 1.9746\n",
      "Epoch 1 Batch 834 Loss 1.6071\n",
      "Epoch 1 Batch 835 Loss 2.4467\n",
      "Epoch 1 Batch 836 Loss 1.7554\n",
      "Epoch 1 Batch 837 Loss 2.0316\n",
      "Epoch 1 Batch 838 Loss 1.7289\n",
      "Epoch 1 Batch 839 Loss 1.9930\n",
      "Epoch 1 Batch 840 Loss 2.4718\n",
      "Epoch 1 Batch 841 Loss 2.5023\n",
      "Epoch 1 Batch 842 Loss 2.0485\n",
      "Epoch 1 Batch 843 Loss 2.0092\n",
      "Epoch 1 Batch 844 Loss 2.0314\n",
      "Epoch 1 Batch 845 Loss 2.0255\n",
      "Epoch 1 Batch 846 Loss 2.0997\n",
      "Epoch 1 Batch 847 Loss 1.7341\n",
      "Epoch 1 Batch 848 Loss 2.0275\n",
      "Epoch 1 Batch 849 Loss 1.8715\n",
      "Epoch 1 Batch 850 Loss 2.1985\n",
      "Epoch 1 Batch 851 Loss 1.7941\n",
      "Epoch 1 Batch 852 Loss 2.3432\n",
      "Epoch 1 Batch 853 Loss 2.4601\n",
      "Epoch 1 Batch 854 Loss 2.2956\n",
      "Epoch 1 Batch 855 Loss 1.6971\n",
      "Epoch 1 Batch 856 Loss 1.6715\n",
      "Epoch 1 Batch 857 Loss 1.8224\n",
      "Epoch 1 Batch 858 Loss 2.0515\n",
      "Epoch 1 Batch 859 Loss 2.0390\n",
      "Epoch 1 Batch 860 Loss 1.9064\n",
      "Epoch 1 Batch 861 Loss 2.3547\n",
      "Epoch 1 Batch 862 Loss 2.3969\n",
      "Epoch 1 Batch 863 Loss 2.3493\n",
      "Epoch 1 Batch 864 Loss 2.1218\n",
      "Epoch 1 Batch 865 Loss 2.4976\n",
      "Epoch 1 Batch 866 Loss 2.3448\n",
      "Epoch 1 Batch 867 Loss 1.7818\n",
      "Epoch 1 Batch 868 Loss 2.4196\n",
      "Epoch 1 Batch 869 Loss 2.0723\n",
      "Epoch 1 Batch 870 Loss 1.9971\n",
      "Epoch 1 Batch 871 Loss 1.9488\n",
      "Epoch 1 Batch 872 Loss 1.6431\n",
      "Epoch 1 Batch 873 Loss 1.6429\n",
      "Epoch 1 Batch 874 Loss 1.4335\n",
      "Epoch 1 Batch 875 Loss 1.4238\n",
      "Epoch 1 Batch 876 Loss 1.6854\n",
      "Epoch 1 Batch 877 Loss 2.0444\n",
      "Epoch 1 Batch 878 Loss 1.7062\n",
      "Epoch 1 Batch 879 Loss 2.1609\n",
      "Epoch 1 Batch 880 Loss 2.0943\n",
      "Epoch 1 Batch 881 Loss 1.8683\n",
      "Epoch 1 Batch 882 Loss 1.9446\n",
      "Epoch 1 Batch 883 Loss 1.6336\n",
      "Epoch 1 Batch 884 Loss 2.5277\n",
      "Epoch 1 Batch 885 Loss 2.1730\n",
      "Epoch 1 Batch 886 Loss 1.9815\n",
      "Epoch 1 Batch 887 Loss 1.6212\n",
      "Epoch 1 Batch 888 Loss 1.9788\n",
      "Epoch 1 Batch 889 Loss 1.8978\n",
      "Epoch 1 Batch 890 Loss 1.6024\n",
      "Epoch 1 Batch 891 Loss 1.9578\n",
      "Epoch 1 Batch 892 Loss 1.8239\n",
      "Epoch 1 Batch 893 Loss 1.7974\n",
      "Epoch 1 Batch 894 Loss 1.7538\n",
      "Epoch 1 Batch 895 Loss 1.7739\n",
      "Epoch 1 Batch 896 Loss 1.7759\n",
      "Epoch 1 Batch 897 Loss 1.8942\n",
      "Epoch 1 Batch 898 Loss 1.7583\n",
      "Epoch 1 Batch 899 Loss 1.9397\n",
      "Epoch 1 Batch 900 Loss 1.4924\n",
      "Epoch 1 Batch 901 Loss 2.2278\n",
      "Epoch 1 Batch 902 Loss 2.2525\n",
      "Epoch 1 Batch 903 Loss 2.1225\n",
      "Epoch 1 Batch 904 Loss 2.3955\n",
      "Epoch 1 Batch 905 Loss 2.1950\n",
      "Epoch 1 Batch 906 Loss 2.4741\n",
      "Epoch 1 Batch 907 Loss 2.4266\n",
      "Epoch 1 Batch 908 Loss 2.2340\n",
      "Epoch 1 Batch 909 Loss 2.1700\n",
      "Epoch 1 Batch 910 Loss 2.2908\n",
      "Epoch 1 Batch 911 Loss 2.1347\n",
      "Epoch 1 Batch 912 Loss 2.5314\n",
      "Epoch 1 Batch 913 Loss 2.5463\n",
      "Epoch 1 Batch 914 Loss 2.2889\n",
      "Epoch 1 Batch 915 Loss 2.2261\n",
      "Epoch 1 Batch 916 Loss 2.3260\n",
      "Epoch 1 Batch 917 Loss 1.7457\n",
      "Epoch 1 Batch 918 Loss 2.4429\n",
      "Epoch 1 Batch 919 Loss 2.1006\n",
      "Epoch 1 Batch 920 Loss 2.5141\n",
      "Epoch 1 Batch 921 Loss 2.1585\n",
      "Epoch 1 Batch 922 Loss 1.7170\n",
      "Epoch 1 Batch 923 Loss 1.8823\n",
      "Epoch 1 Batch 924 Loss 1.7760\n",
      "Epoch 1 Batch 925 Loss 2.7093\n",
      "Epoch 1 Batch 926 Loss 2.3396\n",
      "Epoch 1 Batch 927 Loss 2.4062\n",
      "Epoch 1 Batch 928 Loss 2.4543\n",
      "Epoch 1 Batch 929 Loss 2.6289\n",
      "Epoch 1 Batch 930 Loss 2.8570\n",
      "Epoch 1 Batch 931 Loss 2.3965\n",
      "Epoch 1 Batch 932 Loss 2.4683\n",
      "Epoch 1 Batch 933 Loss 2.2443\n",
      "Epoch 1 Batch 934 Loss 2.8250\n",
      "Epoch 1 Batch 935 Loss 2.3619\n",
      "Epoch 1 Batch 936 Loss 2.7351\n",
      "Epoch 1 Batch 937 Loss 2.4441\n",
      "Epoch 1 Batch 938 Loss 2.3494\n",
      "Epoch 1 Batch 939 Loss 2.1794\n",
      "Epoch 1 Batch 940 Loss 1.8147\n",
      "Epoch 1 Batch 941 Loss 2.1396\n",
      "Epoch 1 Batch 942 Loss 1.8446\n",
      "Epoch 1 Batch 943 Loss 1.6152\n",
      "Epoch 1 Batch 944 Loss 1.7342\n",
      "Epoch 1 Batch 945 Loss 2.0794\n",
      "Epoch 1 Batch 946 Loss 2.4743\n",
      "Epoch 1 Batch 947 Loss 1.9658\n",
      "Epoch 1 Batch 948 Loss 1.9839\n",
      "Epoch 1 Batch 949 Loss 2.1747\n",
      "Epoch 1 Batch 950 Loss 2.5878\n",
      "Epoch 1 Batch 951 Loss 2.2511\n",
      "Epoch 1 Batch 952 Loss 1.8490\n",
      "Epoch 1 Batch 953 Loss 2.3306\n",
      "Epoch 1 Batch 954 Loss 2.1028\n",
      "Epoch 1 Batch 955 Loss 2.3605\n",
      "Epoch 1 Batch 956 Loss 1.7978\n",
      "Epoch 1 Batch 957 Loss 2.3300\n",
      "Epoch 1 Batch 958 Loss 2.2094\n",
      "Epoch 1 Batch 959 Loss 1.9960\n",
      "Epoch 1 Batch 960 Loss 1.7825\n",
      "Epoch 1 Batch 961 Loss 2.2673\n",
      "Epoch 1 Batch 962 Loss 2.0155\n",
      "Epoch 1 Batch 963 Loss 2.3542\n",
      "Epoch 1 Batch 964 Loss 2.1071\n",
      "Epoch 1 Batch 965 Loss 2.3299\n",
      "Epoch 1 Batch 966 Loss 2.5253\n",
      "Epoch 1 Batch 967 Loss 2.3181\n",
      "Epoch 1 Batch 968 Loss 1.8614\n",
      "Epoch 1 Batch 969 Loss 1.9916\n",
      "Epoch 1 Batch 970 Loss 1.8463\n",
      "Epoch 1 Batch 971 Loss 2.1719\n",
      "Epoch 1 Batch 972 Loss 1.7526\n",
      "Epoch 1 Batch 973 Loss 1.9762\n",
      "Epoch 1 Batch 974 Loss 2.2132\n",
      "Epoch 1 Batch 975 Loss 1.7836\n",
      "Epoch 1 Batch 976 Loss 1.7922\n",
      "Epoch 1 Batch 977 Loss 1.6472\n",
      "Epoch 1 Batch 978 Loss 1.7403\n",
      "Epoch 1 Batch 979 Loss 1.9284\n",
      "Epoch 1 Batch 980 Loss 2.1083\n",
      "Epoch 1 Batch 981 Loss 1.9260\n",
      "Epoch 1 Batch 982 Loss 1.7080\n",
      "Epoch 1 Batch 983 Loss 1.8797\n",
      "Epoch 1 Batch 984 Loss 2.0305\n",
      "Epoch 1 Batch 985 Loss 2.2423\n",
      "Epoch 1 Batch 986 Loss 2.1642\n",
      "Epoch 1 Batch 987 Loss 2.4047\n",
      "Epoch 1 Batch 988 Loss 1.9462\n",
      "Epoch 1 Batch 989 Loss 1.8656\n",
      "Epoch 1 Batch 990 Loss 1.8702\n",
      "Epoch 1 Batch 991 Loss 1.9628\n",
      "Epoch 1 Batch 992 Loss 1.9181\n",
      "Epoch 1 Batch 993 Loss 1.7745\n",
      "Epoch 1 Batch 994 Loss 1.8211\n",
      "Epoch 1 Batch 995 Loss 1.6158\n",
      "Epoch 1 Batch 996 Loss 1.7910\n",
      "Epoch 1 Batch 997 Loss 2.5455\n",
      "Epoch 1 Batch 998 Loss 2.2129\n",
      "Epoch 1 Batch 999 Loss 2.1256\n",
      "Epoch 1 Batch 1000 Loss 2.1161\n",
      "Epoch 1 Batch 1001 Loss 1.9351\n",
      "Epoch 1 Batch 1002 Loss 1.8672\n",
      "Epoch 1 Batch 1003 Loss 2.2766\n",
      "Epoch 1 Batch 1004 Loss 2.1122\n",
      "Epoch 1 Batch 1005 Loss 2.0271\n",
      "Epoch 1 Batch 1006 Loss 2.0243\n",
      "Epoch 1 Batch 1007 Loss 2.1954\n",
      "Epoch 1 Batch 1008 Loss 2.3671\n",
      "Epoch 1 Batch 1009 Loss 2.1368\n",
      "Epoch 1 Batch 1010 Loss 1.5178\n",
      "Epoch 1 Batch 1011 Loss 2.3141\n",
      "Epoch 1 Batch 1012 Loss 2.0115\n",
      "Epoch 1 Batch 1013 Loss 2.1820\n",
      "Epoch 1 Batch 1014 Loss 2.0830\n",
      "Epoch 1 Batch 1015 Loss 2.1877\n",
      "Epoch 1 Batch 1016 Loss 2.4551\n",
      "Epoch 1 Batch 1017 Loss 1.6873\n",
      "Epoch 1 Batch 1018 Loss 1.7039\n",
      "Epoch 1 Batch 1019 Loss 1.9609\n",
      "Epoch 1 Batch 1020 Loss 1.8378\n",
      "Epoch 1 Batch 1021 Loss 2.0801\n",
      "Epoch 1 Batch 1022 Loss 2.2191\n",
      "Epoch 1 Batch 1023 Loss 2.0660\n",
      "Epoch 1 Batch 1024 Loss 1.9932\n",
      "Epoch 1 Batch 1025 Loss 2.0619\n",
      "Epoch 1 Batch 1026 Loss 1.8108\n",
      "Epoch 1 Batch 1027 Loss 2.0838\n",
      "Epoch 1 Batch 1028 Loss 1.7374\n",
      "Epoch 1 Batch 1029 Loss 1.7102\n",
      "Epoch 1 Batch 1030 Loss 1.6616\n",
      "Epoch 1 Batch 1031 Loss 2.3106\n",
      "Epoch 1 Batch 1032 Loss 1.9454\n",
      "Epoch 1 Batch 1033 Loss 1.8427\n",
      "Epoch 1 Batch 1034 Loss 1.5593\n",
      "Epoch 1 Batch 1035 Loss 1.9063\n",
      "Epoch 1 Batch 1036 Loss 1.9542\n",
      "Epoch 1 Batch 1037 Loss 2.0901\n",
      "Epoch 1 Batch 1038 Loss 1.8707\n",
      "Epoch 1 Batch 1039 Loss 2.0464\n",
      "Epoch 1 Batch 1040 Loss 1.5681\n",
      "Epoch 1 Batch 1041 Loss 1.6964\n",
      "Epoch 1 Batch 1042 Loss 1.8606\n",
      "Epoch 1 Batch 1043 Loss 1.3291\n",
      "Epoch 1 Batch 1044 Loss 1.8032\n",
      "Epoch 1 Batch 1045 Loss 1.8456\n",
      "Epoch 1 Batch 1046 Loss 1.5234\n",
      "Epoch 1 Batch 1047 Loss 1.8214\n",
      "Epoch 1 Batch 1048 Loss 1.9603\n",
      "Epoch 1 Batch 1049 Loss 2.1444\n",
      "Epoch 1 Batch 1050 Loss 2.3294\n",
      "Epoch 1 Batch 1051 Loss 2.4650\n",
      "Epoch 1 Batch 1052 Loss 2.5198\n",
      "Epoch 1 Batch 1053 Loss 2.5531\n",
      "Epoch 1 Batch 1054 Loss 2.2628\n",
      "Epoch 1 Batch 1055 Loss 2.0692\n",
      "Epoch 1 Batch 1056 Loss 1.7879\n",
      "Epoch 1 Batch 1057 Loss 1.5887\n",
      "Epoch 1 Batch 1058 Loss 1.8731\n",
      "Epoch 1 Batch 1059 Loss 2.0194\n",
      "Epoch 1 Batch 1060 Loss 2.1343\n",
      "Epoch 1 Batch 1061 Loss 1.7870\n",
      "Epoch 1 Batch 1062 Loss 1.8620\n",
      "Epoch 1 Batch 1063 Loss 1.9162\n",
      "Epoch 1 Batch 1064 Loss 1.3663\n",
      "Epoch 1 Batch 1065 Loss 1.4498\n",
      "Epoch 1 Batch 1066 Loss 2.0474\n",
      "Epoch 1 Batch 1067 Loss 1.7387\n",
      "Epoch 1 Batch 1068 Loss 1.5780\n",
      "Epoch 1 Batch 1069 Loss 1.6264\n",
      "Epoch 1 Batch 1070 Loss 1.5685\n",
      "Epoch 1 Batch 1071 Loss 1.9154\n",
      "Epoch 1 Batch 1072 Loss 2.1787\n",
      "Epoch 1 Batch 1073 Loss 2.1961\n",
      "Epoch 1 Batch 1074 Loss 2.1875\n",
      "Epoch 1 Batch 1075 Loss 2.4071\n",
      "Epoch 1 Batch 1076 Loss 2.2429\n",
      "Epoch 1 Batch 1077 Loss 1.9977\n",
      "Epoch 1 Batch 1078 Loss 1.6804\n",
      "Epoch 1 Batch 1079 Loss 1.6886\n",
      "Epoch 1 Batch 1080 Loss 2.2619\n",
      "Epoch 1 Batch 1081 Loss 1.8085\n",
      "Epoch 1 Batch 1082 Loss 2.3852\n",
      "Epoch 1 Batch 1083 Loss 1.9668\n",
      "Epoch 1 Batch 1084 Loss 1.9526\n",
      "Epoch 1 Batch 1085 Loss 1.7347\n",
      "Epoch 1 Batch 1086 Loss 2.3070\n",
      "Epoch 1 Batch 1087 Loss 2.1801\n",
      "Epoch 1 Batch 1088 Loss 2.3887\n",
      "Epoch 1 Batch 1089 Loss 2.2191\n",
      "Epoch 1 Batch 1090 Loss 2.0519\n",
      "Epoch 1 Batch 1091 Loss 1.8294\n",
      "Epoch 1 Batch 1092 Loss 2.3713\n",
      "Epoch 1 Batch 1093 Loss 2.4331\n",
      "Epoch 1 Batch 1094 Loss 2.6299\n",
      "Epoch 1 Batch 1095 Loss 2.5068\n",
      "Epoch 1 Batch 1096 Loss 2.1777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 1097 Loss 1.9514\n",
      "Epoch 1 Batch 1098 Loss 2.1116\n",
      "Epoch 1 Batch 1099 Loss 2.5544\n",
      "Epoch 1 Batch 1100 Loss 1.8820\n",
      "Epoch 1 Batch 1101 Loss 2.2018\n",
      "Epoch 1 Batch 1102 Loss 1.8250\n",
      "Epoch 1 Batch 1103 Loss 2.1760\n",
      "Epoch 1 Batch 1104 Loss 1.8418\n",
      "Epoch 1 Batch 1105 Loss 2.2748\n",
      "Epoch 1 Batch 1106 Loss 2.4008\n",
      "Epoch 1 Batch 1107 Loss 2.1038\n",
      "Epoch 1 Batch 1108 Loss 2.2074\n",
      "Epoch 1 Batch 1109 Loss 2.4590\n",
      "Epoch 1 Batch 1110 Loss 2.3036\n",
      "Epoch 1 Batch 1111 Loss 1.9071\n",
      "Epoch 1 Batch 1112 Loss 1.8184\n",
      "Epoch 1 Batch 1113 Loss 1.7409\n",
      "Epoch 1 Batch 1114 Loss 1.7913\n",
      "Epoch 1 Batch 1115 Loss 2.3523\n",
      "Epoch 1 Batch 1116 Loss 1.9769\n",
      "Epoch 1 Batch 1117 Loss 1.9477\n",
      "Epoch 1 Batch 1118 Loss 2.2411\n",
      "Epoch 1 Batch 1119 Loss 1.9220\n",
      "Epoch 1 Batch 1120 Loss 2.2216\n",
      "Epoch 1 Batch 1121 Loss 2.6205\n",
      "Epoch 1 Batch 1122 Loss 2.3379\n",
      "Epoch 1 Batch 1123 Loss 1.8189\n",
      "Epoch 1 Batch 1124 Loss 1.9431\n",
      "Epoch 1 Batch 1125 Loss 2.6112\n",
      "Epoch 1 Batch 1126 Loss 2.0671\n",
      "Epoch 1 Batch 1127 Loss 2.3646\n",
      "Epoch 1 Batch 1128 Loss 2.1319\n",
      "Epoch 1 Batch 1129 Loss 3.0079\n",
      "Epoch 1 Batch 1130 Loss 2.2884\n",
      "Epoch 1 Batch 1131 Loss 1.7882\n",
      "Epoch 1 Batch 1132 Loss 2.2508\n",
      "Epoch 1 Batch 1133 Loss 2.0675\n",
      "Epoch 1 Batch 1134 Loss 2.7492\n",
      "Epoch 1 Batch 1135 Loss 2.0754\n",
      "Epoch 1 Batch 1136 Loss 2.2030\n",
      "Epoch 1 Batch 1137 Loss 2.4731\n",
      "Epoch 1 Batch 1138 Loss 2.3684\n",
      "Epoch 1 Batch 1139 Loss 2.3616\n",
      "Epoch 1 Batch 1140 Loss 2.2952\n",
      "Epoch 1 Batch 1141 Loss 2.3766\n",
      "Epoch 1 Batch 1142 Loss 2.0146\n",
      "Epoch 1 Batch 1143 Loss 1.9236\n",
      "Epoch 1 Batch 1144 Loss 1.6909\n",
      "Epoch 1 Batch 1145 Loss 1.6295\n",
      "Epoch 1 Batch 1146 Loss 1.8378\n",
      "Epoch 1 Batch 1147 Loss 1.7729\n",
      "Epoch 1 Batch 1148 Loss 1.8932\n",
      "Epoch 1 Batch 1149 Loss 2.0657\n",
      "Epoch 1 Batch 1150 Loss 1.7306\n",
      "Epoch 1 Batch 1151 Loss 2.1507\n",
      "Epoch 1 Batch 1152 Loss 1.9680\n",
      "Epoch 1 Batch 1153 Loss 1.8394\n",
      "Epoch 1 Batch 1154 Loss 2.3113\n",
      "Epoch 1 Batch 1155 Loss 2.3920\n",
      "Epoch 1 Batch 1156 Loss 2.7433\n",
      "Epoch 1 Batch 1157 Loss 2.6948\n",
      "Epoch 1 Batch 1158 Loss 2.2435\n",
      "Epoch 1 Batch 1159 Loss 1.8535\n",
      "Epoch 1 Batch 1160 Loss 2.1007\n",
      "Epoch 1 Batch 1161 Loss 1.8616\n",
      "Epoch 1 Batch 1162 Loss 2.0481\n",
      "Epoch 1 Batch 1163 Loss 1.8401\n",
      "Epoch 1 Batch 1164 Loss 1.8617\n",
      "Epoch 1 Batch 1165 Loss 2.0131\n",
      "Epoch 1 Batch 1166 Loss 2.0553\n",
      "Epoch 1 Batch 1167 Loss 2.4527\n",
      "Epoch 1 Batch 1168 Loss 1.7694\n",
      "Epoch 1 Batch 1169 Loss 1.9332\n",
      "Epoch 1 Batch 1170 Loss 1.7199\n",
      "Epoch 1 Batch 1171 Loss 1.8901\n",
      "Epoch 1 Batch 1172 Loss 1.7204\n",
      "Epoch 1 Batch 1173 Loss 1.4240\n",
      "Epoch 1 Batch 1174 Loss 1.5900\n",
      "Epoch 1 Batch 1175 Loss 1.2365\n",
      "Epoch 1 Batch 1176 Loss 1.3236\n",
      "Epoch 1 Batch 1177 Loss 1.5619\n",
      "Epoch 1 Batch 1178 Loss 1.7099\n",
      "Epoch 1 Batch 1179 Loss 1.6868\n",
      "Epoch 1 Batch 1180 Loss 1.9057\n",
      "Epoch 1 Batch 1181 Loss 2.1088\n",
      "Epoch 1 Batch 1182 Loss 1.9930\n",
      "Epoch 1 Batch 1183 Loss 2.0163\n",
      "Epoch 1 Batch 1184 Loss 2.2585\n",
      "Epoch 1 Batch 1185 Loss 2.3161\n",
      "Epoch 1 Batch 1186 Loss 1.8632\n",
      "Epoch 1 Batch 1187 Loss 2.1297\n",
      "Epoch 1 Batch 1188 Loss 2.1596\n",
      "Epoch 1 Batch 1189 Loss 2.7786\n",
      "Epoch 1 Batch 1190 Loss 1.8079\n",
      "Epoch 1 Batch 1191 Loss 2.2485\n",
      "Epoch 1 Batch 1192 Loss 2.0951\n",
      "Epoch 1 Batch 1193 Loss 2.3338\n",
      "Epoch 1 Batch 1194 Loss 1.9845\n",
      "Epoch 1 Batch 1195 Loss 2.1752\n",
      "Epoch 1 Batch 1196 Loss 2.5465\n",
      "Epoch 1 Batch 1197 Loss 1.6692\n",
      "Epoch 1 Batch 1198 Loss 1.7101\n",
      "Epoch 1 Batch 1199 Loss 1.5726\n",
      "Epoch 1 Batch 1200 Loss 1.5657\n",
      "Epoch 1 Batch 1201 Loss 2.0981\n",
      "Epoch 1 Batch 1202 Loss 1.9104\n",
      "Epoch 1 Batch 1203 Loss 1.9327\n",
      "Epoch 1 Batch 1204 Loss 1.7829\n",
      "Epoch 1 Batch 1205 Loss 1.5370\n",
      "Epoch 1 Batch 1206 Loss 2.2248\n",
      "Epoch 1 Batch 1207 Loss 1.7532\n",
      "Epoch 1 Batch 1208 Loss 2.0043\n",
      "Epoch 1 Batch 1209 Loss 2.2780\n",
      "Epoch 1 Batch 1210 Loss 1.8644\n",
      "Epoch 1 Batch 1211 Loss 2.3553\n",
      "Epoch 1 Batch 1212 Loss 2.2141\n",
      "Epoch 1 Batch 1213 Loss 2.3537\n",
      "Epoch 1 Batch 1214 Loss 1.9960\n",
      "Epoch 1 Batch 1215 Loss 1.8939\n",
      "Epoch 1 Batch 1216 Loss 2.0579\n",
      "Epoch 1 Batch 1217 Loss 2.0615\n",
      "Epoch 1 Batch 1218 Loss 1.9633\n",
      "Epoch 1 Batch 1219 Loss 1.6032\n",
      "Epoch 1 Batch 1220 Loss 1.6342\n",
      "Epoch 1 Batch 1221 Loss 1.6423\n",
      "Epoch 1 Batch 1222 Loss 2.4392\n",
      "Epoch 1 Batch 1223 Loss 2.0343\n",
      "Epoch 1 Batch 1224 Loss 2.3210\n",
      "Epoch 1 Batch 1225 Loss 1.8535\n",
      "Epoch 1 Batch 1226 Loss 2.0055\n",
      "Epoch 1 Batch 1227 Loss 2.0924\n",
      "Epoch 1 Batch 1228 Loss 1.8480\n",
      "Epoch 1 Batch 1229 Loss 1.9360\n",
      "Epoch 1 Batch 1230 Loss 2.0916\n",
      "Epoch 1 Batch 1231 Loss 2.5932\n",
      "Epoch 1 Batch 1232 Loss 2.2031\n",
      "Epoch 1 Batch 1233 Loss 2.3681\n",
      "Epoch 1 Batch 1234 Loss 2.3540\n",
      "Epoch 1 Batch 1235 Loss 1.9854\n",
      "Epoch 1 Batch 1236 Loss 1.9427\n",
      "Epoch 1 Batch 1237 Loss 1.8192\n",
      "Epoch 1 Batch 1238 Loss 1.8948\n",
      "Epoch 1 Batch 1239 Loss 2.0352\n",
      "Epoch 1 Batch 1240 Loss 1.8957\n",
      "Epoch 1 Batch 1241 Loss 1.6873\n",
      "Epoch 1 Batch 1242 Loss 1.6945\n",
      "Epoch 1 Batch 1243 Loss 2.3189\n",
      "Epoch 1 Batch 1244 Loss 2.2007\n",
      "Epoch 1 Batch 1245 Loss 2.1875\n",
      "Epoch 1 Batch 1246 Loss 2.1712\n",
      "Epoch 1 Batch 1247 Loss 1.8616\n",
      "Epoch 1 Batch 1248 Loss 2.2681\n",
      "Epoch 1 Batch 1249 Loss 1.6785\n",
      "Epoch 1 Batch 1250 Loss 1.9852\n",
      "Epoch 1 Batch 1251 Loss 1.9623\n",
      "Epoch 1 Batch 1252 Loss 2.5345\n",
      "Epoch 1 Batch 1253 Loss 2.3095\n",
      "Epoch 1 Batch 1254 Loss 1.9821\n",
      "Epoch 1 Batch 1255 Loss 1.7143\n",
      "Epoch 1 Batch 1256 Loss 2.2344\n",
      "Epoch 1 Batch 1257 Loss 1.6613\n",
      "Epoch 1 Batch 1258 Loss 1.8922\n",
      "Epoch 1 Batch 1259 Loss 2.2919\n",
      "Epoch 1 Batch 1260 Loss 1.8664\n",
      "Epoch 1 Batch 1261 Loss 2.3582\n",
      "Epoch 1 Batch 1262 Loss 1.8328\n",
      "Epoch 1 Batch 1263 Loss 2.6568\n",
      "Epoch 1 Batch 1264 Loss 2.1590\n",
      "Epoch 1 Batch 1265 Loss 2.0805\n",
      "Epoch 1 Batch 1266 Loss 1.9523\n",
      "Epoch 1 Batch 1267 Loss 2.1667\n",
      "Epoch 1 Batch 1268 Loss 2.0772\n",
      "Epoch 1 Batch 1269 Loss 2.0148\n",
      "Epoch 1 Batch 1270 Loss 2.3067\n",
      "Epoch 1 Batch 1271 Loss 2.4012\n",
      "Epoch 1 Batch 1272 Loss 2.6003\n",
      "Epoch 1 Batch 1273 Loss 2.9253\n",
      "Epoch 1 Batch 1274 Loss 2.5344\n",
      "Epoch 1 Batch 1275 Loss 2.5515\n",
      "Epoch 1 Batch 1276 Loss 2.5458\n",
      "Epoch 1 Batch 1277 Loss 1.8789\n",
      "Epoch 1 Batch 1278 Loss 1.7942\n",
      "Epoch 1 Batch 1279 Loss 2.2673\n",
      "Epoch 1 Batch 1280 Loss 2.2495\n",
      "Epoch 1 Batch 1281 Loss 2.2992\n",
      "Epoch 1 Batch 1282 Loss 1.9200\n",
      "Epoch 1 Batch 1283 Loss 1.7754\n",
      "Epoch 1 Batch 1284 Loss 1.9811\n",
      "Epoch 1 Batch 1285 Loss 2.0553\n",
      "Epoch 1 Batch 1286 Loss 1.7431\n",
      "Epoch 1 Batch 1287 Loss 1.5488\n",
      "Epoch 1 Batch 1288 Loss 1.8024\n",
      "Epoch 1 Batch 1289 Loss 1.5940\n",
      "Epoch 1 Batch 1290 Loss 1.6689\n",
      "Epoch 1 Batch 1291 Loss 1.7419\n",
      "Epoch 1 Batch 1292 Loss 1.4845\n",
      "Epoch 1 Batch 1293 Loss 1.3816\n",
      "Epoch 1 Batch 1294 Loss 1.6115\n",
      "Epoch 1 Batch 1295 Loss 1.6372\n",
      "Epoch 1 Batch 1296 Loss 1.1355\n",
      "Epoch 1 Batch 1297 Loss 1.3409\n",
      "Epoch 1 Batch 1298 Loss 1.6408\n",
      "Epoch 1 Batch 1299 Loss 1.3532\n",
      "Epoch 1 Batch 1300 Loss 1.6309\n",
      "Epoch 1 Batch 1301 Loss 1.3059\n",
      "Epoch 1 Batch 1302 Loss 1.4286\n",
      "Epoch 1 Batch 1303 Loss 1.5007\n",
      "Epoch 1 Batch 1304 Loss 1.5819\n",
      "Epoch 1 Batch 1305 Loss 1.8487\n",
      "Epoch 1 Batch 1306 Loss 1.7631\n",
      "Epoch 1 Batch 1307 Loss 1.8992\n",
      "Epoch 1 Batch 1308 Loss 1.9098\n",
      "Epoch 1 Batch 1309 Loss 2.4236\n",
      "Epoch 1 Batch 1310 Loss 1.9775\n",
      "Epoch 1 Batch 1311 Loss 1.9607\n",
      "Epoch 1 Batch 1312 Loss 1.8111\n",
      "Epoch 1 Batch 1313 Loss 1.5408\n",
      "Epoch 1 Batch 1314 Loss 1.3518\n",
      "Epoch 1 Batch 1315 Loss 1.6384\n",
      "Epoch 1 Batch 1316 Loss 1.2168\n",
      "Epoch 1 Batch 1317 Loss 2.2479\n",
      "Epoch 1 Batch 1318 Loss 2.2394\n",
      "Epoch 1 Batch 1319 Loss 2.0187\n",
      "Epoch 1 Batch 1320 Loss 1.7905\n",
      "Epoch 1 Batch 1321 Loss 1.7734\n",
      "Epoch 1 Batch 1322 Loss 2.1645\n",
      "Epoch 1 Batch 1323 Loss 2.3692\n",
      "Epoch 1 Batch 1324 Loss 2.1922\n",
      "Epoch 1 Batch 1325 Loss 2.1658\n",
      "Epoch 1 Batch 1326 Loss 2.5356\n",
      "Epoch 1 Batch 1327 Loss 1.9855\n",
      "Epoch 1 Batch 1328 Loss 1.7357\n",
      "Epoch 1 Batch 1329 Loss 2.0450\n",
      "Epoch 1 Batch 1330 Loss 1.2103\n",
      "Epoch 1 Batch 1331 Loss 1.4047\n",
      "Epoch 1 Batch 1332 Loss 1.8212\n",
      "Epoch 1 Batch 1333 Loss 1.7857\n",
      "Epoch 1 Batch 1334 Loss 1.8898\n",
      "Epoch 1 Batch 1335 Loss 2.1040\n",
      "Epoch 1 Batch 1336 Loss 1.7728\n",
      "Epoch 1 Batch 1337 Loss 2.6680\n",
      "Epoch 1 Batch 1338 Loss 2.2405\n",
      "Epoch 1 Batch 1339 Loss 2.9142\n",
      "Epoch 1 Batch 1340 Loss 1.9883\n",
      "Epoch 1 Batch 1341 Loss 2.0429\n",
      "Epoch 1 Batch 1342 Loss 2.0315\n",
      "Epoch 1 Batch 1343 Loss 1.8501\n",
      "Epoch 1 Batch 1344 Loss 1.9513\n",
      "Epoch 1 Batch 1345 Loss 1.8095\n",
      "Epoch 1 Batch 1346 Loss 2.0221\n",
      "Epoch 1 Batch 1347 Loss 1.8004\n",
      "Epoch 1 Batch 1348 Loss 2.0407\n",
      "Epoch 1 Batch 1349 Loss 1.8402\n",
      "Epoch 1 Batch 1350 Loss 2.0470\n",
      "Epoch 1 Batch 1351 Loss 2.1775\n",
      "Epoch 1 Batch 1352 Loss 1.6225\n",
      "Epoch 1 Batch 1353 Loss 1.7805\n",
      "Epoch 1 Batch 1354 Loss 2.0105\n",
      "Epoch 1 Batch 1355 Loss 1.8710\n",
      "Epoch 1 Batch 1356 Loss 1.9503\n",
      "Epoch 1 Batch 1357 Loss 2.1073\n",
      "Epoch 1 Batch 1358 Loss 1.3378\n",
      "Epoch 1 Batch 1359 Loss 1.9896\n",
      "Epoch 1 Batch 1360 Loss 1.9957\n",
      "Epoch 1 Batch 1361 Loss 2.4892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 1362 Loss 1.7904\n",
      "Epoch 1 Batch 1363 Loss 1.9410\n",
      "Epoch 1 Batch 1364 Loss 1.9698\n",
      "Epoch 1 Batch 1365 Loss 1.8498\n",
      "Epoch 1 Batch 1366 Loss 1.3814\n",
      "Epoch 1 Batch 1367 Loss 1.9299\n",
      "Epoch 1 Batch 1368 Loss 1.8727\n",
      "Epoch 1 Batch 1369 Loss 2.2228\n",
      "Epoch 1 Batch 1370 Loss 1.9084\n",
      "Epoch 1 Batch 1371 Loss 1.5553\n",
      "Epoch 1 Batch 1372 Loss 1.7523\n",
      "Epoch 1 Batch 1373 Loss 1.9314\n",
      "Epoch 1 Batch 1374 Loss 1.8457\n",
      "Epoch 1 Batch 1375 Loss 1.9347\n",
      "Epoch 1 Batch 1376 Loss 1.6482\n",
      "Epoch 1 Batch 1377 Loss 1.5274\n",
      "Epoch 1 Batch 1378 Loss 1.9787\n",
      "Epoch 1 Batch 1379 Loss 2.2462\n",
      "Epoch 1 Batch 1380 Loss 1.8603\n",
      "Epoch 1 Batch 1381 Loss 2.2549\n",
      "Epoch 1 Batch 1382 Loss 1.8865\n",
      "Epoch 1 Batch 1383 Loss 1.8384\n",
      "Epoch 1 Batch 1384 Loss 1.9382\n",
      "Epoch 1 Batch 1385 Loss 2.1904\n",
      "Epoch 1 Batch 1386 Loss 1.8501\n",
      "Epoch 1 Batch 1387 Loss 1.7880\n",
      "Epoch 1 Batch 1388 Loss 1.8334\n",
      "Epoch 1 Batch 1389 Loss 2.2105\n",
      "Epoch 1 Batch 1390 Loss 1.9402\n",
      "Epoch 1 Batch 1391 Loss 1.9529\n",
      "Epoch 1 Batch 1392 Loss 1.8810\n",
      "Epoch 1 Batch 1393 Loss 2.4910\n",
      "Epoch 1 Batch 1394 Loss 2.3827\n",
      "Epoch 1 Batch 1395 Loss 1.3479\n",
      "Epoch 1 Batch 1396 Loss 1.8002\n",
      "Epoch 1 Batch 1397 Loss 1.2712\n",
      "Epoch 1 Batch 1398 Loss 1.6437\n",
      "Epoch 1 Batch 1399 Loss 1.6044\n",
      "Epoch 1 Batch 1400 Loss 1.8029\n",
      "Epoch 1 Batch 1401 Loss 1.7860\n",
      "Epoch 1 Batch 1402 Loss 1.6324\n",
      "Epoch 1 Batch 1403 Loss 1.7833\n",
      "Epoch 1 Batch 1404 Loss 2.0617\n",
      "Epoch 1 Batch 1405 Loss 2.0421\n",
      "Epoch 1 Batch 1406 Loss 2.1920\n",
      "Epoch 1 Batch 1407 Loss 2.4529\n",
      "Epoch 1 Batch 1408 Loss 2.4646\n",
      "Epoch 1 Batch 1409 Loss 2.3845\n",
      "Epoch 1 Batch 1410 Loss 2.2841\n",
      "Epoch 1 Batch 1411 Loss 2.4467\n",
      "Epoch 1 Batch 1412 Loss 1.7454\n",
      "Epoch 1 Batch 1413 Loss 1.4980\n",
      "Epoch 1 Batch 1414 Loss 1.3814\n",
      "Epoch 1 Batch 1415 Loss 1.8954\n",
      "Epoch 1 Batch 1416 Loss 1.5066\n",
      "Epoch 1 Batch 1417 Loss 1.6357\n",
      "Epoch 1 Batch 1418 Loss 1.8226\n",
      "Epoch 1 Batch 1419 Loss 1.6512\n",
      "Epoch 1 Batch 1420 Loss 1.9241\n",
      "Epoch 1 Batch 1421 Loss 2.3080\n",
      "Epoch 1 Batch 1422 Loss 1.9890\n",
      "Epoch 1 Batch 1423 Loss 1.9692\n",
      "Epoch 1 Batch 1424 Loss 1.8880\n",
      "Epoch 1 Batch 1425 Loss 2.0123\n",
      "Epoch 1 Batch 1426 Loss 1.7704\n",
      "Epoch 1 Batch 1427 Loss 1.9153\n",
      "Epoch 1 Batch 1428 Loss 2.2974\n",
      "Epoch 1 Batch 1429 Loss 1.6792\n",
      "Epoch 1 Batch 1430 Loss 1.5265\n",
      "Epoch 1 Batch 1431 Loss 1.6797\n",
      "Epoch 1 Batch 1432 Loss 2.0634\n",
      "Epoch 1 Batch 1433 Loss 2.0866\n",
      "Epoch 1 Batch 1434 Loss 2.3796\n",
      "Epoch 1 Batch 1435 Loss 2.3765\n",
      "Epoch 1 Batch 1436 Loss 1.9315\n",
      "Epoch 1 Batch 1437 Loss 2.2075\n",
      "Epoch 1 Batch 1438 Loss 1.8647\n",
      "Epoch 1 Batch 1439 Loss 2.1894\n",
      "Epoch 1 Batch 1440 Loss 1.8684\n",
      "Epoch 1 Batch 1441 Loss 1.9815\n",
      "Epoch 1 Batch 1442 Loss 2.4015\n",
      "Epoch 1 Batch 1443 Loss 2.6836\n",
      "Epoch 1 Batch 1444 Loss 2.2601\n",
      "Epoch 1 Batch 1445 Loss 2.6816\n",
      "Epoch 1 Batch 1446 Loss 2.4295\n",
      "Epoch 1 Batch 1447 Loss 2.6626\n",
      "Epoch 1 Batch 1448 Loss 1.9458\n",
      "Epoch 1 Batch 1449 Loss 1.5043\n",
      "Epoch 1 Batch 1450 Loss 1.8801\n",
      "Epoch 1 Batch 1451 Loss 1.6219\n",
      "Epoch 1 Batch 1452 Loss 2.0896\n",
      "Epoch 1 Batch 1453 Loss 1.7872\n",
      "Epoch 1 Batch 1454 Loss 1.7864\n",
      "Epoch 1 Batch 1455 Loss 1.7278\n",
      "Epoch 1 Batch 1456 Loss 1.8790\n",
      "Epoch 1 Batch 1457 Loss 1.8770\n",
      "Epoch 1 Batch 1458 Loss 1.6102\n",
      "Epoch 1 Batch 1459 Loss 1.5518\n",
      "Epoch 1 Batch 1460 Loss 1.2177\n",
      "Epoch 1 Batch 1461 Loss 1.0307\n",
      "Epoch 1 Batch 1462 Loss 1.3970\n",
      "Epoch 1 Batch 1463 Loss 1.6482\n",
      "Epoch 1 Batch 1464 Loss 1.4004\n",
      "Epoch 1 Batch 1465 Loss 1.8747\n",
      "Epoch 1 Batch 1466 Loss 2.2804\n",
      "Epoch 1 Batch 1467 Loss 2.3020\n",
      "Epoch 1 Batch 1468 Loss 2.3280\n",
      "Epoch 1 Batch 1469 Loss 2.4724\n",
      "Epoch 1 Batch 1470 Loss 1.6490\n",
      "Epoch 1 Batch 1471 Loss 2.2941\n",
      "Epoch 1 Batch 1472 Loss 1.7992\n",
      "Epoch 1 Batch 1473 Loss 2.2564\n",
      "Epoch 1 Batch 1474 Loss 1.8756\n",
      "Epoch 1 Batch 1475 Loss 1.6112\n",
      "Epoch 1 Batch 1476 Loss 2.4984\n",
      "Epoch 1 Batch 1477 Loss 2.0014\n",
      "Epoch 1 Batch 1478 Loss 1.7273\n",
      "Epoch 1 Batch 1479 Loss 1.8077\n",
      "Epoch 1 Batch 1480 Loss 1.5833\n",
      "Epoch 1 Batch 1481 Loss 2.0451\n",
      "Epoch 1 Batch 1482 Loss 2.1364\n",
      "Epoch 1 Batch 1483 Loss 2.1186\n",
      "Epoch 1 Batch 1484 Loss 1.4353\n",
      "Epoch 1 Batch 1485 Loss 2.1879\n",
      "Epoch 1 Batch 1486 Loss 1.1272\n",
      "Epoch 1 Batch 1487 Loss 1.7113\n",
      "Epoch 1 Batch 1488 Loss 1.5725\n",
      "Epoch 1 Batch 1489 Loss 1.7216\n",
      "Epoch 1 Batch 1490 Loss 2.2782\n",
      "Epoch 1 Batch 1491 Loss 2.1841\n",
      "Epoch 1 Batch 1492 Loss 2.0532\n",
      "Epoch 1 Batch 1493 Loss 2.1932\n",
      "Epoch 1 Batch 1494 Loss 2.1962\n",
      "Epoch 1 Batch 1495 Loss 2.1878\n",
      "Epoch 1 Batch 1496 Loss 1.8103\n",
      "Epoch 1 Batch 1497 Loss 1.6448\n",
      "Epoch 1 Batch 1498 Loss 1.8050\n",
      "Epoch 1 Batch 1499 Loss 1.8758\n",
      "Epoch 1 Batch 1500 Loss 1.9863\n",
      "Epoch 1 Batch 1501 Loss 2.1101\n",
      "Epoch 1 Batch 1502 Loss 1.8720\n",
      "Epoch 1 Batch 1503 Loss 1.7284\n",
      "Epoch 1 Batch 1504 Loss 1.7147\n",
      "Epoch 1 Batch 1505 Loss 1.6138\n",
      "Epoch 1 Batch 1506 Loss 1.5006\n",
      "Epoch 1 Batch 1507 Loss 1.8231\n",
      "Epoch 1 Batch 1508 Loss 2.0065\n",
      "Epoch 1 Batch 1509 Loss 1.7982\n",
      "Epoch 1 Batch 1510 Loss 1.8335\n",
      "Epoch 1 Batch 1511 Loss 1.6902\n",
      "Epoch 1 Batch 1512 Loss 1.9896\n",
      "Epoch 1 Batch 1513 Loss 2.1477\n",
      "Epoch 1 Batch 1514 Loss 2.1922\n",
      "Epoch 1 Batch 1515 Loss 2.6930\n",
      "Epoch 1 Batch 1516 Loss 2.9117\n",
      "Epoch 1 Batch 1517 Loss 2.4563\n",
      "Epoch 1 Batch 1518 Loss 1.7978\n",
      "Epoch 1 Batch 1519 Loss 1.6032\n",
      "Epoch 1 Batch 1520 Loss 1.6584\n",
      "Epoch 1 Batch 1521 Loss 1.9117\n",
      "Epoch 1 Batch 1522 Loss 2.5070\n",
      "Epoch 1 Batch 1523 Loss 1.9038\n",
      "Epoch 1 Batch 1524 Loss 1.9275\n",
      "Epoch 1 Batch 1525 Loss 1.8063\n",
      "Epoch 1 Batch 1526 Loss 1.9272\n",
      "Epoch 1 Batch 1527 Loss 1.8467\n",
      "Epoch 1 Batch 1528 Loss 2.1410\n",
      "Epoch 1 Batch 1529 Loss 2.2986\n",
      "Epoch 1 Batch 1530 Loss 2.0322\n",
      "Epoch 1 Batch 1531 Loss 1.8948\n",
      "Epoch 1 Batch 1532 Loss 2.2542\n",
      "Epoch 1 Batch 1533 Loss 2.3753\n",
      "Epoch 1 Batch 1534 Loss 2.4711\n",
      "Epoch 1 Batch 1535 Loss 2.0066\n",
      "Epoch 1 Batch 1536 Loss 2.2536\n",
      "Epoch 1 Batch 1537 Loss 1.9880\n",
      "Epoch 1 Batch 1538 Loss 1.7759\n",
      "Epoch 1 Batch 1539 Loss 1.8990\n",
      "Epoch 1 Batch 1540 Loss 1.9099\n",
      "Epoch 1 Batch 1541 Loss 1.9038\n",
      "Epoch 1 Batch 1542 Loss 1.2716\n",
      "Epoch 1 Batch 1543 Loss 1.9244\n",
      "Epoch 1 Batch 1544 Loss 1.7067\n",
      "Epoch 1 Batch 1545 Loss 1.9200\n",
      "Epoch 1 Batch 1546 Loss 2.0239\n",
      "Epoch 1 Batch 1547 Loss 1.9750\n",
      "Epoch 1 Batch 1548 Loss 2.2204\n",
      "Epoch 1 Batch 1549 Loss 1.5784\n",
      "Epoch 1 Batch 1550 Loss 1.6147\n",
      "Epoch 1 Batch 1551 Loss 1.7183\n",
      "Epoch 1 Batch 1552 Loss 1.7698\n",
      "Epoch 1 Batch 1553 Loss 1.9222\n",
      "Epoch 1 Batch 1554 Loss 2.2137\n",
      "Epoch 1 Batch 1555 Loss 2.1731\n",
      "Epoch 1 Batch 1556 Loss 2.0766\n",
      "Epoch 1 Batch 1557 Loss 2.1673\n",
      "Epoch 1 Batch 1558 Loss 1.4380\n",
      "Epoch 1 Batch 1559 Loss 1.8187\n",
      "Epoch 1 Batch 1560 Loss 1.9397\n",
      "Epoch 1 Batch 1561 Loss 1.6844\n",
      "Epoch 1 Batch 1562 Loss 1.8899\n",
      "Epoch 1 Batch 1563 Loss 1.9956\n",
      "Epoch 1 Batch 1564 Loss 1.7241\n",
      "Epoch 1 Batch 1565 Loss 2.2132\n",
      "Epoch 1 Batch 1566 Loss 2.0301\n",
      "Epoch 1 Batch 1567 Loss 1.9335\n",
      "Epoch 1 Batch 1568 Loss 2.1804\n",
      "Epoch 1 Batch 1569 Loss 2.3744\n",
      "Epoch 1 Batch 1570 Loss 1.8997\n",
      "Epoch 1 Batch 1571 Loss 2.2690\n",
      "Epoch 1 Batch 1572 Loss 1.9613\n",
      "Epoch 1 Batch 1573 Loss 2.2833\n",
      "Epoch 1 Batch 1574 Loss 2.4356\n",
      "Epoch 1 Batch 1575 Loss 2.1873\n",
      "Epoch 1 Batch 1576 Loss 2.6674\n",
      "Epoch 1 Batch 1577 Loss 2.2700\n",
      "Epoch 1 Batch 1578 Loss 2.3281\n",
      "Epoch 1 Batch 1579 Loss 1.8144\n",
      "Epoch 1 Batch 1580 Loss 1.9158\n",
      "Epoch 1 Batch 1581 Loss 1.9604\n",
      "Epoch 1 Batch 1582 Loss 1.6426\n",
      "Epoch 1 Batch 1583 Loss 1.7485\n",
      "Epoch 1 Batch 1584 Loss 2.7204\n",
      "Epoch 1 Batch 1585 Loss 1.9175\n",
      "Epoch 1 Batch 1586 Loss 1.8551\n",
      "Epoch 1 Batch 1587 Loss 2.0100\n",
      "Epoch 1 Batch 1588 Loss 1.5271\n",
      "Epoch 1 Batch 1589 Loss 1.8594\n",
      "Epoch 1 Batch 1590 Loss 2.2013\n",
      "Epoch 1 Batch 1591 Loss 2.5418\n",
      "Epoch 1 Batch 1592 Loss 1.9585\n",
      "Epoch 1 Batch 1593 Loss 2.1276\n",
      "Epoch 1 Batch 1594 Loss 1.8342\n",
      "Epoch 1 Batch 1595 Loss 1.7418\n",
      "Epoch 1 Batch 1596 Loss 2.0896\n",
      "Epoch 1 Batch 1597 Loss 2.1993\n",
      "Epoch 1 Batch 1598 Loss 2.5150\n",
      "Epoch 1 Batch 1599 Loss 2.2990\n",
      "Epoch 1 Batch 1600 Loss 1.9187\n",
      "Epoch 1 Batch 1601 Loss 2.3099\n",
      "Epoch 1 Batch 1602 Loss 2.8128\n",
      "Epoch 1 Batch 1603 Loss 1.9480\n",
      "Epoch 1 Batch 1604 Loss 1.8646\n",
      "Epoch 1 Batch 1605 Loss 2.3353\n",
      "Epoch 1 Batch 1606 Loss 2.3336\n",
      "Epoch 1 Batch 1607 Loss 2.0957\n",
      "Epoch 1 Batch 1608 Loss 2.1944\n",
      "Epoch 1 Batch 1609 Loss 2.3444\n",
      "Epoch 1 Batch 1610 Loss 1.9637\n",
      "Epoch 1 Batch 1611 Loss 1.9270\n",
      "Epoch 1 Batch 1612 Loss 1.8951\n",
      "Epoch 1 Batch 1613 Loss 1.8580\n",
      "Epoch 1 Batch 1614 Loss 1.9579\n",
      "Epoch 1 Batch 1615 Loss 1.9442\n",
      "Epoch 1 Batch 1616 Loss 1.5420\n",
      "Epoch 1 Batch 1617 Loss 2.0800\n",
      "Epoch 1 Batch 1618 Loss 1.7799\n",
      "Epoch 1 Batch 1619 Loss 2.2884\n",
      "Epoch 1 Batch 1620 Loss 1.7705\n",
      "Epoch 1 Batch 1621 Loss 1.8037\n",
      "Epoch 1 Batch 1622 Loss 1.6104\n",
      "Epoch 1 Batch 1623 Loss 1.8077\n",
      "Epoch 1 Batch 1624 Loss 1.9920\n",
      "Epoch 1 Batch 1625 Loss 2.1402\n",
      "Epoch 1 Batch 1626 Loss 1.7673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 1627 Loss 2.0381\n",
      "Epoch 1 Batch 1628 Loss 2.4309\n",
      "Epoch 1 Batch 1629 Loss 3.0875\n",
      "Epoch 1 Batch 1630 Loss 2.5178\n",
      "Epoch 1 Batch 1631 Loss 2.3453\n",
      "Epoch 1 Batch 1632 Loss 2.0195\n",
      "Epoch 1 Batch 1633 Loss 2.3296\n",
      "Epoch 1 Batch 1634 Loss 1.9414\n",
      "Epoch 1 Batch 1635 Loss 2.3559\n",
      "Epoch 1 Batch 1636 Loss 2.1464\n",
      "Epoch 1 Batch 1637 Loss 1.8822\n",
      "Epoch 1 Batch 1638 Loss 1.4894\n",
      "Epoch 1 Batch 1639 Loss 2.2766\n",
      "Epoch 1 Batch 1640 Loss 2.4482\n",
      "Epoch 1 Batch 1641 Loss 2.0682\n",
      "Epoch 1 Batch 1642 Loss 1.9103\n",
      "Epoch 1 Batch 1643 Loss 2.0642\n",
      "Epoch 1 Batch 1644 Loss 1.5936\n",
      "Epoch 1 Batch 1645 Loss 1.5459\n",
      "Epoch 1 Batch 1646 Loss 1.2384\n",
      "Epoch 1 Batch 1647 Loss 1.3248\n",
      "Epoch 1 Batch 1648 Loss 1.7437\n",
      "Epoch 1 Batch 1649 Loss 1.9454\n",
      "Epoch 1 Batch 1650 Loss 1.9406\n",
      "Epoch 1 Batch 1651 Loss 2.3671\n",
      "Epoch 1 Batch 1652 Loss 1.8873\n",
      "Epoch 1 Batch 1653 Loss 1.9898\n",
      "Epoch 1 Batch 1654 Loss 1.5089\n",
      "Epoch 1 Batch 1655 Loss 1.6784\n",
      "Epoch 1 Batch 1656 Loss 2.5173\n",
      "Epoch 1 Batch 1657 Loss 2.3479\n",
      "Epoch 1 Batch 1658 Loss 2.1015\n",
      "Epoch 1 Batch 1659 Loss 1.7931\n",
      "Epoch 1 Batch 1660 Loss 2.0625\n",
      "Epoch 1 Batch 1661 Loss 1.8222\n",
      "Epoch 1 Batch 1662 Loss 2.1134\n",
      "Epoch 1 Batch 1663 Loss 1.9719\n",
      "Epoch 1 Batch 1664 Loss 2.2096\n",
      "Epoch 1 Batch 1665 Loss 2.5721\n",
      "Epoch 1 Batch 1666 Loss 2.1260\n",
      "Epoch 1 Batch 1667 Loss 2.5488\n",
      "Epoch 1 Batch 1668 Loss 1.9776\n",
      "Epoch 1 Batch 1669 Loss 2.0349\n",
      "Epoch 1 Batch 1670 Loss 2.1217\n",
      "Epoch 1 Batch 1671 Loss 2.3756\n",
      "Epoch 1 Batch 1672 Loss 2.1686\n",
      "Epoch 1 Batch 1673 Loss 2.0697\n",
      "Epoch 1 Batch 1674 Loss 2.0991\n",
      "Epoch 1 Batch 1675 Loss 1.9035\n",
      "Epoch 1 Batch 1676 Loss 1.4077\n",
      "Epoch 1 Batch 1677 Loss 2.6191\n",
      "Epoch 1 Batch 1678 Loss 2.7342\n",
      "Epoch 1 Batch 1679 Loss 2.1758\n",
      "Epoch 1 Batch 1680 Loss 2.3851\n",
      "Epoch 1 Batch 1681 Loss 1.8496\n",
      "Epoch 1 Batch 1682 Loss 1.5836\n",
      "Epoch 1 Batch 1683 Loss 1.5505\n",
      "Epoch 1 Batch 1684 Loss 1.6308\n",
      "Epoch 1 Batch 1685 Loss 1.8274\n",
      "Epoch 1 Batch 1686 Loss 1.8234\n",
      "Epoch 1 Batch 1687 Loss 1.6721\n",
      "Epoch 1 Batch 1688 Loss 1.7265\n",
      "Epoch 1 Batch 1689 Loss 1.5447\n",
      "Epoch 1 Batch 1690 Loss 1.9290\n",
      "Epoch 1 Batch 1691 Loss 2.1456\n",
      "Epoch 1 Batch 1692 Loss 2.6116\n",
      "Epoch 1 Batch 1693 Loss 2.2318\n",
      "Epoch 1 Batch 1694 Loss 2.2763\n",
      "Epoch 1 Batch 1695 Loss 1.9165\n",
      "Epoch 1 Batch 1696 Loss 2.3604\n",
      "Epoch 1 Batch 1697 Loss 2.0426\n",
      "Epoch 1 Batch 1698 Loss 2.5404\n",
      "Epoch 1 Batch 1699 Loss 2.1622\n",
      "Epoch 1 Batch 1700 Loss 2.3657\n",
      "Epoch 1 Batch 1701 Loss 1.7401\n",
      "Epoch 1 Batch 1702 Loss 1.9025\n",
      "Epoch 1 Batch 1703 Loss 2.2089\n",
      "Epoch 1 Batch 1704 Loss 2.1485\n",
      "Epoch 1 Batch 1705 Loss 2.1094\n",
      "Epoch 1 Batch 1706 Loss 1.8356\n",
      "Epoch 1 Batch 1707 Loss 2.0134\n",
      "Epoch 1 Batch 1708 Loss 2.2393\n",
      "Epoch 1 Batch 1709 Loss 2.6071\n",
      "Epoch 1 Batch 1710 Loss 1.9145\n",
      "Epoch 1 Batch 1711 Loss 2.4697\n",
      "Epoch 1 Batch 1712 Loss 1.9703\n",
      "Epoch 1 Batch 1713 Loss 2.3291\n",
      "Epoch 1 Batch 1714 Loss 2.6587\n",
      "Epoch 1 Batch 1715 Loss 1.7794\n",
      "Epoch 1 Batch 1716 Loss 1.8789\n",
      "Epoch 1 Batch 1717 Loss 2.2373\n",
      "Epoch 1 Batch 1718 Loss 1.9586\n",
      "Epoch 1 Batch 1719 Loss 2.2283\n",
      "Epoch 1 Batch 1720 Loss 2.0079\n",
      "Epoch 1 Batch 1721 Loss 2.5472\n",
      "Epoch 1 Batch 1722 Loss 2.1495\n",
      "Epoch 1 Batch 1723 Loss 1.9027\n",
      "Epoch 1 Batch 1724 Loss 1.7781\n",
      "Epoch 1 Batch 1725 Loss 1.6297\n",
      "Epoch 1 Batch 1726 Loss 1.9584\n",
      "Epoch 1 Batch 1727 Loss 1.9382\n",
      "Epoch 1 Batch 1728 Loss 1.8881\n",
      "Epoch 1 Batch 1729 Loss 2.2167\n",
      "Epoch 1 Batch 1730 Loss 2.4475\n",
      "Epoch 1 Batch 1731 Loss 2.3417\n",
      "Epoch 1 Batch 1732 Loss 1.9031\n",
      "Epoch 1 Batch 1733 Loss 1.6572\n",
      "Epoch 1 Batch 1734 Loss 1.4336\n",
      "Epoch 1 Batch 1735 Loss 1.5669\n",
      "Epoch 1 Batch 1736 Loss 1.6324\n",
      "Epoch 1 Batch 1737 Loss 2.5699\n",
      "Epoch 1 Batch 1738 Loss 2.4515\n",
      "Epoch 1 Batch 1739 Loss 1.9767\n",
      "Epoch 1 Batch 1740 Loss 2.3388\n",
      "Epoch 1 Batch 1741 Loss 1.8607\n",
      "Epoch 1 Batch 1742 Loss 1.7193\n",
      "Epoch 1 Batch 1743 Loss 1.6360\n",
      "Epoch 1 Batch 1744 Loss 1.5841\n",
      "Epoch 1 Batch 1745 Loss 1.8818\n",
      "Epoch 1 Batch 1746 Loss 1.8924\n",
      "Epoch 1 Batch 1747 Loss 1.9194\n",
      "Epoch 1 Batch 1748 Loss 1.8369\n",
      "Epoch 1 Batch 1749 Loss 1.9226\n",
      "Epoch 1 Batch 1750 Loss 1.7414\n",
      "Epoch 1 Batch 1751 Loss 1.9589\n",
      "Epoch 1 Batch 1752 Loss 2.0852\n",
      "Epoch 1 Batch 1753 Loss 2.5352\n",
      "Epoch 1 Batch 1754 Loss 2.0982\n",
      "Epoch 1 Batch 1755 Loss 2.3569\n",
      "Epoch 1 Batch 1756 Loss 2.8342\n",
      "Epoch 1 Batch 1757 Loss 2.1235\n",
      "Epoch 1 Batch 1758 Loss 1.9135\n",
      "Epoch 1 Batch 1759 Loss 2.0117\n",
      "Epoch 1 Batch 1760 Loss 1.8415\n",
      "Epoch 1 Batch 1761 Loss 2.2977\n",
      "Epoch 1 Batch 1762 Loss 2.0710\n",
      "Epoch 1 Batch 1763 Loss 1.6047\n",
      "Epoch 1 Batch 1764 Loss 2.2289\n",
      "Epoch 1 Batch 1765 Loss 1.8834\n",
      "Epoch 1 Batch 1766 Loss 2.0490\n",
      "Epoch 1 Batch 1767 Loss 1.7297\n",
      "Epoch 1 Batch 1768 Loss 2.0825\n",
      "Epoch 1 Batch 1769 Loss 2.3902\n",
      "Epoch 1 Batch 1770 Loss 2.7532\n",
      "Epoch 1 Batch 1771 Loss 1.7293\n",
      "Epoch 1 Batch 1772 Loss 1.9333\n",
      "Epoch 1 Batch 1773 Loss 2.7936\n",
      "Epoch 1 Batch 1774 Loss 1.7900\n",
      "Epoch 1 Batch 1775 Loss 2.2370\n",
      "Epoch 1 Batch 1776 Loss 2.0808\n",
      "Epoch 1 Batch 1777 Loss 1.9726\n",
      "Epoch 1 Batch 1778 Loss 1.9546\n",
      "Epoch 1 Batch 1779 Loss 2.2355\n",
      "Epoch 1 Batch 1780 Loss 2.1399\n",
      "Epoch 1 Batch 1781 Loss 2.6224\n",
      "Epoch 1 Batch 1782 Loss 2.4475\n",
      "Epoch 1 Batch 1783 Loss 2.4931\n",
      "Epoch 1 Batch 1784 Loss 2.4267\n",
      "Epoch 1 Batch 1785 Loss 2.2661\n",
      "Epoch 1 Batch 1786 Loss 1.9855\n",
      "Epoch 1 Batch 1787 Loss 2.0822\n",
      "Epoch 1 Batch 1788 Loss 1.9658\n",
      "Epoch 1 Batch 1789 Loss 2.6111\n",
      "Epoch 1 Batch 1790 Loss 1.7938\n",
      "Epoch 1 Batch 1791 Loss 1.6318\n",
      "Epoch 1 Batch 1792 Loss 1.8702\n",
      "Epoch 1 Batch 1793 Loss 2.1917\n",
      "Epoch 1 Batch 1794 Loss 2.2533\n",
      "Epoch 1 Batch 1795 Loss 1.8135\n",
      "Epoch 1 Batch 1796 Loss 1.8454\n",
      "Epoch 1 Batch 1797 Loss 1.6937\n",
      "Epoch 1 Batch 1798 Loss 1.7858\n",
      "Epoch 1 Batch 1799 Loss 1.7313\n",
      "Epoch 1 Batch 1800 Loss 1.9363\n",
      "Epoch 1 Batch 1801 Loss 2.3156\n",
      "Epoch 1 Batch 1802 Loss 2.1345\n",
      "Epoch 1 Batch 1803 Loss 2.4028\n",
      "Epoch 1 Batch 1804 Loss 2.4441\n",
      "Epoch 1 Batch 1805 Loss 1.8781\n",
      "Epoch 1 Batch 1806 Loss 1.8445\n",
      "Epoch 1 Batch 1807 Loss 1.7513\n",
      "Epoch 1 Batch 1808 Loss 2.2539\n",
      "Epoch 1 Batch 1809 Loss 2.0482\n",
      "Epoch 1 Batch 1810 Loss 2.2512\n",
      "Epoch 1 Batch 1811 Loss 2.5556\n",
      "Epoch 1 Batch 1812 Loss 2.6137\n",
      "Epoch 1 Batch 1813 Loss 2.4675\n",
      "Epoch 1 Batch 1814 Loss 2.1577\n",
      "Epoch 1 Batch 1815 Loss 2.0535\n",
      "Epoch 1 Batch 1816 Loss 1.9209\n",
      "Epoch 1 Batch 1817 Loss 1.2819\n",
      "Epoch 1 Batch 1818 Loss 2.1446\n",
      "Epoch 1 Batch 1819 Loss 2.4513\n",
      "Epoch 1 Batch 1820 Loss 2.1350\n",
      "Epoch 1 Batch 1821 Loss 2.2074\n",
      "Epoch 1 Batch 1822 Loss 1.9601\n",
      "Epoch 1 Batch 1823 Loss 2.1605\n",
      "Epoch 1 Batch 1824 Loss 1.8645\n",
      "Epoch 1 Batch 1825 Loss 2.3149\n",
      "Epoch 1 Batch 1826 Loss 2.3900\n",
      "Epoch 1 Batch 1827 Loss 2.1109\n",
      "Epoch 1 Batch 1828 Loss 2.0239\n",
      "Epoch 1 Batch 1829 Loss 2.2065\n",
      "Epoch 1 Batch 1830 Loss 1.8134\n",
      "Epoch 1 Batch 1831 Loss 2.1675\n",
      "Epoch 1 Batch 1832 Loss 2.6559\n",
      "Epoch 1 Batch 1833 Loss 2.3838\n",
      "Epoch 1 Batch 1834 Loss 2.6865\n",
      "Epoch 1 Batch 1835 Loss 2.5049\n",
      "Epoch 1 Batch 1836 Loss 2.3448\n",
      "Epoch 1 Batch 1837 Loss 2.2818\n",
      "Epoch 1 Batch 1838 Loss 2.0928\n",
      "Epoch 1 Batch 1839 Loss 1.8190\n",
      "Epoch 1 Batch 1840 Loss 2.5011\n",
      "Epoch 1 Batch 1841 Loss 2.1390\n",
      "Epoch 1 Batch 1842 Loss 1.8180\n",
      "Epoch 1 Batch 1843 Loss 1.8694\n",
      "Epoch 1 Batch 1844 Loss 1.8496\n",
      "Epoch 1 Batch 1845 Loss 1.7143\n",
      "Epoch 1 Batch 1846 Loss 2.1653\n",
      "Epoch 1 Batch 1847 Loss 2.0013\n",
      "Epoch 1 Batch 1848 Loss 2.0439\n",
      "Epoch 1 Batch 1849 Loss 2.3385\n",
      "Epoch 1 Batch 1850 Loss 1.9006\n",
      "Epoch 1 Batch 1851 Loss 1.4569\n",
      "Epoch 1 Batch 1852 Loss 1.8778\n",
      "Epoch 1 Batch 1853 Loss 1.7043\n",
      "Epoch 1 Batch 1854 Loss 1.9149\n",
      "Epoch 1 Batch 1855 Loss 1.8492\n",
      "Epoch 1 Batch 1856 Loss 1.6156\n",
      "Epoch 1 Batch 1857 Loss 1.6979\n",
      "Epoch 1 Batch 1858 Loss 1.5859\n",
      "Epoch 1 Batch 1859 Loss 2.1511\n",
      "Epoch 1 Batch 1860 Loss 1.5234\n",
      "Epoch 1 Batch 1861 Loss 1.5665\n",
      "Epoch 1 Batch 1862 Loss 1.7411\n",
      "Epoch 1 Batch 1863 Loss 2.0755\n",
      "Epoch 1 Batch 1864 Loss 1.6346\n",
      "Epoch 1 Batch 1865 Loss 2.2866\n",
      "Epoch 1 Batch 1866 Loss 1.8395\n",
      "Epoch 1 Batch 1867 Loss 1.8541\n",
      "Epoch 1 Batch 1868 Loss 2.3049\n",
      "Epoch 1 Batch 1869 Loss 1.9006\n",
      "Epoch 1 Batch 1870 Loss 2.7059\n",
      "Epoch 1 Batch 1871 Loss 2.2029\n",
      "Epoch 1 Batch 1872 Loss 2.2893\n",
      "Epoch 1 Batch 1873 Loss 2.1544\n",
      "Epoch 1 Batch 1874 Loss 2.7115\n",
      "Epoch 1 Batch 1875 Loss 2.4691\n",
      "Epoch 1 Batch 1876 Loss 2.7593\n",
      "Epoch 1 Batch 1877 Loss 2.0222\n",
      "Epoch 1 Batch 1878 Loss 1.6414\n",
      "Epoch 1 Batch 1879 Loss 1.6365\n",
      "Epoch 1 Batch 1880 Loss 1.8936\n",
      "Epoch 1 Batch 1881 Loss 1.8368\n",
      "Epoch 1 Batch 1882 Loss 2.1665\n",
      "Epoch 1 Batch 1883 Loss 2.3070\n",
      "Epoch 1 Batch 1884 Loss 2.0055\n",
      "Epoch 1 Batch 1885 Loss 1.8724\n",
      "Epoch 1 Batch 1886 Loss 2.1859\n",
      "Epoch 1 Batch 1887 Loss 1.9497\n",
      "Epoch 1 Batch 1888 Loss 1.8980\n",
      "Epoch 1 Batch 1889 Loss 1.9077\n",
      "Epoch 1 Batch 1890 Loss 1.8248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 1891 Loss 1.4927\n",
      "Epoch 1 Batch 1892 Loss 1.7133\n",
      "Epoch 1 Batch 1893 Loss 1.4191\n",
      "Epoch 1 Batch 1894 Loss 1.7326\n",
      "Epoch 1 Batch 1895 Loss 1.7665\n",
      "Epoch 1 Batch 1896 Loss 1.6516\n",
      "Epoch 1 Batch 1897 Loss 1.6096\n",
      "Epoch 1 Batch 1898 Loss 2.3039\n",
      "Epoch 1 Batch 1899 Loss 1.9149\n",
      "Epoch 1 Batch 1900 Loss 1.6664\n",
      "Epoch 1 Batch 1901 Loss 2.2313\n",
      "Epoch 1 Batch 1902 Loss 2.2011\n",
      "Epoch 1 Batch 1903 Loss 2.3241\n",
      "Epoch 1 Batch 1904 Loss 2.7000\n",
      "Epoch 1 Batch 1905 Loss 1.9077\n",
      "Epoch 1 Batch 1906 Loss 2.3542\n",
      "Epoch 1 Batch 1907 Loss 2.2012\n",
      "Epoch 1 Batch 1908 Loss 2.6784\n",
      "Epoch 1 Batch 1909 Loss 2.0925\n",
      "Epoch 1 Batch 1910 Loss 2.3479\n",
      "Epoch 1 Batch 1911 Loss 2.1575\n",
      "Epoch 1 Batch 1912 Loss 2.6232\n",
      "Epoch 1 Batch 1913 Loss 2.1465\n",
      "Epoch 1 Batch 1914 Loss 2.2676\n",
      "Epoch 1 Batch 1915 Loss 2.6808\n",
      "Epoch 1 Batch 1916 Loss 2.2956\n",
      "Epoch 1 Batch 1917 Loss 2.1460\n",
      "Epoch 1 Batch 1918 Loss 1.9070\n",
      "Epoch 1 Batch 1919 Loss 1.6421\n",
      "Epoch 1 Batch 1920 Loss 1.5645\n",
      "Epoch 1 Batch 1921 Loss 1.9378\n",
      "Epoch 1 Batch 1922 Loss 1.9983\n",
      "Epoch 1 Batch 1923 Loss 2.2635\n",
      "Epoch 1 Batch 1924 Loss 2.0835\n",
      "Epoch 1 Batch 1925 Loss 1.9687\n",
      "Epoch 1 Batch 1926 Loss 2.2936\n",
      "Epoch 1 Batch 1927 Loss 2.2738\n",
      "Epoch 1 Batch 1928 Loss 2.1258\n",
      "Epoch 1 Batch 1929 Loss 1.8767\n",
      "Epoch 1 Batch 1930 Loss 1.5330\n",
      "Epoch 1 Batch 1931 Loss 2.0711\n",
      "Epoch 1 Batch 1932 Loss 1.8986\n",
      "Epoch 1 Batch 1933 Loss 2.2708\n",
      "Epoch 1 Batch 1934 Loss 2.3251\n",
      "Epoch 1 Batch 1935 Loss 1.7672\n",
      "Epoch 1 Batch 1936 Loss 2.7452\n",
      "Epoch 1 Batch 1937 Loss 2.1979\n",
      "Epoch 1 Batch 1938 Loss 1.5011\n",
      "Epoch 1 Batch 1939 Loss 1.8475\n",
      "Epoch 1 Batch 1940 Loss 2.4968\n",
      "Epoch 1 Batch 1941 Loss 2.3218\n",
      "Epoch 1 Batch 1942 Loss 1.7916\n",
      "Epoch 1 Batch 1943 Loss 2.4123\n",
      "Epoch 1 Batch 1944 Loss 2.3122\n",
      "Epoch 1 Batch 1945 Loss 1.7900\n",
      "Epoch 1 Batch 1946 Loss 1.9722\n",
      "Epoch 1 Batch 1947 Loss 2.7765\n",
      "Epoch 1 Batch 1948 Loss 2.0166\n",
      "Epoch 1 Batch 1949 Loss 1.9613\n",
      "Epoch 1 Batch 1950 Loss 1.9408\n",
      "Epoch 1 Batch 1951 Loss 1.9650\n",
      "Epoch 1 Batch 1952 Loss 1.9249\n",
      "Epoch 1 Batch 1953 Loss 1.7331\n",
      "Epoch 1 Batch 1954 Loss 1.9561\n",
      "Epoch 1 Batch 1955 Loss 2.5428\n",
      "Epoch 1 Batch 1956 Loss 2.4569\n",
      "Epoch 1 Batch 1957 Loss 2.5110\n",
      "Epoch 1 Batch 1958 Loss 1.7547\n",
      "Epoch 1 Batch 1959 Loss 2.2816\n",
      "Epoch 1 Batch 1960 Loss 2.0714\n",
      "Epoch 1 Batch 1961 Loss 2.1773\n",
      "Epoch 1 Batch 1962 Loss 2.1829\n",
      "Epoch 1 Batch 1963 Loss 2.1063\n",
      "Epoch 1 Batch 1964 Loss 2.2158\n",
      "Epoch 1 Batch 1965 Loss 1.9959\n",
      "Epoch 1 Batch 1966 Loss 1.5518\n",
      "Epoch 1 Batch 1967 Loss 1.8461\n",
      "Epoch 1 Batch 1968 Loss 1.9768\n",
      "Epoch 1 Batch 1969 Loss 2.0869\n",
      "Epoch 1 Batch 1970 Loss 2.6457\n",
      "Epoch 1 Batch 1971 Loss 2.4176\n",
      "Epoch 1 Batch 1972 Loss 2.7226\n",
      "Epoch 1 Batch 1973 Loss 1.9651\n",
      "Epoch 1 Batch 1974 Loss 2.0978\n",
      "Epoch 1 Batch 1975 Loss 2.3174\n",
      "Epoch 1 Batch 1976 Loss 1.8633\n",
      "Epoch 1 Batch 1977 Loss 1.9756\n",
      "Epoch 1 Batch 1978 Loss 1.9091\n",
      "Epoch 1 Batch 1979 Loss 2.0223\n",
      "Epoch 1 Batch 1980 Loss 1.8802\n",
      "Epoch 1 Batch 1981 Loss 1.4187\n",
      "Epoch 1 Batch 1982 Loss 1.9766\n",
      "Epoch 1 Batch 1983 Loss 1.8592\n",
      "Epoch 1 Batch 1984 Loss 2.3921\n",
      "Epoch 1 Batch 1985 Loss 2.6457\n",
      "Epoch 1 Batch 1986 Loss 2.2233\n",
      "Epoch 1 Batch 1987 Loss 1.5226\n",
      "Epoch 1 Batch 1988 Loss 2.0177\n",
      "Epoch 1 Batch 1989 Loss 1.8364\n",
      "Epoch 1 Batch 1990 Loss 1.6492\n",
      "Epoch 1 Batch 1991 Loss 1.5375\n",
      "Epoch 1 Batch 1992 Loss 1.8011\n",
      "Epoch 1 Batch 1993 Loss 1.7651\n",
      "Epoch 1 Batch 1994 Loss 1.9438\n",
      "Epoch 1 Batch 1995 Loss 2.0321\n",
      "Epoch 1 Batch 1996 Loss 1.9452\n",
      "Epoch 1 Batch 1997 Loss 1.3429\n",
      "Epoch 1 Batch 1998 Loss 1.7680\n",
      "Epoch 1 Batch 1999 Loss 2.0813\n",
      "Epoch 1 Batch 2000 Loss 1.9971\n",
      "Epoch 1 Batch 2001 Loss 2.1973\n",
      "Epoch 1 Batch 2002 Loss 2.2967\n",
      "Epoch 1 Batch 2003 Loss 2.6491\n",
      "Epoch 1 Batch 2004 Loss 2.2654\n",
      "Epoch 1 Batch 2005 Loss 2.4834\n",
      "Epoch 1 Batch 2006 Loss 2.0227\n",
      "Epoch 1 Batch 2007 Loss 2.3154\n",
      "Epoch 1 Batch 2008 Loss 2.1410\n",
      "Epoch 1 Batch 2009 Loss 2.1997\n",
      "Epoch 1 Batch 2010 Loss 2.1674\n",
      "Epoch 1 Batch 2011 Loss 2.4556\n",
      "Epoch 1 Batch 2012 Loss 2.2486\n",
      "Epoch 1 Batch 2013 Loss 2.4957\n",
      "Epoch 1 Batch 2014 Loss 1.9545\n",
      "Epoch 1 Batch 2015 Loss 1.7851\n",
      "Epoch 1 Batch 2016 Loss 1.7736\n",
      "Epoch 1 Batch 2017 Loss 1.7974\n",
      "Epoch 1 Batch 2018 Loss 2.1416\n",
      "Epoch 1 Batch 2019 Loss 2.1897\n",
      "Epoch 1 Batch 2020 Loss 2.2614\n",
      "Epoch 1 Batch 2021 Loss 2.2826\n",
      "Epoch 1 Batch 2022 Loss 2.3307\n",
      "Epoch 1 Batch 2023 Loss 2.4671\n",
      "Epoch 1 Batch 2024 Loss 1.6489\n",
      "Epoch 1 Batch 2025 Loss 1.6921\n",
      "Epoch 1 Batch 2026 Loss 1.3683\n",
      "Epoch 1 Batch 2027 Loss 1.5394\n",
      "Epoch 1 Batch 2028 Loss 1.7651\n",
      "Epoch 1 Batch 2029 Loss 2.1299\n",
      "Epoch 1 Batch 2030 Loss 1.7851\n",
      "Epoch 1 Batch 2031 Loss 1.8222\n",
      "Epoch 1 Batch 2032 Loss 1.9205\n",
      "Epoch 1 Batch 2033 Loss 2.3396\n",
      "Epoch 1 Batch 2034 Loss 1.5607\n",
      "Epoch 1 Batch 2035 Loss 1.9177\n",
      "Epoch 1 Batch 2036 Loss 1.9690\n",
      "Epoch 1 Batch 2037 Loss 2.4136\n",
      "Epoch 1 Batch 2038 Loss 2.0027\n",
      "Epoch 1 Batch 2039 Loss 1.9768\n",
      "Epoch 1 Batch 2040 Loss 1.5243\n",
      "Epoch 1 Batch 2041 Loss 1.9288\n",
      "Epoch 1 Batch 2042 Loss 2.1780\n",
      "Epoch 1 Batch 2043 Loss 2.1740\n",
      "Epoch 1 Batch 2044 Loss 2.2624\n",
      "Epoch 1 Batch 2045 Loss 1.9921\n",
      "Epoch 1 Batch 2046 Loss 2.1195\n",
      "Epoch 1 Batch 2047 Loss 2.0447\n",
      "Epoch 1 Batch 2048 Loss 1.5847\n",
      "Epoch 1 Batch 2049 Loss 1.9284\n",
      "Epoch 1 Batch 2050 Loss 1.8188\n",
      "Epoch 1 Batch 2051 Loss 2.2072\n",
      "Epoch 1 Batch 2052 Loss 2.0731\n",
      "Epoch 1 Batch 2053 Loss 1.8787\n",
      "Epoch 1 Batch 2054 Loss 2.1250\n",
      "Epoch 1 Batch 2055 Loss 1.9846\n",
      "Epoch 1 Batch 2056 Loss 2.1514\n",
      "Epoch 1 Batch 2057 Loss 1.3126\n",
      "Epoch 1 Batch 2058 Loss 1.8131\n",
      "Epoch 1 Batch 2059 Loss 1.8888\n",
      "Epoch 1 Batch 2060 Loss 2.2266\n",
      "Epoch 1 Batch 2061 Loss 2.4767\n",
      "Epoch 1 Batch 2062 Loss 1.9180\n",
      "Epoch 1 Batch 2063 Loss 2.2158\n",
      "Epoch 1 Batch 2064 Loss 2.4059\n",
      "Epoch 1 Batch 2065 Loss 2.4316\n",
      "Epoch 1 Batch 2066 Loss 1.9383\n",
      "Epoch 1 Batch 2067 Loss 2.2205\n",
      "Epoch 1 Batch 2068 Loss 2.4630\n",
      "Epoch 1 Batch 2069 Loss 2.5227\n",
      "Epoch 1 Batch 2070 Loss 2.1244\n",
      "Epoch 1 Batch 2071 Loss 2.1234\n",
      "Epoch 1 Batch 2072 Loss 1.8699\n",
      "Epoch 1 Batch 2073 Loss 1.9066\n",
      "Epoch 1 Batch 2074 Loss 2.2140\n",
      "Epoch 1 Batch 2075 Loss 3.0514\n",
      "Epoch 1 Batch 2076 Loss 2.1077\n",
      "Epoch 1 Batch 2077 Loss 2.6025\n",
      "Epoch 1 Batch 2078 Loss 1.8081\n",
      "Epoch 1 Batch 2079 Loss 1.4826\n",
      "Epoch 1 Batch 2080 Loss 2.2756\n",
      "Epoch 1 Batch 2081 Loss 1.9145\n",
      "Epoch 1 Batch 2082 Loss 1.8801\n",
      "Epoch 1 Batch 2083 Loss 1.9810\n",
      "Epoch 1 Batch 2084 Loss 2.0145\n",
      "Epoch 1 Batch 2085 Loss 1.8816\n",
      "Epoch 1 Batch 2086 Loss 1.2021\n",
      "Epoch 1 Batch 2087 Loss 1.9417\n",
      "Epoch 1 Batch 2088 Loss 1.9263\n",
      "Epoch 1 Batch 2089 Loss 1.9118\n",
      "Epoch 1 Batch 2090 Loss 1.9907\n",
      "Epoch 1 Batch 2091 Loss 1.9512\n",
      "Epoch 1 Batch 2092 Loss 2.4932\n",
      "Epoch 1 Batch 2093 Loss 2.8993\n",
      "Epoch 1 Batch 2094 Loss 1.8019\n",
      "Epoch 1 Batch 2095 Loss 2.1157\n",
      "Epoch 1 Batch 2096 Loss 2.3230\n",
      "Epoch 1 Batch 2097 Loss 2.5378\n",
      "Epoch 1 Batch 2098 Loss 2.7553\n",
      "Epoch 1 Batch 2099 Loss 1.7243\n",
      "Epoch 1 Batch 2100 Loss 2.0552\n",
      "Epoch 1 Batch 2101 Loss 2.2797\n",
      "Epoch 1 Batch 2102 Loss 2.1236\n",
      "Epoch 1 Batch 2103 Loss 2.3275\n",
      "Epoch 1 Batch 2104 Loss 2.2670\n",
      "Epoch 1 Batch 2105 Loss 1.7488\n",
      "Epoch 1 Batch 2106 Loss 2.0046\n",
      "Epoch 1 Batch 2107 Loss 2.6842\n",
      "Epoch 1 Batch 2108 Loss 2.4836\n",
      "Epoch 1 Batch 2109 Loss 1.7875\n",
      "Epoch 1 Batch 2110 Loss 1.5999\n",
      "Epoch 1 Batch 2111 Loss 1.8575\n",
      "Epoch 1 Batch 2112 Loss 1.5487\n",
      "Epoch 1 Batch 2113 Loss 1.8555\n",
      "Epoch 1 Batch 2114 Loss 2.0926\n",
      "Epoch 1 Batch 2115 Loss 2.0926\n",
      "Epoch 1 Batch 2116 Loss 2.1760\n",
      "Epoch 1 Batch 2117 Loss 1.5889\n",
      "Epoch 1 Batch 2118 Loss 1.9416\n",
      "Epoch 1 Batch 2119 Loss 1.6872\n",
      "Epoch 1 Batch 2120 Loss 1.7407\n",
      "Epoch 1 Batch 2121 Loss 1.8513\n",
      "Epoch 1 Batch 2122 Loss 2.3531\n",
      "Epoch 1 Batch 2123 Loss 2.0533\n",
      "Epoch 1 Batch 2124 Loss 2.2509\n",
      "Epoch 1 Batch 2125 Loss 2.2455\n",
      "Epoch 1 Batch 2126 Loss 2.9565\n",
      "Epoch 1 Batch 2127 Loss 3.0002\n",
      "Epoch 1 Batch 2128 Loss 2.3625\n",
      "Epoch 1 Batch 2129 Loss 2.8212\n",
      "Epoch 1 Batch 2130 Loss 1.9848\n",
      "Epoch 1 Batch 2131 Loss 2.1131\n",
      "Epoch 1 Batch 2132 Loss 2.2616\n",
      "Epoch 1 Batch 2133 Loss 2.1037\n",
      "Epoch 1 Batch 2134 Loss 1.8984\n",
      "Epoch 1 Batch 2135 Loss 1.9900\n",
      "Epoch 1 Batch 2136 Loss 1.9540\n",
      "Epoch 1 Batch 2137 Loss 2.0418\n",
      "Epoch 1 Batch 2138 Loss 2.3265\n",
      "Epoch 1 Batch 2139 Loss 2.4411\n",
      "Epoch 1 Batch 2140 Loss 2.4833\n",
      "Epoch 1 Batch 2141 Loss 2.8689\n",
      "Epoch 1 Batch 2142 Loss 2.3148\n",
      "Epoch 1 Batch 2143 Loss 2.4400\n",
      "Epoch 1 Batch 2144 Loss 2.2960\n",
      "Epoch 1 Batch 2145 Loss 1.8701\n",
      "Epoch 1 Batch 2146 Loss 1.7371\n",
      "Epoch 1 Batch 2147 Loss 1.9963\n",
      "Epoch 1 Batch 2148 Loss 1.6285\n",
      "Epoch 1 Batch 2149 Loss 1.8683\n",
      "Epoch 1 Batch 2150 Loss 2.5696\n",
      "Epoch 1 Batch 2151 Loss 2.6512\n",
      "Epoch 1 Batch 2152 Loss 2.6433\n",
      "Epoch 1 Batch 2153 Loss 2.4501\n",
      "Epoch 1 Batch 2154 Loss 2.4400\n",
      "Epoch 1 Batch 2155 Loss 3.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 2156 Loss 2.5878\n",
      "Epoch 1 Batch 2157 Loss 2.7671\n",
      "Epoch 1 Batch 2158 Loss 2.2817\n",
      "Epoch 1 Batch 2159 Loss 2.2082\n",
      "Epoch 1 Batch 2160 Loss 1.9927\n",
      "Epoch 1 Batch 2161 Loss 1.9696\n",
      "Epoch 1 Batch 2162 Loss 2.3299\n",
      "Epoch 1 Batch 2163 Loss 2.3940\n",
      "Epoch 1 Batch 2164 Loss 1.9304\n",
      "Epoch 1 Batch 2165 Loss 1.9813\n",
      "Epoch 1 Batch 2166 Loss 2.0056\n",
      "Epoch 1 Batch 2167 Loss 2.4268\n",
      "Epoch 1 Batch 2168 Loss 1.3391\n",
      "Epoch 1 Batch 2169 Loss 1.7596\n",
      "Epoch 1 Batch 2170 Loss 1.7103\n",
      "Epoch 1 Batch 2171 Loss 1.9348\n",
      "Epoch 1 Batch 2172 Loss 1.7945\n",
      "Epoch 1 Batch 2173 Loss 1.8460\n",
      "Epoch 1 Batch 2174 Loss 1.6604\n",
      "Epoch 1 Batch 2175 Loss 1.7494\n",
      "Epoch 1 Batch 2176 Loss 1.9226\n",
      "Epoch 1 Batch 2177 Loss 2.3602\n",
      "Epoch 1 Batch 2178 Loss 2.0529\n",
      "Epoch 1 Batch 2179 Loss 2.5452\n",
      "Epoch 1 Batch 2180 Loss 2.4591\n",
      "Epoch 1 Batch 2181 Loss 2.2445\n",
      "Epoch 1 Batch 2182 Loss 2.1897\n",
      "Epoch 1 Batch 2183 Loss 1.6442\n",
      "Epoch 1 Batch 2184 Loss 1.8325\n",
      "Epoch 1 Batch 2185 Loss 1.9808\n",
      "Epoch 1 Batch 2186 Loss 2.3138\n",
      "Epoch 1 Batch 2187 Loss 2.0417\n",
      "Epoch 1 Batch 2188 Loss 1.8829\n",
      "Epoch 1 Batch 2189 Loss 1.9248\n",
      "Epoch 1 Batch 2190 Loss 2.2376\n",
      "Epoch 1 Batch 2191 Loss 2.2576\n",
      "Epoch 1 Batch 2192 Loss 2.1137\n",
      "Epoch 1 Batch 2193 Loss 1.9168\n",
      "Epoch 1 Batch 2194 Loss 1.7349\n",
      "Epoch 1 Batch 2195 Loss 1.6718\n",
      "Epoch 1 Batch 2196 Loss 1.4833\n",
      "Epoch 1 Batch 2197 Loss 1.7799\n",
      "Epoch 1 Batch 2198 Loss 1.7379\n",
      "Epoch 1 Batch 2199 Loss 1.7736\n",
      "Epoch 1 Batch 2200 Loss 1.7564\n",
      "Epoch 1 Batch 2201 Loss 1.8206\n",
      "Epoch 1 Batch 2202 Loss 1.9813\n",
      "Epoch 1 Batch 2203 Loss 2.2513\n",
      "Epoch 1 Batch 2204 Loss 1.8813\n",
      "Epoch 1 Batch 2205 Loss 1.6504\n",
      "Epoch 1 Batch 2206 Loss 1.9696\n",
      "Epoch 1 Batch 2207 Loss 1.9017\n",
      "Epoch 1 Batch 2208 Loss 1.8346\n",
      "Epoch 1 Batch 2209 Loss 2.3973\n",
      "Epoch 1 Batch 2210 Loss 2.3695\n",
      "Epoch 1 Batch 2211 Loss 2.0662\n",
      "Epoch 1 Batch 2212 Loss 2.3493\n",
      "Epoch 1 Batch 2213 Loss 2.2034\n",
      "Epoch 1 Batch 2214 Loss 2.1237\n",
      "Epoch 1 Batch 2215 Loss 2.2829\n",
      "Epoch 1 Batch 2216 Loss 2.2274\n",
      "Epoch 1 Batch 2217 Loss 2.2346\n",
      "Epoch 1 Batch 2218 Loss 1.9945\n",
      "Epoch 1 Batch 2219 Loss 1.9496\n",
      "Epoch 1 Batch 2220 Loss 1.5042\n",
      "Epoch 1 Batch 2221 Loss 1.5566\n",
      "Epoch 1 Batch 2222 Loss 1.6713\n",
      "Epoch 1 Batch 2223 Loss 1.9421\n",
      "Epoch 1 Batch 2224 Loss 1.8029\n",
      "Epoch 1 Batch 2225 Loss 2.2149\n",
      "Epoch 1 Batch 2226 Loss 2.3557\n",
      "Epoch 1 Batch 2227 Loss 2.3366\n",
      "Epoch 1 Batch 2228 Loss 1.9140\n",
      "Epoch 1 Batch 2229 Loss 2.1872\n",
      "Epoch 1 Batch 2230 Loss 1.7143\n",
      "Epoch 1 Batch 2231 Loss 1.4486\n",
      "Epoch 1 Batch 2232 Loss 1.5599\n",
      "Epoch 1 Batch 2233 Loss 1.5361\n",
      "Epoch 1 Batch 2234 Loss 1.9528\n",
      "Epoch 1 Batch 2235 Loss 1.6848\n",
      "Epoch 1 Batch 2236 Loss 2.3460\n",
      "Epoch 1 Batch 2237 Loss 2.4670\n",
      "Epoch 1 Batch 2238 Loss 2.1563\n",
      "Epoch 1 Batch 2239 Loss 2.2480\n",
      "Epoch 1 Batch 2240 Loss 2.4255\n",
      "Epoch 1 Batch 2241 Loss 1.6867\n",
      "Epoch 1 Batch 2242 Loss 1.9952\n",
      "Epoch 1 Batch 2243 Loss 1.8501\n",
      "Epoch 1 Batch 2244 Loss 1.7950\n",
      "Epoch 1 Batch 2245 Loss 1.6417\n",
      "Epoch 1 Batch 2246 Loss 1.7402\n",
      "Epoch 1 Batch 2247 Loss 1.6587\n",
      "Epoch 1 Batch 2248 Loss 1.4920\n",
      "Epoch 1 Batch 2249 Loss 1.6868\n",
      "Epoch 1 Batch 2250 Loss 1.3916\n",
      "Epoch 1 Batch 2251 Loss 2.0268\n",
      "Epoch 1 Batch 2252 Loss 1.8552\n",
      "Epoch 1 Batch 2253 Loss 1.8871\n",
      "Epoch 1 Batch 2254 Loss 1.6538\n",
      "Epoch 1 Batch 2255 Loss 1.7023\n",
      "Epoch 1 Batch 2256 Loss 2.1358\n",
      "Epoch 1 Batch 2257 Loss 2.1212\n",
      "Epoch 1 Batch 2258 Loss 2.2068\n",
      "Epoch 1 Batch 2259 Loss 1.7163\n",
      "Epoch 1 Batch 2260 Loss 2.0261\n",
      "Epoch 1 Batch 2261 Loss 1.8119\n",
      "Epoch 1 Batch 2262 Loss 1.4527\n",
      "Epoch 1 Batch 2263 Loss 1.9776\n",
      "Epoch 1 Batch 2264 Loss 1.5811\n",
      "Epoch 1 Batch 2265 Loss 1.3309\n",
      "Epoch 1 Batch 2266 Loss 1.8515\n",
      "Epoch 1 Batch 2267 Loss 1.8383\n",
      "Epoch 1 Batch 2268 Loss 1.6184\n",
      "Epoch 1 Batch 2269 Loss 2.0689\n",
      "Epoch 1 Batch 2270 Loss 1.8389\n",
      "Epoch 1 Batch 2271 Loss 1.5456\n",
      "Epoch 1 Batch 2272 Loss 2.0027\n",
      "Epoch 1 Batch 2273 Loss 1.6441\n",
      "Epoch 1 Batch 2274 Loss 2.3430\n",
      "Epoch 1 Batch 2275 Loss 2.1396\n",
      "Epoch 1 Batch 2276 Loss 2.4144\n",
      "Epoch 1 Batch 2277 Loss 2.0500\n",
      "Epoch 1 Batch 2278 Loss 1.5726\n",
      "Epoch 1 Batch 2279 Loss 1.9460\n",
      "Epoch 1 Batch 2280 Loss 1.6575\n",
      "Epoch 1 Batch 2281 Loss 1.8574\n",
      "Epoch 1 Batch 2282 Loss 1.4186\n",
      "Epoch 1 Batch 2283 Loss 1.7166\n",
      "Epoch 1 Batch 2284 Loss 1.6965\n",
      "Epoch 1 Batch 2285 Loss 1.6941\n",
      "Epoch 1 Batch 2286 Loss 1.7271\n",
      "Epoch 1 Batch 2287 Loss 1.2922\n",
      "Epoch 1 Batch 2288 Loss 1.7294\n",
      "Epoch 1 Batch 2289 Loss 1.9844\n",
      "Epoch 1 Batch 2290 Loss 1.7037\n",
      "Epoch 1 Batch 2291 Loss 2.2282\n",
      "Epoch 1 Batch 2292 Loss 1.9326\n",
      "Epoch 1 Batch 2293 Loss 1.6427\n",
      "Epoch 1 Batch 2294 Loss 1.7415\n",
      "Epoch 1 Batch 2295 Loss 2.5301\n",
      "Epoch 1 Batch 2296 Loss 2.5722\n",
      "Epoch 1 Batch 2297 Loss 1.4274\n",
      "Epoch 1 Batch 2298 Loss 1.8515\n",
      "Epoch 1 Batch 2299 Loss 2.0132\n",
      "Epoch 1 Batch 2300 Loss 1.7069\n",
      "Epoch 1 Batch 2301 Loss 2.0983\n",
      "Epoch 1 Batch 2302 Loss 1.6903\n",
      "Epoch 1 Batch 2303 Loss 1.5650\n",
      "Epoch 1 Batch 2304 Loss 2.1707\n",
      "Epoch 1 Batch 2305 Loss 2.0343\n",
      "Epoch 1 Batch 2306 Loss 2.1441\n",
      "Epoch 1 Batch 2307 Loss 2.0273\n",
      "Epoch 1 Batch 2308 Loss 1.8116\n",
      "Epoch 1 Batch 2309 Loss 1.5911\n",
      "Epoch 1 Batch 2310 Loss 1.2300\n",
      "Epoch 1 Batch 2311 Loss 1.7042\n",
      "Epoch 1 Batch 2312 Loss 2.4909\n",
      "Epoch 1 Batch 2313 Loss 2.3849\n",
      "Epoch 1 Batch 2314 Loss 2.2094\n",
      "Epoch 1 Batch 2315 Loss 1.6057\n",
      "Epoch 1 Batch 2316 Loss 1.9038\n",
      "Epoch 1 Batch 2317 Loss 2.3089\n",
      "Epoch 1 Batch 2318 Loss 2.2538\n",
      "Epoch 1 Batch 2319 Loss 2.0311\n",
      "Epoch 1 Batch 2320 Loss 1.7949\n",
      "Epoch 1 Batch 2321 Loss 1.8304\n",
      "Epoch 1 Batch 2322 Loss 1.9408\n",
      "Epoch 1 Batch 2323 Loss 1.8569\n",
      "Epoch 1 Batch 2324 Loss 1.7697\n",
      "Epoch 1 Batch 2325 Loss 2.1827\n",
      "Epoch 1 Batch 2326 Loss 2.0427\n",
      "Epoch 1 Batch 2327 Loss 1.3229\n",
      "Epoch 1 Batch 2328 Loss 1.6383\n",
      "Epoch 1 Batch 2329 Loss 1.3799\n",
      "Epoch 1 Batch 2330 Loss 1.6882\n",
      "Epoch 1 Batch 2331 Loss 1.4152\n",
      "Epoch 1 Batch 2332 Loss 1.3764\n",
      "Epoch 1 Batch 2333 Loss 1.0539\n",
      "Epoch 1 Batch 2334 Loss 1.8587\n",
      "Epoch 1 Batch 2335 Loss 1.5943\n",
      "Epoch 1 Batch 2336 Loss 1.4869\n",
      "Epoch 1 Batch 2337 Loss 1.3940\n",
      "Epoch 1 Batch 2338 Loss 1.6051\n",
      "Epoch 1 Batch 2339 Loss 2.0765\n",
      "Epoch 1 Batch 2340 Loss 1.6344\n",
      "Epoch 1 Batch 2341 Loss 1.6473\n",
      "Epoch 1 Batch 2342 Loss 1.6329\n",
      "Epoch 1 Batch 2343 Loss 1.7553\n",
      "Epoch 1 Batch 2344 Loss 1.8944\n",
      "Epoch 1 Batch 2345 Loss 1.8274\n",
      "Epoch 1 Batch 2346 Loss 2.2704\n",
      "Epoch 1 Batch 2347 Loss 2.0252\n",
      "Epoch 1 Batch 2348 Loss 1.5095\n",
      "Epoch 1 Batch 2349 Loss 1.8497\n",
      "Epoch 1 Batch 2350 Loss 1.9604\n",
      "Epoch 1 Batch 2351 Loss 1.6964\n",
      "Epoch 1 Batch 2352 Loss 2.1355\n",
      "Epoch 1 Batch 2353 Loss 1.8546\n",
      "Epoch 1 Batch 2354 Loss 1.4800\n",
      "Epoch 1 Batch 2355 Loss 1.8065\n",
      "Epoch 1 Batch 2356 Loss 1.6003\n",
      "Epoch 1 Batch 2357 Loss 1.5191\n",
      "Epoch 1 Batch 2358 Loss 1.5671\n",
      "Epoch 1 Batch 2359 Loss 1.7533\n",
      "Epoch 1 Batch 2360 Loss 1.3500\n",
      "Epoch 1 Batch 2361 Loss 1.5641\n",
      "Epoch 1 Batch 2362 Loss 1.7807\n",
      "Epoch 1 Batch 2363 Loss 2.1259\n",
      "Epoch 1 Batch 2364 Loss 1.6276\n",
      "Epoch 1 Batch 2365 Loss 1.5041\n",
      "Epoch 1 Batch 2366 Loss 1.7145\n",
      "Epoch 1 Batch 2367 Loss 1.4634\n",
      "Epoch 1 Batch 2368 Loss 1.8256\n",
      "Epoch 1 Batch 2369 Loss 1.7950\n",
      "Epoch 1 Batch 2370 Loss 1.6416\n",
      "Epoch 1 Batch 2371 Loss 1.3781\n",
      "Epoch 1 Batch 2372 Loss 1.4253\n",
      "Epoch 1 Batch 2373 Loss 1.4508\n",
      "Epoch 1 Batch 2374 Loss 1.2599\n",
      "Epoch 1 Batch 2375 Loss 1.7012\n",
      "Epoch 1 Batch 2376 Loss 1.4007\n",
      "Epoch 1 Batch 2377 Loss 1.5052\n",
      "Epoch 1 Batch 2378 Loss 1.7529\n",
      "Epoch 1 Batch 2379 Loss 1.7232\n",
      "Epoch 1 Batch 2380 Loss 1.7129\n",
      "Epoch 1 Batch 2381 Loss 1.6420\n",
      "Epoch 1 Batch 2382 Loss 1.9800\n",
      "Epoch 1 Batch 2383 Loss 2.2726\n",
      "Epoch 1 Batch 2384 Loss 1.7897\n",
      "Epoch 1 Batch 2385 Loss 1.9640\n",
      "Epoch 1 Batch 2386 Loss 1.8237\n",
      "Epoch 1 Batch 2387 Loss 2.0536\n",
      "Epoch 1 Batch 2388 Loss 2.3067\n",
      "Epoch 1 Batch 2389 Loss 1.9555\n",
      "Epoch 1 Batch 2390 Loss 1.6764\n",
      "Epoch 1 Batch 2391 Loss 2.0261\n",
      "Epoch 1 Batch 2392 Loss 1.7280\n",
      "Epoch 1 Batch 2393 Loss 1.7967\n",
      "Epoch 1 Batch 2394 Loss 1.5304\n",
      "Epoch 1 Batch 2395 Loss 1.4064\n",
      "Epoch 1 Batch 2396 Loss 1.8872\n",
      "Epoch 1 Batch 2397 Loss 1.8812\n",
      "Epoch 1 Batch 2398 Loss 1.6764\n",
      "Epoch 1 Batch 2399 Loss 1.1721\n",
      "Epoch 1 Batch 2400 Loss 1.4228\n",
      "Epoch 1 Batch 2401 Loss 1.6322\n",
      "Epoch 1 Batch 2402 Loss 1.5326\n",
      "Epoch 1 Batch 2403 Loss 1.9771\n",
      "Epoch 1 Batch 2404 Loss 1.9147\n",
      "Epoch 1 Batch 2405 Loss 1.9606\n",
      "Epoch 1 Batch 2406 Loss 1.7823\n",
      "Epoch 1 Batch 2407 Loss 2.0795\n",
      "Epoch 1 Batch 2408 Loss 2.3116\n",
      "Epoch 1 Batch 2409 Loss 2.0118\n",
      "Epoch 1 Batch 2410 Loss 2.0173\n",
      "Epoch 1 Batch 2411 Loss 1.4101\n",
      "Epoch 1 Batch 2412 Loss 1.5953\n",
      "Epoch 1 Batch 2413 Loss 1.6284\n",
      "Epoch 1 Batch 2414 Loss 1.3148\n",
      "Epoch 1 Batch 2415 Loss 1.9937\n",
      "Epoch 1 Batch 2416 Loss 1.7354\n",
      "Epoch 1 Batch 2417 Loss 1.6779\n",
      "Epoch 1 Batch 2418 Loss 1.3772\n",
      "Epoch 1 Batch 2419 Loss 1.6430\n",
      "Epoch 1 Batch 2420 Loss 1.2117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 2421 Loss 1.1731\n",
      "Epoch 1 Batch 2422 Loss 1.3447\n",
      "Epoch 1 Batch 2423 Loss 1.6988\n",
      "Epoch 1 Batch 2424 Loss 1.6421\n",
      "Epoch 1 Batch 2425 Loss 2.0204\n",
      "Epoch 1 Batch 2426 Loss 2.1050\n",
      "Epoch 1 Batch 2427 Loss 2.2699\n",
      "Epoch 1 Batch 2428 Loss 1.7757\n",
      "Epoch 1 Batch 2429 Loss 1.3712\n",
      "Epoch 1 Batch 2430 Loss 1.3357\n",
      "Epoch 1 Batch 2431 Loss 1.7401\n",
      "Epoch 1 Batch 2432 Loss 1.4299\n",
      "Epoch 1 Batch 2433 Loss 1.6912\n",
      "Epoch 1 Batch 2434 Loss 1.5623\n",
      "Epoch 1 Batch 2435 Loss 1.4666\n",
      "Epoch 1 Batch 2436 Loss 1.5178\n",
      "Epoch 1 Batch 2437 Loss 1.5787\n",
      "Epoch 1 Batch 2438 Loss 1.6110\n",
      "Epoch 1 Batch 2439 Loss 1.8370\n",
      "Epoch 1 Batch 2440 Loss 1.4826\n",
      "Epoch 1 Batch 2441 Loss 1.2456\n",
      "Epoch 1 Batch 2442 Loss 1.3944\n",
      "Epoch 1 Batch 2443 Loss 1.3060\n",
      "Epoch 1 Batch 2444 Loss 1.5318\n",
      "Epoch 1 Batch 2445 Loss 1.7917\n",
      "Epoch 1 Batch 2446 Loss 2.0239\n",
      "Epoch 1 Batch 2447 Loss 1.3499\n",
      "Epoch 1 Batch 2448 Loss 1.1558\n",
      "Epoch 1 Batch 2449 Loss 1.3024\n",
      "Epoch 1 Batch 2450 Loss 1.8391\n",
      "Epoch 1 Batch 2451 Loss 2.2103\n",
      "Epoch 1 Batch 2452 Loss 2.0204\n",
      "Epoch 1 Batch 2453 Loss 2.0463\n",
      "Epoch 1 Batch 2454 Loss 2.2221\n",
      "Epoch 1 Batch 2455 Loss 2.1701\n",
      "Epoch 1 Batch 2456 Loss 2.0110\n",
      "Epoch 1 Batch 2457 Loss 1.9681\n",
      "Epoch 1 Batch 2458 Loss 2.0605\n",
      "Epoch 1 Batch 2459 Loss 1.6750\n",
      "Epoch 1 Batch 2460 Loss 1.9695\n",
      "Epoch 1 Batch 2461 Loss 1.8257\n",
      "Epoch 1 Batch 2462 Loss 1.5500\n",
      "Epoch 1 Batch 2463 Loss 1.4576\n",
      "Epoch 1 Batch 2464 Loss 1.5214\n",
      "Epoch 1 Batch 2465 Loss 1.2600\n",
      "Epoch 1 Batch 2466 Loss 1.3120\n",
      "Epoch 1 Batch 2467 Loss 1.9616\n",
      "Epoch 1 Batch 2468 Loss 1.5844\n",
      "Epoch 1 Batch 2469 Loss 1.7886\n",
      "Epoch 1 Batch 2470 Loss 1.7740\n",
      "Epoch 1 Batch 2471 Loss 2.1643\n",
      "Epoch 1 Batch 2472 Loss 2.0453\n",
      "Epoch 1 Batch 2473 Loss 2.2881\n",
      "Epoch 1 Batch 2474 Loss 1.6875\n",
      "Epoch 1 Batch 2475 Loss 1.6381\n",
      "Epoch 1 Batch 2476 Loss 1.5104\n",
      "Epoch 1 Batch 2477 Loss 1.2593\n",
      "Epoch 1 Batch 2478 Loss 1.2361\n",
      "Epoch 1 Batch 2479 Loss 1.3405\n",
      "Epoch 1 Batch 2480 Loss 1.2204\n",
      "Epoch 1 Batch 2481 Loss 1.5758\n",
      "Epoch 1 Batch 2482 Loss 1.5540\n",
      "Epoch 1 Batch 2483 Loss 1.5972\n",
      "Epoch 1 Batch 2484 Loss 1.9160\n",
      "Epoch 1 Batch 2485 Loss 1.7290\n",
      "Epoch 1 Batch 2486 Loss 1.6146\n",
      "Epoch 1 Batch 2487 Loss 1.5202\n",
      "Epoch 1 Batch 2488 Loss 1.6896\n",
      "Epoch 1 Batch 2489 Loss 2.0888\n",
      "Epoch 1 Batch 2490 Loss 1.6742\n",
      "Epoch 1 Batch 2491 Loss 1.8652\n",
      "Epoch 1 Batch 2492 Loss 2.0094\n",
      "Epoch 1 Batch 2493 Loss 2.3594\n",
      "Epoch 1 Batch 2494 Loss 1.9234\n",
      "Epoch 1 Batch 2495 Loss 1.3147\n",
      "Epoch 1 Batch 2496 Loss 2.0060\n",
      "Epoch 1 Batch 2497 Loss 1.5403\n",
      "Epoch 1 Batch 2498 Loss 1.9006\n",
      "Epoch 1 Batch 2499 Loss 1.8865\n",
      "Epoch 1 Batch 2500 Loss 1.6252\n",
      "Epoch 1 Batch 2501 Loss 1.5835\n",
      "Epoch 1 Batch 2502 Loss 1.5579\n",
      "Epoch 1 Batch 2503 Loss 1.3300\n",
      "Epoch 1 Batch 2504 Loss 1.8355\n",
      "Epoch 1 Batch 2505 Loss 1.7113\n",
      "Epoch 1 Batch 2506 Loss 1.5915\n",
      "Epoch 1 Batch 2507 Loss 2.0340\n",
      "Epoch 1 Batch 2508 Loss 2.0471\n",
      "Epoch 1 Batch 2509 Loss 1.1115\n",
      "Epoch 1 Batch 2510 Loss 1.3813\n",
      "Epoch 1 Batch 2511 Loss 1.5703\n",
      "Epoch 1 Batch 2512 Loss 1.9539\n",
      "Epoch 1 Batch 2513 Loss 2.1408\n",
      "Epoch 1 Batch 2514 Loss 2.3114\n",
      "Epoch 1 Batch 2515 Loss 1.6437\n",
      "Epoch 1 Batch 2516 Loss 1.5431\n",
      "Epoch 1 Batch 2517 Loss 1.5953\n",
      "Epoch 1 Batch 2518 Loss 1.5220\n",
      "Epoch 1 Batch 2519 Loss 1.8988\n",
      "Epoch 1 Batch 2520 Loss 1.9875\n",
      "Epoch 1 Batch 2521 Loss 1.6747\n",
      "Epoch 1 Batch 2522 Loss 1.8802\n",
      "Epoch 1 Batch 2523 Loss 1.9599\n",
      "Epoch 1 Batch 2524 Loss 1.7626\n",
      "Epoch 1 Batch 2525 Loss 1.6192\n",
      "Epoch 1 Batch 2526 Loss 2.0841\n",
      "Epoch 1 Batch 2527 Loss 2.2280\n",
      "Epoch 1 Batch 2528 Loss 2.4061\n",
      "Epoch 1 Batch 2529 Loss 2.2143\n",
      "Epoch 1 Batch 2530 Loss 1.7899\n",
      "Epoch 1 Batch 2531 Loss 1.8311\n",
      "Epoch 1 Batch 2532 Loss 1.7236\n",
      "Epoch 1 Batch 2533 Loss 1.5585\n",
      "Epoch 1 Batch 2534 Loss 1.8914\n",
      "Epoch 1 Batch 2535 Loss 1.7177\n",
      "Epoch 1 Batch 2536 Loss 1.6662\n",
      "Epoch 1 Batch 2537 Loss 1.3662\n",
      "Epoch 1 Batch 2538 Loss 1.5867\n",
      "Epoch 1 Batch 2539 Loss 1.5253\n",
      "Epoch 1 Batch 2540 Loss 1.7252\n",
      "Epoch 1 Batch 2541 Loss 1.4147\n",
      "Epoch 1 Batch 2542 Loss 1.4009\n",
      "Epoch 1 Batch 2543 Loss 1.5991\n",
      "Epoch 1 Batch 2544 Loss 1.4881\n",
      "Epoch 1 Batch 2545 Loss 1.8778\n",
      "Epoch 1 Batch 2546 Loss 1.5372\n",
      "Epoch 1 Batch 2547 Loss 1.7194\n",
      "Epoch 1 Batch 2548 Loss 2.0194\n",
      "Epoch 1 Batch 2549 Loss 2.0970\n",
      "Epoch 1 Batch 2550 Loss 2.1944\n",
      "Epoch 1 Batch 2551 Loss 1.5235\n",
      "Epoch 1 Batch 2552 Loss 1.7692\n",
      "Epoch 1 Batch 2553 Loss 2.0480\n",
      "Epoch 1 Batch 2554 Loss 2.0876\n",
      "Epoch 1 Batch 2555 Loss 2.0319\n",
      "Epoch 1 Batch 2556 Loss 1.8479\n",
      "Epoch 1 Batch 2557 Loss 1.7221\n",
      "Epoch 1 Batch 2558 Loss 1.6475\n",
      "Epoch 1 Batch 2559 Loss 1.6683\n",
      "Epoch 1 Batch 2560 Loss 1.3235\n",
      "Epoch 1 Batch 2561 Loss 1.2099\n",
      "Epoch 1 Batch 2562 Loss 1.3468\n",
      "Epoch 1 Batch 2563 Loss 1.6719\n",
      "Epoch 1 Batch 2564 Loss 1.4517\n",
      "Epoch 1 Batch 2565 Loss 1.2645\n",
      "Epoch 1 Batch 2566 Loss 1.4480\n",
      "Epoch 1 Batch 2567 Loss 1.5576\n",
      "Epoch 1 Batch 2568 Loss 1.7769\n",
      "Epoch 1 Batch 2569 Loss 1.3751\n",
      "Epoch 1 Batch 2570 Loss 1.8966\n",
      "Epoch 1 Batch 2571 Loss 1.9393\n",
      "Epoch 1 Batch 2572 Loss 1.9605\n",
      "Epoch 1 Batch 2573 Loss 1.9389\n",
      "Epoch 1 Batch 2574 Loss 1.9671\n",
      "Epoch 1 Batch 2575 Loss 1.5802\n",
      "Epoch 1 Batch 2576 Loss 1.8258\n",
      "Epoch 1 Batch 2577 Loss 1.2708\n",
      "Epoch 1 Batch 2578 Loss 1.6659\n",
      "Epoch 1 Batch 2579 Loss 1.6896\n",
      "Epoch 1 Batch 2580 Loss 1.5705\n",
      "Epoch 1 Batch 2581 Loss 1.5187\n",
      "Epoch 1 Batch 2582 Loss 2.0227\n",
      "Epoch 1 Batch 2583 Loss 1.7786\n",
      "Epoch 1 Batch 2584 Loss 2.3331\n",
      "Epoch 1 Batch 2585 Loss 2.0579\n",
      "Epoch 1 Batch 2586 Loss 1.5340\n",
      "Epoch 1 Batch 2587 Loss 1.3626\n",
      "Epoch 1 Batch 2588 Loss 1.5497\n",
      "Epoch 1 Batch 2589 Loss 1.5310\n",
      "Epoch 1 Batch 2590 Loss 1.3767\n",
      "Epoch 1 Loss 2.0969\n",
      "Time taken for 1 epoch 557.8615369796753 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 2.7798\n",
      "Epoch 2 Batch 1 Loss 2.3691\n",
      "Epoch 2 Batch 2 Loss 2.3069\n",
      "Epoch 2 Batch 3 Loss 2.6836\n",
      "Epoch 2 Batch 4 Loss 2.2820\n",
      "Epoch 2 Batch 5 Loss 2.1199\n",
      "Epoch 2 Batch 6 Loss 2.3334\n",
      "Epoch 2 Batch 7 Loss 2.2744\n",
      "Epoch 2 Batch 8 Loss 2.8233\n",
      "Epoch 2 Batch 9 Loss 2.6722\n",
      "Epoch 2 Batch 10 Loss 2.0251\n",
      "Epoch 2 Batch 11 Loss 1.6091\n",
      "Epoch 2 Batch 12 Loss 1.6586\n",
      "Epoch 2 Batch 13 Loss 1.3296\n",
      "Epoch 2 Batch 14 Loss 1.2031\n",
      "Epoch 2 Batch 15 Loss 1.5461\n",
      "Epoch 2 Batch 16 Loss 1.4879\n",
      "Epoch 2 Batch 17 Loss 1.3713\n",
      "Epoch 2 Batch 18 Loss 1.5195\n",
      "Epoch 2 Batch 19 Loss 2.4849\n",
      "Epoch 2 Batch 20 Loss 2.4148\n",
      "Epoch 2 Batch 21 Loss 2.0650\n",
      "Epoch 2 Batch 22 Loss 2.0544\n",
      "Epoch 2 Batch 23 Loss 1.7996\n",
      "Epoch 2 Batch 24 Loss 1.5042\n",
      "Epoch 2 Batch 25 Loss 1.3226\n",
      "Epoch 2 Batch 26 Loss 2.0158\n",
      "Epoch 2 Batch 27 Loss 2.2204\n",
      "Epoch 2 Batch 28 Loss 2.4025\n",
      "Epoch 2 Batch 29 Loss 2.0893\n",
      "Epoch 2 Batch 30 Loss 1.8008\n",
      "Epoch 2 Batch 31 Loss 1.8817\n",
      "Epoch 2 Batch 32 Loss 1.5054\n",
      "Epoch 2 Batch 33 Loss 1.3592\n",
      "Epoch 2 Batch 34 Loss 1.3286\n",
      "Epoch 2 Batch 35 Loss 1.2508\n",
      "Epoch 2 Batch 36 Loss 1.1140\n",
      "Epoch 2 Batch 37 Loss 0.9617\n",
      "Epoch 2 Batch 38 Loss 1.2425\n",
      "Epoch 2 Batch 39 Loss 1.2230\n",
      "Epoch 2 Batch 40 Loss 1.5257\n",
      "Epoch 2 Batch 41 Loss 1.3371\n",
      "Epoch 2 Batch 42 Loss 1.3854\n",
      "Epoch 2 Batch 43 Loss 1.2219\n",
      "Epoch 2 Batch 44 Loss 1.5451\n",
      "Epoch 2 Batch 45 Loss 1.7015\n",
      "Epoch 2 Batch 46 Loss 1.8773\n",
      "Epoch 2 Batch 47 Loss 1.7667\n",
      "Epoch 2 Batch 48 Loss 1.8419\n",
      "Epoch 2 Batch 49 Loss 2.0963\n",
      "Epoch 2 Batch 50 Loss 1.5127\n",
      "Epoch 2 Batch 51 Loss 1.6593\n",
      "Epoch 2 Batch 52 Loss 1.6901\n",
      "Epoch 2 Batch 53 Loss 1.8313\n",
      "Epoch 2 Batch 54 Loss 1.9131\n",
      "Epoch 2 Batch 55 Loss 1.7154\n",
      "Epoch 2 Batch 56 Loss 2.4847\n",
      "Epoch 2 Batch 57 Loss 2.9021\n",
      "Epoch 2 Batch 58 Loss 2.2364\n",
      "Epoch 2 Batch 59 Loss 2.0651\n",
      "Epoch 2 Batch 60 Loss 1.6696\n",
      "Epoch 2 Batch 61 Loss 1.7101\n",
      "Epoch 2 Batch 62 Loss 1.9403\n",
      "Epoch 2 Batch 63 Loss 2.1167\n",
      "Epoch 2 Batch 64 Loss 1.8767\n",
      "Epoch 2 Batch 65 Loss 1.5586\n",
      "Epoch 2 Batch 66 Loss 1.4602\n",
      "Epoch 2 Batch 67 Loss 1.5853\n",
      "Epoch 2 Batch 68 Loss 1.6044\n",
      "Epoch 2 Batch 69 Loss 1.7405\n",
      "Epoch 2 Batch 70 Loss 2.1873\n",
      "Epoch 2 Batch 71 Loss 1.8190\n",
      "Epoch 2 Batch 72 Loss 1.5542\n",
      "Epoch 2 Batch 73 Loss 2.0516\n",
      "Epoch 2 Batch 74 Loss 2.5688\n",
      "Epoch 2 Batch 75 Loss 1.9250\n",
      "Epoch 2 Batch 76 Loss 2.0656\n",
      "Epoch 2 Batch 77 Loss 1.9572\n",
      "Epoch 2 Batch 78 Loss 2.0372\n",
      "Epoch 2 Batch 79 Loss 2.0682\n",
      "Epoch 2 Batch 80 Loss 1.9532\n",
      "Epoch 2 Batch 81 Loss 1.7049\n",
      "Epoch 2 Batch 82 Loss 1.3182\n",
      "Epoch 2 Batch 83 Loss 1.7760\n",
      "Epoch 2 Batch 84 Loss 2.3064\n",
      "Epoch 2 Batch 85 Loss 1.9121\n",
      "Epoch 2 Batch 86 Loss 2.1258\n",
      "Epoch 2 Batch 87 Loss 2.2994\n",
      "Epoch 2 Batch 88 Loss 1.7048\n",
      "Epoch 2 Batch 89 Loss 1.7322\n",
      "Epoch 2 Batch 90 Loss 2.0097\n",
      "Epoch 2 Batch 91 Loss 1.4032\n",
      "Epoch 2 Batch 92 Loss 2.0501\n",
      "Epoch 2 Batch 93 Loss 1.5811\n",
      "Epoch 2 Batch 94 Loss 1.3897\n",
      "Epoch 2 Batch 95 Loss 1.5473\n",
      "Epoch 2 Batch 96 Loss 1.5154\n",
      "Epoch 2 Batch 97 Loss 2.0228\n",
      "Epoch 2 Batch 98 Loss 2.2063\n",
      "Epoch 2 Batch 99 Loss 2.2541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Batch 100 Loss 2.3936\n",
      "Epoch 2 Batch 101 Loss 2.3195\n",
      "Epoch 2 Batch 102 Loss 1.7828\n",
      "Epoch 2 Batch 103 Loss 1.8159\n",
      "Epoch 2 Batch 104 Loss 2.1964\n",
      "Epoch 2 Batch 105 Loss 2.3287\n",
      "Epoch 2 Batch 106 Loss 1.9103\n",
      "Epoch 2 Batch 107 Loss 2.2531\n",
      "Epoch 2 Batch 108 Loss 2.3847\n",
      "Epoch 2 Batch 109 Loss 1.9533\n",
      "Epoch 2 Batch 110 Loss 1.5149\n",
      "Epoch 2 Batch 111 Loss 1.2306\n",
      "Epoch 2 Batch 112 Loss 1.3896\n",
      "Epoch 2 Batch 113 Loss 2.1623\n",
      "Epoch 2 Batch 114 Loss 1.7363\n",
      "Epoch 2 Batch 115 Loss 1.5454\n",
      "Epoch 2 Batch 116 Loss 1.9604\n",
      "Epoch 2 Batch 117 Loss 1.5932\n",
      "Epoch 2 Batch 118 Loss 2.1242\n",
      "Epoch 2 Batch 119 Loss 1.9119\n",
      "Epoch 2 Batch 120 Loss 1.4624\n",
      "Epoch 2 Batch 121 Loss 1.2976\n",
      "Epoch 2 Batch 122 Loss 2.0307\n",
      "Epoch 2 Batch 123 Loss 1.6156\n",
      "Epoch 2 Batch 124 Loss 1.8505\n",
      "Epoch 2 Batch 125 Loss 1.5033\n",
      "Epoch 2 Batch 126 Loss 1.4752\n",
      "Epoch 2 Batch 127 Loss 1.8315\n",
      "Epoch 2 Batch 128 Loss 1.3561\n",
      "Epoch 2 Batch 129 Loss 1.6398\n",
      "Epoch 2 Batch 130 Loss 1.6611\n",
      "Epoch 2 Batch 131 Loss 1.5822\n",
      "Epoch 2 Batch 132 Loss 1.8288\n",
      "Epoch 2 Batch 133 Loss 1.8132\n",
      "Epoch 2 Batch 134 Loss 2.0950\n",
      "Epoch 2 Batch 135 Loss 1.5528\n",
      "Epoch 2 Batch 136 Loss 2.2114\n",
      "Epoch 2 Batch 137 Loss 1.8854\n",
      "Epoch 2 Batch 138 Loss 2.0280\n",
      "Epoch 2 Batch 139 Loss 1.5657\n",
      "Epoch 2 Batch 140 Loss 1.6927\n",
      "Epoch 2 Batch 141 Loss 1.6123\n",
      "Epoch 2 Batch 142 Loss 1.7469\n",
      "Epoch 2 Batch 143 Loss 2.1735\n",
      "Epoch 2 Batch 144 Loss 1.7193\n",
      "Epoch 2 Batch 145 Loss 1.8241\n",
      "Epoch 2 Batch 146 Loss 1.8980\n",
      "Epoch 2 Batch 147 Loss 1.5784\n",
      "Epoch 2 Batch 148 Loss 2.4603\n",
      "Epoch 2 Batch 149 Loss 2.0801\n",
      "Epoch 2 Batch 150 Loss 2.1320\n",
      "Epoch 2 Batch 151 Loss 1.8242\n",
      "Epoch 2 Batch 152 Loss 1.6783\n",
      "Epoch 2 Batch 153 Loss 1.7243\n",
      "Epoch 2 Batch 154 Loss 1.4301\n",
      "Epoch 2 Batch 155 Loss 1.9690\n",
      "Epoch 2 Batch 156 Loss 2.0496\n",
      "Epoch 2 Batch 157 Loss 2.4723\n",
      "Epoch 2 Batch 158 Loss 2.6481\n",
      "Epoch 2 Batch 159 Loss 2.0377\n",
      "Epoch 2 Batch 160 Loss 2.3538\n",
      "Epoch 2 Batch 161 Loss 1.8796\n",
      "Epoch 2 Batch 162 Loss 1.3671\n",
      "Epoch 2 Batch 163 Loss 1.3574\n",
      "Epoch 2 Batch 164 Loss 1.2532\n",
      "Epoch 2 Batch 165 Loss 1.8969\n",
      "Epoch 2 Batch 166 Loss 1.5238\n",
      "Epoch 2 Batch 167 Loss 1.4761\n",
      "Epoch 2 Batch 168 Loss 1.8508\n",
      "Epoch 2 Batch 169 Loss 1.8825\n",
      "Epoch 2 Batch 170 Loss 2.3021\n",
      "Epoch 2 Batch 171 Loss 1.7663\n",
      "Epoch 2 Batch 172 Loss 2.0432\n",
      "Epoch 2 Batch 173 Loss 1.5534\n",
      "Epoch 2 Batch 174 Loss 1.7289\n",
      "Epoch 2 Batch 175 Loss 1.6547\n",
      "Epoch 2 Batch 176 Loss 1.7239\n",
      "Epoch 2 Batch 177 Loss 1.5678\n",
      "Epoch 2 Batch 178 Loss 1.7553\n",
      "Epoch 2 Batch 179 Loss 1.8701\n",
      "Epoch 2 Batch 180 Loss 1.2757\n",
      "Epoch 2 Batch 181 Loss 1.5259\n",
      "Epoch 2 Batch 182 Loss 1.3718\n",
      "Epoch 2 Batch 183 Loss 1.3631\n",
      "Epoch 2 Batch 184 Loss 1.0984\n",
      "Epoch 2 Batch 185 Loss 1.3318\n",
      "Epoch 2 Batch 186 Loss 1.1053\n",
      "Epoch 2 Batch 187 Loss 1.2430\n",
      "Epoch 2 Batch 188 Loss 1.5563\n",
      "Epoch 2 Batch 189 Loss 1.4331\n",
      "Epoch 2 Batch 190 Loss 1.6274\n",
      "Epoch 2 Batch 191 Loss 1.9824\n",
      "Epoch 2 Batch 192 Loss 1.9420\n",
      "Epoch 2 Batch 193 Loss 2.5371\n",
      "Epoch 2 Batch 194 Loss 2.0394\n",
      "Epoch 2 Batch 195 Loss 2.0901\n",
      "Epoch 2 Batch 196 Loss 1.5512\n",
      "Epoch 2 Batch 197 Loss 1.9930\n",
      "Epoch 2 Batch 198 Loss 1.7059\n",
      "Epoch 2 Batch 199 Loss 1.7422\n",
      "Epoch 2 Batch 200 Loss 1.9585\n",
      "Epoch 2 Batch 201 Loss 1.6032\n",
      "Epoch 2 Batch 202 Loss 1.3185\n",
      "Epoch 2 Batch 203 Loss 1.7349\n",
      "Epoch 2 Batch 204 Loss 1.4620\n",
      "Epoch 2 Batch 205 Loss 1.6384\n",
      "Epoch 2 Batch 206 Loss 1.7458\n",
      "Epoch 2 Batch 207 Loss 1.8126\n",
      "Epoch 2 Batch 208 Loss 1.9837\n",
      "Epoch 2 Batch 209 Loss 1.5560\n",
      "Epoch 2 Batch 210 Loss 1.8513\n",
      "Epoch 2 Batch 211 Loss 1.2578\n",
      "Epoch 2 Batch 212 Loss 1.7175\n",
      "Epoch 2 Batch 213 Loss 1.9886\n",
      "Epoch 2 Batch 214 Loss 1.7625\n",
      "Epoch 2 Batch 215 Loss 2.0151\n",
      "Epoch 2 Batch 216 Loss 1.7131\n",
      "Epoch 2 Batch 217 Loss 1.7966\n",
      "Epoch 2 Batch 218 Loss 1.5979\n",
      "Epoch 2 Batch 219 Loss 1.7065\n",
      "Epoch 2 Batch 220 Loss 1.8448\n",
      "Epoch 2 Batch 221 Loss 1.7440\n",
      "Epoch 2 Batch 222 Loss 1.5842\n",
      "Epoch 2 Batch 223 Loss 1.6192\n",
      "Epoch 2 Batch 224 Loss 1.4531\n",
      "Epoch 2 Batch 225 Loss 1.3660\n",
      "Epoch 2 Batch 226 Loss 1.9574\n",
      "Epoch 2 Batch 227 Loss 1.5998\n",
      "Epoch 2 Batch 228 Loss 1.9443\n",
      "Epoch 2 Batch 229 Loss 1.8008\n",
      "Epoch 2 Batch 230 Loss 1.4364\n",
      "Epoch 2 Batch 231 Loss 1.3754\n",
      "Epoch 2 Batch 232 Loss 1.7007\n",
      "Epoch 2 Batch 233 Loss 1.1500\n",
      "Epoch 2 Batch 234 Loss 1.4167\n",
      "Epoch 2 Batch 235 Loss 1.1544\n",
      "Epoch 2 Batch 236 Loss 1.4275\n",
      "Epoch 2 Batch 237 Loss 2.0215\n",
      "Epoch 2 Batch 238 Loss 1.7150\n",
      "Epoch 2 Batch 239 Loss 1.4574\n",
      "Epoch 2 Batch 240 Loss 1.8850\n",
      "Epoch 2 Batch 241 Loss 1.9651\n",
      "Epoch 2 Batch 242 Loss 1.3657\n",
      "Epoch 2 Batch 243 Loss 1.7927\n",
      "Epoch 2 Batch 244 Loss 1.3788\n",
      "Epoch 2 Batch 245 Loss 1.9302\n",
      "Epoch 2 Batch 246 Loss 1.6392\n",
      "Epoch 2 Batch 247 Loss 1.8128\n",
      "Epoch 2 Batch 248 Loss 2.2665\n",
      "Epoch 2 Batch 249 Loss 1.9454\n",
      "Epoch 2 Batch 250 Loss 1.8229\n",
      "Epoch 2 Batch 251 Loss 1.8235\n",
      "Epoch 2 Batch 252 Loss 2.5035\n",
      "Epoch 2 Batch 253 Loss 2.1243\n",
      "Epoch 2 Batch 254 Loss 1.8861\n",
      "Epoch 2 Batch 255 Loss 1.6621\n",
      "Epoch 2 Batch 256 Loss 1.6773\n",
      "Epoch 2 Batch 257 Loss 1.5703\n",
      "Epoch 2 Batch 258 Loss 1.8518\n",
      "Epoch 2 Batch 259 Loss 1.6164\n",
      "Epoch 2 Batch 260 Loss 1.3595\n",
      "Epoch 2 Batch 261 Loss 1.4696\n",
      "Epoch 2 Batch 262 Loss 1.4207\n",
      "Epoch 2 Batch 263 Loss 1.7332\n",
      "Epoch 2 Batch 264 Loss 1.2230\n",
      "Epoch 2 Batch 265 Loss 1.6878\n",
      "Epoch 2 Batch 266 Loss 1.5643\n",
      "Epoch 2 Batch 267 Loss 1.4748\n",
      "Epoch 2 Batch 268 Loss 1.4881\n",
      "Epoch 2 Batch 269 Loss 1.9827\n",
      "Epoch 2 Batch 270 Loss 2.7525\n",
      "Epoch 2 Batch 271 Loss 2.1424\n",
      "Epoch 2 Batch 272 Loss 2.1850\n",
      "Epoch 2 Batch 273 Loss 2.3544\n",
      "Epoch 2 Batch 274 Loss 1.8305\n",
      "Epoch 2 Batch 275 Loss 1.8721\n",
      "Epoch 2 Batch 276 Loss 1.8353\n",
      "Epoch 2 Batch 277 Loss 1.7574\n",
      "Epoch 2 Batch 278 Loss 2.0680\n",
      "Epoch 2 Batch 279 Loss 1.8350\n",
      "Epoch 2 Batch 280 Loss 1.6681\n",
      "Epoch 2 Batch 281 Loss 1.7936\n",
      "Epoch 2 Batch 282 Loss 2.1162\n",
      "Epoch 2 Batch 283 Loss 2.0974\n",
      "Epoch 2 Batch 284 Loss 1.7272\n",
      "Epoch 2 Batch 285 Loss 1.3156\n",
      "Epoch 2 Batch 286 Loss 1.5059\n",
      "Epoch 2 Batch 287 Loss 1.4880\n",
      "Epoch 2 Batch 288 Loss 1.7116\n",
      "Epoch 2 Batch 289 Loss 1.8555\n",
      "Epoch 2 Batch 290 Loss 1.9503\n",
      "Epoch 2 Batch 291 Loss 1.4966\n",
      "Epoch 2 Batch 292 Loss 1.3415\n",
      "Epoch 2 Batch 293 Loss 1.6196\n",
      "Epoch 2 Batch 294 Loss 1.7022\n",
      "Epoch 2 Batch 295 Loss 1.6078\n",
      "Epoch 2 Batch 296 Loss 1.5645\n",
      "Epoch 2 Batch 297 Loss 1.3482\n",
      "Epoch 2 Batch 298 Loss 1.4039\n",
      "Epoch 2 Batch 299 Loss 1.4415\n",
      "Epoch 2 Batch 300 Loss 1.8582\n",
      "Epoch 2 Batch 301 Loss 1.5158\n",
      "Epoch 2 Batch 302 Loss 1.2366\n",
      "Epoch 2 Batch 303 Loss 1.3125\n",
      "Epoch 2 Batch 304 Loss 1.7559\n",
      "Epoch 2 Batch 305 Loss 1.8452\n",
      "Epoch 2 Batch 306 Loss 1.6877\n",
      "Epoch 2 Batch 307 Loss 1.8144\n",
      "Epoch 2 Batch 308 Loss 1.9128\n",
      "Epoch 2 Batch 309 Loss 1.8614\n",
      "Epoch 2 Batch 310 Loss 1.4011\n",
      "Epoch 2 Batch 311 Loss 2.0681\n",
      "Epoch 2 Batch 312 Loss 1.7399\n",
      "Epoch 2 Batch 313 Loss 1.8023\n",
      "Epoch 2 Batch 314 Loss 1.6654\n",
      "Epoch 2 Batch 315 Loss 1.3790\n",
      "Epoch 2 Batch 316 Loss 1.6093\n",
      "Epoch 2 Batch 317 Loss 1.7826\n",
      "Epoch 2 Batch 318 Loss 1.5507\n",
      "Epoch 2 Batch 319 Loss 1.6989\n",
      "Epoch 2 Batch 320 Loss 1.6932\n",
      "Epoch 2 Batch 321 Loss 1.9588\n",
      "Epoch 2 Batch 322 Loss 2.2700\n",
      "Epoch 2 Batch 323 Loss 1.2973\n",
      "Epoch 2 Batch 324 Loss 1.4542\n",
      "Epoch 2 Batch 325 Loss 1.4315\n",
      "Epoch 2 Batch 326 Loss 1.8565\n",
      "Epoch 2 Batch 327 Loss 1.5752\n",
      "Epoch 2 Batch 328 Loss 2.2559\n",
      "Epoch 2 Batch 329 Loss 1.6997\n",
      "Epoch 2 Batch 330 Loss 1.8265\n",
      "Epoch 2 Batch 331 Loss 1.4002\n",
      "Epoch 2 Batch 332 Loss 1.6036\n",
      "Epoch 2 Batch 333 Loss 1.5735\n",
      "Epoch 2 Batch 334 Loss 1.4062\n",
      "Epoch 2 Batch 335 Loss 1.3723\n",
      "Epoch 2 Batch 336 Loss 1.8622\n",
      "Epoch 2 Batch 337 Loss 1.6558\n",
      "Epoch 2 Batch 338 Loss 1.7429\n",
      "Epoch 2 Batch 339 Loss 1.9507\n",
      "Epoch 2 Batch 340 Loss 1.7360\n",
      "Epoch 2 Batch 341 Loss 2.0742\n",
      "Epoch 2 Batch 342 Loss 1.4605\n",
      "Epoch 2 Batch 343 Loss 1.9258\n",
      "Epoch 2 Batch 344 Loss 1.5560\n",
      "Epoch 2 Batch 345 Loss 1.4537\n",
      "Epoch 2 Batch 346 Loss 1.9206\n",
      "Epoch 2 Batch 347 Loss 1.9400\n",
      "Epoch 2 Batch 348 Loss 1.8007\n",
      "Epoch 2 Batch 349 Loss 2.0031\n",
      "Epoch 2 Batch 350 Loss 1.5093\n",
      "Epoch 2 Batch 351 Loss 1.2860\n",
      "Epoch 2 Batch 352 Loss 1.7424\n",
      "Epoch 2 Batch 353 Loss 1.5740\n",
      "Epoch 2 Batch 354 Loss 1.6463\n",
      "Epoch 2 Batch 355 Loss 1.8823\n",
      "Epoch 2 Batch 356 Loss 1.5944\n",
      "Epoch 2 Batch 357 Loss 1.2958\n",
      "Epoch 2 Batch 358 Loss 2.4113\n",
      "Epoch 2 Batch 359 Loss 1.7390\n",
      "Epoch 2 Batch 360 Loss 2.0305\n",
      "Epoch 2 Batch 361 Loss 1.6338\n",
      "Epoch 2 Batch 362 Loss 1.6094\n",
      "Epoch 2 Batch 363 Loss 1.7738\n",
      "Epoch 2 Batch 364 Loss 1.7359\n",
      "Epoch 2 Batch 365 Loss 2.0254\n",
      "Epoch 2 Batch 366 Loss 2.4549\n",
      "Epoch 2 Batch 367 Loss 1.5730\n",
      "Epoch 2 Batch 368 Loss 1.5041\n",
      "Epoch 2 Batch 369 Loss 2.1548\n",
      "Epoch 2 Batch 370 Loss 2.2873\n",
      "Epoch 2 Batch 371 Loss 1.6232\n",
      "Epoch 2 Batch 372 Loss 1.0754\n",
      "Epoch 2 Batch 373 Loss 1.7754\n",
      "Epoch 2 Batch 374 Loss 1.7632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Batch 375 Loss 1.4967\n",
      "Epoch 2 Batch 376 Loss 1.3759\n",
      "Epoch 2 Batch 377 Loss 1.4877\n",
      "Epoch 2 Batch 378 Loss 1.8386\n",
      "Epoch 2 Batch 379 Loss 1.2301\n",
      "Epoch 2 Batch 380 Loss 1.3078\n",
      "Epoch 2 Batch 381 Loss 1.2047\n",
      "Epoch 2 Batch 382 Loss 1.4628\n",
      "Epoch 2 Batch 383 Loss 1.9474\n",
      "Epoch 2 Batch 384 Loss 1.8416\n",
      "Epoch 2 Batch 385 Loss 1.3272\n",
      "Epoch 2 Batch 386 Loss 1.4036\n",
      "Epoch 2 Batch 387 Loss 1.7270\n",
      "Epoch 2 Batch 388 Loss 2.1856\n",
      "Epoch 2 Batch 389 Loss 1.9213\n",
      "Epoch 2 Batch 390 Loss 1.8738\n",
      "Epoch 2 Batch 391 Loss 1.5980\n",
      "Epoch 2 Batch 392 Loss 1.6689\n",
      "Epoch 2 Batch 393 Loss 1.7502\n",
      "Epoch 2 Batch 394 Loss 1.5422\n",
      "Epoch 2 Batch 395 Loss 1.2696\n",
      "Epoch 2 Batch 396 Loss 1.4181\n",
      "Epoch 2 Batch 397 Loss 1.5015\n",
      "Epoch 2 Batch 398 Loss 1.7623\n",
      "Epoch 2 Batch 399 Loss 1.5451\n",
      "Epoch 2 Batch 400 Loss 1.6928\n",
      "Epoch 2 Batch 401 Loss 1.7104\n",
      "Epoch 2 Batch 402 Loss 1.0621\n",
      "Epoch 2 Batch 403 Loss 1.5584\n",
      "Epoch 2 Batch 404 Loss 1.6678\n",
      "Epoch 2 Batch 405 Loss 1.5503\n",
      "Epoch 2 Batch 406 Loss 1.5830\n",
      "Epoch 2 Batch 407 Loss 1.8535\n",
      "Epoch 2 Batch 408 Loss 2.4730\n",
      "Epoch 2 Batch 409 Loss 1.6903\n",
      "Epoch 2 Batch 410 Loss 1.8595\n",
      "Epoch 2 Batch 411 Loss 1.8924\n",
      "Epoch 2 Batch 412 Loss 1.6892\n",
      "Epoch 2 Batch 413 Loss 1.9915\n",
      "Epoch 2 Batch 414 Loss 2.0528\n",
      "Epoch 2 Batch 415 Loss 2.0476\n",
      "Epoch 2 Batch 416 Loss 2.0049\n",
      "Epoch 2 Batch 417 Loss 1.7443\n",
      "Epoch 2 Batch 418 Loss 1.5731\n",
      "Epoch 2 Batch 419 Loss 1.7411\n",
      "Epoch 2 Batch 420 Loss 1.5547\n",
      "Epoch 2 Batch 421 Loss 1.8946\n",
      "Epoch 2 Batch 422 Loss 1.5472\n",
      "Epoch 2 Batch 423 Loss 2.0235\n",
      "Epoch 2 Batch 424 Loss 1.4112\n",
      "Epoch 2 Batch 425 Loss 1.9655\n",
      "Epoch 2 Batch 426 Loss 2.2890\n",
      "Epoch 2 Batch 427 Loss 1.8670\n",
      "Epoch 2 Batch 428 Loss 2.4722\n",
      "Epoch 2 Batch 429 Loss 2.1205\n",
      "Epoch 2 Batch 430 Loss 1.8298\n",
      "Epoch 2 Batch 431 Loss 1.8662\n",
      "Epoch 2 Batch 432 Loss 1.9620\n",
      "Epoch 2 Batch 433 Loss 1.3382\n",
      "Epoch 2 Batch 434 Loss 1.5945\n",
      "Epoch 2 Batch 435 Loss 1.3300\n",
      "Epoch 2 Batch 436 Loss 2.1378\n",
      "Epoch 2 Batch 437 Loss 1.8326\n",
      "Epoch 2 Batch 438 Loss 1.8714\n",
      "Epoch 2 Batch 439 Loss 1.8133\n",
      "Epoch 2 Batch 440 Loss 1.9048\n",
      "Epoch 2 Batch 441 Loss 2.1347\n",
      "Epoch 2 Batch 442 Loss 1.8700\n",
      "Epoch 2 Batch 443 Loss 1.7165\n",
      "Epoch 2 Batch 444 Loss 1.5935\n",
      "Epoch 2 Batch 445 Loss 1.3680\n",
      "Epoch 2 Batch 446 Loss 2.1171\n",
      "Epoch 2 Batch 447 Loss 1.8392\n",
      "Epoch 2 Batch 448 Loss 2.4252\n",
      "Epoch 2 Batch 449 Loss 1.6376\n",
      "Epoch 2 Batch 450 Loss 2.3103\n",
      "Epoch 2 Batch 451 Loss 2.1367\n",
      "Epoch 2 Batch 452 Loss 2.1559\n",
      "Epoch 2 Batch 453 Loss 1.9941\n",
      "Epoch 2 Batch 454 Loss 2.1239\n",
      "Epoch 2 Batch 455 Loss 1.7125\n",
      "Epoch 2 Batch 456 Loss 1.4847\n",
      "Epoch 2 Batch 457 Loss 1.5459\n",
      "Epoch 2 Batch 458 Loss 1.1464\n",
      "Epoch 2 Batch 459 Loss 1.3385\n",
      "Epoch 2 Batch 460 Loss 1.4034\n",
      "Epoch 2 Batch 461 Loss 1.7565\n",
      "Epoch 2 Batch 462 Loss 1.7675\n",
      "Epoch 2 Batch 463 Loss 2.1041\n",
      "Epoch 2 Batch 464 Loss 1.6231\n",
      "Epoch 2 Batch 465 Loss 1.8745\n",
      "Epoch 2 Batch 466 Loss 2.1250\n",
      "Epoch 2 Batch 467 Loss 2.0315\n",
      "Epoch 2 Batch 468 Loss 1.9984\n",
      "Epoch 2 Batch 469 Loss 1.8132\n",
      "Epoch 2 Batch 470 Loss 1.8748\n",
      "Epoch 2 Batch 471 Loss 2.2709\n",
      "Epoch 2 Batch 472 Loss 1.6948\n",
      "Epoch 2 Batch 473 Loss 1.4676\n",
      "Epoch 2 Batch 474 Loss 1.5929\n",
      "Epoch 2 Batch 475 Loss 1.6836\n",
      "Epoch 2 Batch 476 Loss 1.3131\n",
      "Epoch 2 Batch 477 Loss 1.5809\n",
      "Epoch 2 Batch 478 Loss 1.2131\n",
      "Epoch 2 Batch 479 Loss 1.2474\n",
      "Epoch 2 Batch 480 Loss 1.7096\n",
      "Epoch 2 Batch 481 Loss 1.8848\n",
      "Epoch 2 Batch 482 Loss 1.5313\n",
      "Epoch 2 Batch 483 Loss 1.9730\n",
      "Epoch 2 Batch 484 Loss 2.2830\n",
      "Epoch 2 Batch 485 Loss 1.9638\n",
      "Epoch 2 Batch 486 Loss 1.7314\n",
      "Epoch 2 Batch 487 Loss 1.4320\n",
      "Epoch 2 Batch 488 Loss 1.5427\n",
      "Epoch 2 Batch 489 Loss 1.3621\n",
      "Epoch 2 Batch 490 Loss 1.5894\n",
      "Epoch 2 Batch 491 Loss 1.6630\n",
      "Epoch 2 Batch 492 Loss 1.8537\n",
      "Epoch 2 Batch 493 Loss 1.9136\n",
      "Epoch 2 Batch 494 Loss 1.7876\n",
      "Epoch 2 Batch 495 Loss 1.6390\n",
      "Epoch 2 Batch 496 Loss 1.5241\n",
      "Epoch 2 Batch 497 Loss 1.2103\n",
      "Epoch 2 Batch 498 Loss 1.3507\n",
      "Epoch 2 Batch 499 Loss 1.7527\n",
      "Epoch 2 Batch 500 Loss 1.4086\n",
      "Epoch 2 Batch 501 Loss 1.2763\n",
      "Epoch 2 Batch 502 Loss 1.2816\n",
      "Epoch 2 Batch 503 Loss 0.9411\n",
      "Epoch 2 Batch 504 Loss 1.2891\n",
      "Epoch 2 Batch 505 Loss 1.7355\n",
      "Epoch 2 Batch 506 Loss 1.4678\n",
      "Epoch 2 Batch 507 Loss 2.0489\n",
      "Epoch 2 Batch 508 Loss 1.7841\n",
      "Epoch 2 Batch 509 Loss 1.8341\n",
      "Epoch 2 Batch 510 Loss 1.9003\n",
      "Epoch 2 Batch 511 Loss 1.7922\n",
      "Epoch 2 Batch 512 Loss 1.4975\n",
      "Epoch 2 Batch 513 Loss 1.7797\n",
      "Epoch 2 Batch 514 Loss 1.6222\n",
      "Epoch 2 Batch 515 Loss 1.9122\n",
      "Epoch 2 Batch 516 Loss 1.6040\n",
      "Epoch 2 Batch 517 Loss 1.8715\n",
      "Epoch 2 Batch 518 Loss 1.7702\n",
      "Epoch 2 Batch 519 Loss 1.4818\n",
      "Epoch 2 Batch 520 Loss 1.1795\n",
      "Epoch 2 Batch 521 Loss 1.3617\n",
      "Epoch 2 Batch 522 Loss 1.4086\n",
      "Epoch 2 Batch 523 Loss 1.6788\n",
      "Epoch 2 Batch 524 Loss 1.6136\n",
      "Epoch 2 Batch 525 Loss 1.2426\n",
      "Epoch 2 Batch 526 Loss 1.4058\n",
      "Epoch 2 Batch 527 Loss 1.4749\n",
      "Epoch 2 Batch 528 Loss 1.7071\n",
      "Epoch 2 Batch 529 Loss 1.6562\n",
      "Epoch 2 Batch 530 Loss 1.7138\n",
      "Epoch 2 Batch 531 Loss 1.6632\n",
      "Epoch 2 Batch 532 Loss 2.0671\n",
      "Epoch 2 Batch 533 Loss 2.1387\n",
      "Epoch 2 Batch 534 Loss 1.9721\n",
      "Epoch 2 Batch 535 Loss 2.0044\n",
      "Epoch 2 Batch 536 Loss 1.5439\n",
      "Epoch 2 Batch 537 Loss 1.8492\n",
      "Epoch 2 Batch 538 Loss 1.5890\n",
      "Epoch 2 Batch 539 Loss 1.6430\n",
      "Epoch 2 Batch 540 Loss 1.4214\n",
      "Epoch 2 Batch 541 Loss 1.4889\n",
      "Epoch 2 Batch 542 Loss 1.5101\n",
      "Epoch 2 Batch 543 Loss 1.5617\n",
      "Epoch 2 Batch 544 Loss 1.7377\n",
      "Epoch 2 Batch 545 Loss 2.2332\n",
      "Epoch 2 Batch 546 Loss 1.7231\n",
      "Epoch 2 Batch 547 Loss 2.0986\n",
      "Epoch 2 Batch 548 Loss 2.1254\n",
      "Epoch 2 Batch 549 Loss 2.5105\n",
      "Epoch 2 Batch 550 Loss 1.4954\n",
      "Epoch 2 Batch 551 Loss 1.8201\n",
      "Epoch 2 Batch 552 Loss 1.4980\n",
      "Epoch 2 Batch 553 Loss 1.6962\n",
      "Epoch 2 Batch 554 Loss 1.4193\n",
      "Epoch 2 Batch 555 Loss 1.7305\n",
      "Epoch 2 Batch 556 Loss 2.3461\n",
      "Epoch 2 Batch 557 Loss 1.3256\n",
      "Epoch 2 Batch 558 Loss 1.6116\n",
      "Epoch 2 Batch 559 Loss 1.8709\n",
      "Epoch 2 Batch 560 Loss 1.0011\n",
      "Epoch 2 Batch 561 Loss 2.1282\n",
      "Epoch 2 Batch 562 Loss 2.0240\n",
      "Epoch 2 Batch 563 Loss 1.7607\n",
      "Epoch 2 Batch 564 Loss 1.6455\n",
      "Epoch 2 Batch 565 Loss 1.5564\n",
      "Epoch 2 Batch 566 Loss 1.5014\n",
      "Epoch 2 Batch 567 Loss 1.6683\n",
      "Epoch 2 Batch 568 Loss 1.9035\n",
      "Epoch 2 Batch 569 Loss 1.6446\n",
      "Epoch 2 Batch 570 Loss 1.3120\n",
      "Epoch 2 Batch 571 Loss 1.4231\n",
      "Epoch 2 Batch 572 Loss 1.2505\n",
      "Epoch 2 Batch 573 Loss 1.3955\n",
      "Epoch 2 Batch 574 Loss 1.8283\n",
      "Epoch 2 Batch 575 Loss 2.0111\n",
      "Epoch 2 Batch 576 Loss 2.1694\n",
      "Epoch 2 Batch 577 Loss 1.6418\n",
      "Epoch 2 Batch 578 Loss 2.1137\n",
      "Epoch 2 Batch 579 Loss 1.6349\n",
      "Epoch 2 Batch 580 Loss 1.8124\n",
      "Epoch 2 Batch 581 Loss 1.5524\n",
      "Epoch 2 Batch 582 Loss 1.9657\n",
      "Epoch 2 Batch 583 Loss 1.4321\n",
      "Epoch 2 Batch 584 Loss 1.7511\n",
      "Epoch 2 Batch 585 Loss 1.4418\n",
      "Epoch 2 Batch 586 Loss 1.9705\n",
      "Epoch 2 Batch 587 Loss 1.6582\n",
      "Epoch 2 Batch 588 Loss 1.3635\n",
      "Epoch 2 Batch 589 Loss 1.5827\n",
      "Epoch 2 Batch 590 Loss 1.7778\n",
      "Epoch 2 Batch 591 Loss 2.0140\n",
      "Epoch 2 Batch 592 Loss 1.8240\n",
      "Epoch 2 Batch 593 Loss 2.0476\n",
      "Epoch 2 Batch 594 Loss 2.1787\n",
      "Epoch 2 Batch 595 Loss 1.5611\n",
      "Epoch 2 Batch 596 Loss 1.7906\n",
      "Epoch 2 Batch 597 Loss 1.8002\n",
      "Epoch 2 Batch 598 Loss 1.7673\n",
      "Epoch 2 Batch 599 Loss 1.7735\n",
      "Epoch 2 Batch 600 Loss 1.7648\n",
      "Epoch 2 Batch 601 Loss 1.4918\n",
      "Epoch 2 Batch 602 Loss 1.9461\n",
      "Epoch 2 Batch 603 Loss 1.9040\n",
      "Epoch 2 Batch 604 Loss 1.8907\n",
      "Epoch 2 Batch 605 Loss 1.8661\n",
      "Epoch 2 Batch 606 Loss 2.0276\n",
      "Epoch 2 Batch 607 Loss 1.8282\n",
      "Epoch 2 Batch 608 Loss 1.8638\n",
      "Epoch 2 Batch 609 Loss 1.7994\n",
      "Epoch 2 Batch 610 Loss 1.8055\n",
      "Epoch 2 Batch 611 Loss 1.4869\n",
      "Epoch 2 Batch 612 Loss 1.8798\n",
      "Epoch 2 Batch 613 Loss 1.8994\n",
      "Epoch 2 Batch 614 Loss 1.4846\n",
      "Epoch 2 Batch 615 Loss 2.0049\n",
      "Epoch 2 Batch 616 Loss 1.5232\n",
      "Epoch 2 Batch 617 Loss 2.2146\n",
      "Epoch 2 Batch 618 Loss 1.9691\n",
      "Epoch 2 Batch 619 Loss 1.8346\n",
      "Epoch 2 Batch 620 Loss 1.2708\n",
      "Epoch 2 Batch 621 Loss 1.1831\n",
      "Epoch 2 Batch 622 Loss 1.7128\n",
      "Epoch 2 Batch 623 Loss 1.5453\n",
      "Epoch 2 Batch 624 Loss 1.5386\n",
      "Epoch 2 Batch 625 Loss 1.3176\n",
      "Epoch 2 Batch 626 Loss 1.6217\n",
      "Epoch 2 Batch 627 Loss 1.3746\n",
      "Epoch 2 Batch 628 Loss 1.8037\n",
      "Epoch 2 Batch 629 Loss 1.8913\n",
      "Epoch 2 Batch 630 Loss 2.1624\n",
      "Epoch 2 Batch 631 Loss 2.0710\n",
      "Epoch 2 Batch 632 Loss 1.9941\n",
      "Epoch 2 Batch 633 Loss 1.8555\n",
      "Epoch 2 Batch 634 Loss 1.7483\n",
      "Epoch 2 Batch 635 Loss 1.8808\n",
      "Epoch 2 Batch 636 Loss 1.9841\n",
      "Epoch 2 Batch 637 Loss 2.0224\n",
      "Epoch 2 Batch 638 Loss 1.2976\n",
      "Epoch 2 Batch 639 Loss 1.7891\n",
      "Epoch 2 Batch 640 Loss 1.8129\n",
      "Epoch 2 Batch 641 Loss 1.4397\n",
      "Epoch 2 Batch 642 Loss 1.6898\n",
      "Epoch 2 Batch 643 Loss 1.6030\n",
      "Epoch 2 Batch 644 Loss 1.4659\n",
      "Epoch 2 Batch 645 Loss 1.4725\n",
      "Epoch 2 Batch 646 Loss 1.7897\n",
      "Epoch 2 Batch 647 Loss 1.8677\n",
      "Epoch 2 Batch 648 Loss 1.9656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Batch 649 Loss 1.6462\n",
      "Epoch 2 Batch 650 Loss 1.8010\n",
      "Epoch 2 Batch 651 Loss 1.8374\n",
      "Epoch 2 Batch 652 Loss 1.7003\n",
      "Epoch 2 Batch 653 Loss 1.8608\n",
      "Epoch 2 Batch 654 Loss 1.7215\n",
      "Epoch 2 Batch 655 Loss 2.1861\n",
      "Epoch 2 Batch 656 Loss 2.0291\n",
      "Epoch 2 Batch 657 Loss 2.3905\n",
      "Epoch 2 Batch 658 Loss 1.4835\n",
      "Epoch 2 Batch 659 Loss 1.8437\n",
      "Epoch 2 Batch 660 Loss 1.7153\n",
      "Epoch 2 Batch 661 Loss 2.0460\n",
      "Epoch 2 Batch 662 Loss 2.0262\n",
      "Epoch 2 Batch 663 Loss 1.8302\n",
      "Epoch 2 Batch 664 Loss 1.7402\n",
      "Epoch 2 Batch 665 Loss 2.0708\n",
      "Epoch 2 Batch 666 Loss 1.5149\n",
      "Epoch 2 Batch 667 Loss 1.9224\n",
      "Epoch 2 Batch 668 Loss 1.9417\n",
      "Epoch 2 Batch 669 Loss 1.8296\n",
      "Epoch 2 Batch 670 Loss 1.9407\n",
      "Epoch 2 Batch 671 Loss 1.3248\n",
      "Epoch 2 Batch 672 Loss 1.6455\n",
      "Epoch 2 Batch 673 Loss 1.7130\n",
      "Epoch 2 Batch 674 Loss 1.8253\n",
      "Epoch 2 Batch 675 Loss 1.5493\n",
      "Epoch 2 Batch 676 Loss 1.7108\n",
      "Epoch 2 Batch 677 Loss 1.6497\n",
      "Epoch 2 Batch 678 Loss 1.8536\n",
      "Epoch 2 Batch 679 Loss 2.0250\n",
      "Epoch 2 Batch 680 Loss 1.6541\n",
      "Epoch 2 Batch 681 Loss 1.7836\n",
      "Epoch 2 Batch 682 Loss 1.7462\n",
      "Epoch 2 Batch 683 Loss 1.6895\n",
      "Epoch 2 Batch 684 Loss 1.8614\n",
      "Epoch 2 Batch 685 Loss 1.8456\n",
      "Epoch 2 Batch 686 Loss 1.5961\n",
      "Epoch 2 Batch 687 Loss 1.5027\n",
      "Epoch 2 Batch 688 Loss 1.6966\n",
      "Epoch 2 Batch 689 Loss 2.0022\n",
      "Epoch 2 Batch 690 Loss 2.0241\n",
      "Epoch 2 Batch 691 Loss 1.8950\n",
      "Epoch 2 Batch 692 Loss 2.1839\n",
      "Epoch 2 Batch 693 Loss 2.0732\n",
      "Epoch 2 Batch 694 Loss 2.0846\n",
      "Epoch 2 Batch 695 Loss 2.0126\n",
      "Epoch 2 Batch 696 Loss 1.6911\n",
      "Epoch 2 Batch 697 Loss 1.5442\n",
      "Epoch 2 Batch 698 Loss 1.6863\n",
      "Epoch 2 Batch 699 Loss 1.6933\n",
      "Epoch 2 Batch 700 Loss 1.6888\n",
      "Epoch 2 Batch 701 Loss 1.1162\n",
      "Epoch 2 Batch 702 Loss 1.3075\n",
      "Epoch 2 Batch 703 Loss 1.6876\n",
      "Epoch 2 Batch 704 Loss 1.8189\n",
      "Epoch 2 Batch 705 Loss 1.4031\n",
      "Epoch 2 Batch 706 Loss 1.8898\n",
      "Epoch 2 Batch 707 Loss 2.0073\n",
      "Epoch 2 Batch 708 Loss 2.3353\n",
      "Epoch 2 Batch 709 Loss 1.9110\n",
      "Epoch 2 Batch 710 Loss 1.8580\n",
      "Epoch 2 Batch 711 Loss 1.9097\n",
      "Epoch 2 Batch 712 Loss 1.4217\n",
      "Epoch 2 Batch 713 Loss 1.2554\n",
      "Epoch 2 Batch 714 Loss 1.6879\n",
      "Epoch 2 Batch 715 Loss 1.5301\n",
      "Epoch 2 Batch 716 Loss 2.1319\n",
      "Epoch 2 Batch 717 Loss 1.5420\n",
      "Epoch 2 Batch 718 Loss 1.7827\n",
      "Epoch 2 Batch 719 Loss 1.9606\n",
      "Epoch 2 Batch 720 Loss 1.5062\n",
      "Epoch 2 Batch 721 Loss 2.3455\n",
      "Epoch 2 Batch 722 Loss 1.9707\n",
      "Epoch 2 Batch 723 Loss 1.7939\n",
      "Epoch 2 Batch 724 Loss 2.2509\n",
      "Epoch 2 Batch 725 Loss 2.2728\n",
      "Epoch 2 Batch 726 Loss 1.7956\n",
      "Epoch 2 Batch 727 Loss 1.5268\n",
      "Epoch 2 Batch 728 Loss 1.1739\n",
      "Epoch 2 Batch 729 Loss 1.5454\n",
      "Epoch 2 Batch 730 Loss 1.5776\n",
      "Epoch 2 Batch 731 Loss 1.4765\n",
      "Epoch 2 Batch 732 Loss 1.3060\n",
      "Epoch 2 Batch 733 Loss 1.4410\n",
      "Epoch 2 Batch 734 Loss 0.9643\n",
      "Epoch 2 Batch 735 Loss 1.2315\n",
      "Epoch 2 Batch 736 Loss 1.4440\n",
      "Epoch 2 Batch 737 Loss 1.1483\n",
      "Epoch 2 Batch 738 Loss 1.0135\n",
      "Epoch 2 Batch 739 Loss 1.4483\n",
      "Epoch 2 Batch 740 Loss 1.5145\n",
      "Epoch 2 Batch 741 Loss 1.7656\n",
      "Epoch 2 Batch 742 Loss 1.9675\n",
      "Epoch 2 Batch 743 Loss 1.8370\n",
      "Epoch 2 Batch 744 Loss 1.3778\n",
      "Epoch 2 Batch 745 Loss 1.7832\n",
      "Epoch 2 Batch 746 Loss 1.9181\n",
      "Epoch 2 Batch 747 Loss 1.5904\n",
      "Epoch 2 Batch 748 Loss 1.6031\n",
      "Epoch 2 Batch 749 Loss 1.7661\n",
      "Epoch 2 Batch 750 Loss 1.5381\n",
      "Epoch 2 Batch 751 Loss 1.8844\n",
      "Epoch 2 Batch 752 Loss 1.4889\n",
      "Epoch 2 Batch 753 Loss 1.8179\n",
      "Epoch 2 Batch 754 Loss 1.5777\n",
      "Epoch 2 Batch 755 Loss 1.2392\n",
      "Epoch 2 Batch 756 Loss 1.7236\n",
      "Epoch 2 Batch 757 Loss 1.4328\n",
      "Epoch 2 Batch 758 Loss 1.6947\n",
      "Epoch 2 Batch 759 Loss 1.7169\n",
      "Epoch 2 Batch 760 Loss 2.0098\n",
      "Epoch 2 Batch 761 Loss 1.9615\n",
      "Epoch 2 Batch 762 Loss 2.2119\n",
      "Epoch 2 Batch 763 Loss 1.9343\n",
      "Epoch 2 Batch 764 Loss 2.0319\n",
      "Epoch 2 Batch 765 Loss 1.7798\n",
      "Epoch 2 Batch 766 Loss 1.8569\n",
      "Epoch 2 Batch 767 Loss 2.0087\n",
      "Epoch 2 Batch 768 Loss 2.3888\n",
      "Epoch 2 Batch 769 Loss 2.0657\n",
      "Epoch 2 Batch 770 Loss 2.0245\n",
      "Epoch 2 Batch 771 Loss 1.8273\n",
      "Epoch 2 Batch 772 Loss 1.8661\n",
      "Epoch 2 Batch 773 Loss 2.0352\n",
      "Epoch 2 Batch 774 Loss 1.8595\n",
      "Epoch 2 Batch 775 Loss 2.4132\n",
      "Epoch 2 Batch 776 Loss 2.0104\n",
      "Epoch 2 Batch 777 Loss 1.6982\n",
      "Epoch 2 Batch 778 Loss 1.7818\n",
      "Epoch 2 Batch 779 Loss 1.8894\n",
      "Epoch 2 Batch 780 Loss 1.5289\n",
      "Epoch 2 Batch 781 Loss 1.5762\n",
      "Epoch 2 Batch 782 Loss 1.7342\n",
      "Epoch 2 Batch 783 Loss 1.6837\n",
      "Epoch 2 Batch 784 Loss 1.5136\n",
      "Epoch 2 Batch 785 Loss 1.9028\n",
      "Epoch 2 Batch 786 Loss 2.0491\n",
      "Epoch 2 Batch 787 Loss 1.5006\n",
      "Epoch 2 Batch 788 Loss 2.0002\n",
      "Epoch 2 Batch 789 Loss 1.8117\n",
      "Epoch 2 Batch 790 Loss 1.4205\n",
      "Epoch 2 Batch 791 Loss 1.8244\n",
      "Epoch 2 Batch 792 Loss 1.5807\n",
      "Epoch 2 Batch 793 Loss 1.3373\n",
      "Epoch 2 Batch 794 Loss 1.1832\n",
      "Epoch 2 Batch 795 Loss 1.3637\n",
      "Epoch 2 Batch 796 Loss 1.4397\n",
      "Epoch 2 Batch 797 Loss 1.2581\n",
      "Epoch 2 Batch 798 Loss 1.4759\n",
      "Epoch 2 Batch 799 Loss 1.4073\n",
      "Epoch 2 Batch 800 Loss 1.6026\n",
      "Epoch 2 Batch 801 Loss 1.7764\n",
      "Epoch 2 Batch 802 Loss 1.6680\n",
      "Epoch 2 Batch 803 Loss 1.3518\n",
      "Epoch 2 Batch 804 Loss 1.3027\n",
      "Epoch 2 Batch 805 Loss 1.4646\n",
      "Epoch 2 Batch 806 Loss 1.2870\n",
      "Epoch 2 Batch 807 Loss 2.2605\n",
      "Epoch 2 Batch 808 Loss 1.9846\n",
      "Epoch 2 Batch 809 Loss 2.3947\n",
      "Epoch 2 Batch 810 Loss 1.7250\n",
      "Epoch 2 Batch 811 Loss 1.7031\n",
      "Epoch 2 Batch 812 Loss 1.9297\n",
      "Epoch 2 Batch 813 Loss 1.7945\n",
      "Epoch 2 Batch 814 Loss 1.6621\n",
      "Epoch 2 Batch 815 Loss 1.6393\n",
      "Epoch 2 Batch 816 Loss 1.9365\n",
      "Epoch 2 Batch 817 Loss 1.8832\n",
      "Epoch 2 Batch 818 Loss 2.4662\n",
      "Epoch 2 Batch 819 Loss 2.2405\n",
      "Epoch 2 Batch 820 Loss 2.5645\n",
      "Epoch 2 Batch 821 Loss 1.7227\n",
      "Epoch 2 Batch 822 Loss 1.5048\n",
      "Epoch 2 Batch 823 Loss 1.7060\n",
      "Epoch 2 Batch 824 Loss 1.2032\n",
      "Epoch 2 Batch 825 Loss 1.7169\n",
      "Epoch 2 Batch 826 Loss 1.6120\n",
      "Epoch 2 Batch 827 Loss 2.0286\n",
      "Epoch 2 Batch 828 Loss 1.9104\n",
      "Epoch 2 Batch 829 Loss 2.1071\n",
      "Epoch 2 Batch 830 Loss 2.2312\n",
      "Epoch 2 Batch 831 Loss 1.9040\n",
      "Epoch 2 Batch 832 Loss 1.9729\n",
      "Epoch 2 Batch 833 Loss 1.4940\n",
      "Epoch 2 Batch 834 Loss 1.3975\n",
      "Epoch 2 Batch 835 Loss 1.6703\n",
      "Epoch 2 Batch 836 Loss 1.5710\n",
      "Epoch 2 Batch 837 Loss 1.6921\n",
      "Epoch 2 Batch 838 Loss 1.5153\n",
      "Epoch 2 Batch 839 Loss 1.6995\n",
      "Epoch 2 Batch 840 Loss 1.8039\n",
      "Epoch 2 Batch 841 Loss 1.6383\n",
      "Epoch 2 Batch 842 Loss 1.8251\n",
      "Epoch 2 Batch 843 Loss 1.8076\n",
      "Epoch 2 Batch 844 Loss 1.6223\n",
      "Epoch 2 Batch 845 Loss 1.6849\n",
      "Epoch 2 Batch 846 Loss 1.5228\n",
      "Epoch 2 Batch 847 Loss 1.6693\n",
      "Epoch 2 Batch 848 Loss 1.7538\n",
      "Epoch 2 Batch 849 Loss 1.7263\n",
      "Epoch 2 Batch 850 Loss 1.8103\n",
      "Epoch 2 Batch 851 Loss 1.8070\n",
      "Epoch 2 Batch 852 Loss 2.1990\n",
      "Epoch 2 Batch 853 Loss 2.0458\n",
      "Epoch 2 Batch 854 Loss 1.4723\n",
      "Epoch 2 Batch 855 Loss 1.2311\n",
      "Epoch 2 Batch 856 Loss 1.2997\n",
      "Epoch 2 Batch 857 Loss 1.6415\n",
      "Epoch 2 Batch 858 Loss 1.5697\n",
      "Epoch 2 Batch 859 Loss 1.7378\n",
      "Epoch 2 Batch 860 Loss 1.8245\n",
      "Epoch 2 Batch 861 Loss 1.9420\n",
      "Epoch 2 Batch 862 Loss 1.6195\n",
      "Epoch 2 Batch 863 Loss 2.3245\n",
      "Epoch 2 Batch 864 Loss 1.6290\n",
      "Epoch 2 Batch 865 Loss 1.7210\n",
      "Epoch 2 Batch 866 Loss 1.7209\n",
      "Epoch 2 Batch 867 Loss 1.8806\n",
      "Epoch 2 Batch 868 Loss 1.6145\n",
      "Epoch 2 Batch 869 Loss 2.0350\n",
      "Epoch 2 Batch 870 Loss 1.4902\n",
      "Epoch 2 Batch 871 Loss 1.1813\n",
      "Epoch 2 Batch 872 Loss 1.7632\n",
      "Epoch 2 Batch 873 Loss 1.3952\n",
      "Epoch 2 Batch 874 Loss 1.4793\n",
      "Epoch 2 Batch 875 Loss 1.3486\n",
      "Epoch 2 Batch 876 Loss 1.2377\n",
      "Epoch 2 Batch 877 Loss 1.9212\n",
      "Epoch 2 Batch 878 Loss 1.6459\n",
      "Epoch 2 Batch 879 Loss 1.6351\n",
      "Epoch 2 Batch 880 Loss 1.6461\n",
      "Epoch 2 Batch 881 Loss 1.3691\n",
      "Epoch 2 Batch 882 Loss 1.6931\n",
      "Epoch 2 Batch 883 Loss 1.3117\n",
      "Epoch 2 Batch 884 Loss 1.3404\n",
      "Epoch 2 Batch 885 Loss 2.2359\n",
      "Epoch 2 Batch 886 Loss 1.7587\n",
      "Epoch 2 Batch 887 Loss 1.7879\n",
      "Epoch 2 Batch 888 Loss 1.2936\n",
      "Epoch 2 Batch 889 Loss 1.3363\n",
      "Epoch 2 Batch 890 Loss 1.5213\n",
      "Epoch 2 Batch 891 Loss 1.7906\n",
      "Epoch 2 Batch 892 Loss 1.6108\n",
      "Epoch 2 Batch 893 Loss 1.5602\n",
      "Epoch 2 Batch 894 Loss 1.5091\n",
      "Epoch 2 Batch 895 Loss 1.6702\n",
      "Epoch 2 Batch 896 Loss 1.6404\n",
      "Epoch 2 Batch 897 Loss 1.7425\n",
      "Epoch 2 Batch 898 Loss 1.4028\n",
      "Epoch 2 Batch 899 Loss 1.5701\n",
      "Epoch 2 Batch 900 Loss 1.6452\n",
      "Epoch 2 Batch 901 Loss 1.6133\n",
      "Epoch 2 Batch 902 Loss 1.5954\n",
      "Epoch 2 Batch 903 Loss 1.8054\n",
      "Epoch 2 Batch 904 Loss 1.9753\n",
      "Epoch 2 Batch 905 Loss 1.7609\n",
      "Epoch 2 Batch 906 Loss 2.2052\n",
      "Epoch 2 Batch 907 Loss 1.9442\n",
      "Epoch 2 Batch 908 Loss 2.0058\n",
      "Epoch 2 Batch 909 Loss 1.6902\n",
      "Epoch 2 Batch 910 Loss 2.0678\n",
      "Epoch 2 Batch 911 Loss 1.9497\n",
      "Epoch 2 Batch 912 Loss 1.8377\n",
      "Epoch 2 Batch 913 Loss 2.0133\n",
      "Epoch 2 Batch 914 Loss 1.7834\n",
      "Epoch 2 Batch 915 Loss 1.7582\n",
      "Epoch 2 Batch 916 Loss 1.8422\n",
      "Epoch 2 Batch 917 Loss 1.6723\n",
      "Epoch 2 Batch 918 Loss 1.9675\n",
      "Epoch 2 Batch 919 Loss 2.0565\n",
      "Epoch 2 Batch 920 Loss 1.6704\n",
      "Epoch 2 Batch 921 Loss 2.1863\n",
      "Epoch 2 Batch 922 Loss 1.6725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Batch 923 Loss 1.6673\n",
      "Epoch 2 Batch 924 Loss 1.8398\n",
      "Epoch 2 Batch 925 Loss 1.9625\n",
      "Epoch 2 Batch 926 Loss 1.9772\n",
      "Epoch 2 Batch 927 Loss 2.1283\n",
      "Epoch 2 Batch 928 Loss 1.9353\n",
      "Epoch 2 Batch 929 Loss 2.1900\n",
      "Epoch 2 Batch 930 Loss 2.2311\n",
      "Epoch 2 Batch 931 Loss 1.6178\n",
      "Epoch 2 Batch 932 Loss 1.7143\n",
      "Epoch 2 Batch 933 Loss 2.3842\n",
      "Epoch 2 Batch 934 Loss 2.4419\n",
      "Epoch 2 Batch 935 Loss 2.3827\n",
      "Epoch 2 Batch 936 Loss 2.1342\n",
      "Epoch 2 Batch 937 Loss 2.1670\n",
      "Epoch 2 Batch 938 Loss 1.8764\n",
      "Epoch 2 Batch 939 Loss 1.8451\n",
      "Epoch 2 Batch 940 Loss 1.5721\n",
      "Epoch 2 Batch 941 Loss 1.5781\n",
      "Epoch 2 Batch 942 Loss 1.2782\n",
      "Epoch 2 Batch 943 Loss 1.3947\n",
      "Epoch 2 Batch 944 Loss 1.6065\n",
      "Epoch 2 Batch 945 Loss 1.8218\n",
      "Epoch 2 Batch 946 Loss 2.0843\n",
      "Epoch 2 Batch 947 Loss 1.9547\n",
      "Epoch 2 Batch 948 Loss 1.4983\n",
      "Epoch 2 Batch 949 Loss 1.9014\n",
      "Epoch 2 Batch 950 Loss 1.8174\n",
      "Epoch 2 Batch 951 Loss 1.8430\n",
      "Epoch 2 Batch 952 Loss 1.7935\n",
      "Epoch 2 Batch 953 Loss 1.8848\n",
      "Epoch 2 Batch 954 Loss 1.6982\n",
      "Epoch 2 Batch 955 Loss 1.7125\n",
      "Epoch 2 Batch 956 Loss 1.9074\n",
      "Epoch 2 Batch 957 Loss 2.0899\n",
      "Epoch 2 Batch 958 Loss 1.7172\n",
      "Epoch 2 Batch 959 Loss 1.6610\n",
      "Epoch 2 Batch 960 Loss 1.7044\n",
      "Epoch 2 Batch 961 Loss 1.8107\n",
      "Epoch 2 Batch 962 Loss 1.7332\n",
      "Epoch 2 Batch 963 Loss 1.8554\n",
      "Epoch 2 Batch 964 Loss 1.7067\n",
      "Epoch 2 Batch 965 Loss 1.8036\n",
      "Epoch 2 Batch 966 Loss 2.2725\n",
      "Epoch 2 Batch 967 Loss 1.9550\n",
      "Epoch 2 Batch 968 Loss 1.7154\n",
      "Epoch 2 Batch 969 Loss 1.4678\n",
      "Epoch 2 Batch 970 Loss 1.5225\n",
      "Epoch 2 Batch 971 Loss 1.5501\n",
      "Epoch 2 Batch 972 Loss 1.7267\n",
      "Epoch 2 Batch 973 Loss 1.9258\n",
      "Epoch 2 Batch 974 Loss 1.7707\n",
      "Epoch 2 Batch 975 Loss 1.7958\n",
      "Epoch 2 Batch 976 Loss 1.5721\n",
      "Epoch 2 Batch 977 Loss 1.4445\n",
      "Epoch 2 Batch 978 Loss 1.3646\n",
      "Epoch 2 Batch 979 Loss 1.5676\n",
      "Epoch 2 Batch 980 Loss 1.4801\n",
      "Epoch 2 Batch 981 Loss 1.2418\n",
      "Epoch 2 Batch 982 Loss 1.5885\n",
      "Epoch 2 Batch 983 Loss 2.0013\n",
      "Epoch 2 Batch 984 Loss 1.4316\n",
      "Epoch 2 Batch 985 Loss 2.0375\n",
      "Epoch 2 Batch 986 Loss 1.7767\n",
      "Epoch 2 Batch 987 Loss 1.6578\n",
      "Epoch 2 Batch 988 Loss 1.5133\n",
      "Epoch 2 Batch 989 Loss 1.4919\n",
      "Epoch 2 Batch 990 Loss 1.8776\n",
      "Epoch 2 Batch 991 Loss 1.6299\n",
      "Epoch 2 Batch 992 Loss 1.3791\n",
      "Epoch 2 Batch 993 Loss 1.8919\n",
      "Epoch 2 Batch 994 Loss 1.7201\n",
      "Epoch 2 Batch 995 Loss 1.1288\n",
      "Epoch 2 Batch 996 Loss 1.8133\n",
      "Epoch 2 Batch 997 Loss 1.8042\n",
      "Epoch 2 Batch 998 Loss 1.4663\n",
      "Epoch 2 Batch 999 Loss 1.8568\n",
      "Epoch 2 Batch 1000 Loss 1.4663\n",
      "Epoch 2 Batch 1001 Loss 1.8363\n",
      "Epoch 2 Batch 1002 Loss 1.8823\n",
      "Epoch 2 Batch 1003 Loss 1.3282\n",
      "Epoch 2 Batch 1004 Loss 2.0314\n",
      "Epoch 2 Batch 1005 Loss 1.7211\n",
      "Epoch 2 Batch 1006 Loss 1.8058\n",
      "Epoch 2 Batch 1007 Loss 1.8792\n",
      "Epoch 2 Batch 1008 Loss 2.1939\n",
      "Epoch 2 Batch 1009 Loss 1.4666\n",
      "Epoch 2 Batch 1010 Loss 1.3623\n",
      "Epoch 2 Batch 1011 Loss 1.6969\n",
      "Epoch 2 Batch 1012 Loss 2.0054\n",
      "Epoch 2 Batch 1013 Loss 2.2421\n",
      "Epoch 2 Batch 1014 Loss 1.3868\n",
      "Epoch 2 Batch 1015 Loss 1.5778\n",
      "Epoch 2 Batch 1016 Loss 2.0055\n",
      "Epoch 2 Batch 1017 Loss 1.7744\n",
      "Epoch 2 Batch 1018 Loss 1.9526\n",
      "Epoch 2 Batch 1019 Loss 1.5780\n",
      "Epoch 2 Batch 1020 Loss 1.4470\n",
      "Epoch 2 Batch 1021 Loss 1.9172\n",
      "Epoch 2 Batch 1022 Loss 1.5388\n",
      "Epoch 2 Batch 1023 Loss 1.8614\n",
      "Epoch 2 Batch 1024 Loss 1.6784\n",
      "Epoch 2 Batch 1025 Loss 1.6726\n",
      "Epoch 2 Batch 1026 Loss 1.4677\n",
      "Epoch 2 Batch 1027 Loss 1.7827\n",
      "Epoch 2 Batch 1028 Loss 1.6861\n",
      "Epoch 2 Batch 1029 Loss 1.4560\n",
      "Epoch 2 Batch 1030 Loss 1.7159\n",
      "Epoch 2 Batch 1031 Loss 1.6938\n",
      "Epoch 2 Batch 1032 Loss 1.4177\n",
      "Epoch 2 Batch 1033 Loss 1.8189\n",
      "Epoch 2 Batch 1034 Loss 1.5858\n",
      "Epoch 2 Batch 1035 Loss 1.6680\n",
      "Epoch 2 Batch 1036 Loss 1.3936\n",
      "Epoch 2 Batch 1037 Loss 1.6818\n",
      "Epoch 2 Batch 1038 Loss 1.4724\n",
      "Epoch 2 Batch 1039 Loss 1.8248\n",
      "Epoch 2 Batch 1040 Loss 1.5722\n",
      "Epoch 2 Batch 1041 Loss 1.4926\n",
      "Epoch 2 Batch 1042 Loss 1.4896\n",
      "Epoch 2 Batch 1043 Loss 1.4405\n",
      "Epoch 2 Batch 1044 Loss 1.0914\n",
      "Epoch 2 Batch 1045 Loss 1.5917\n",
      "Epoch 2 Batch 1046 Loss 1.3108\n",
      "Epoch 2 Batch 1047 Loss 1.6200\n",
      "Epoch 2 Batch 1048 Loss 1.8785\n",
      "Epoch 2 Batch 1049 Loss 1.7279\n",
      "Epoch 2 Batch 1050 Loss 2.0389\n",
      "Epoch 2 Batch 1051 Loss 2.1791\n",
      "Epoch 2 Batch 1052 Loss 1.9036\n",
      "Epoch 2 Batch 1053 Loss 2.0722\n",
      "Epoch 2 Batch 1054 Loss 2.0275\n",
      "Epoch 2 Batch 1055 Loss 1.4679\n",
      "Epoch 2 Batch 1056 Loss 1.3682\n",
      "Epoch 2 Batch 1057 Loss 1.5650\n",
      "Epoch 2 Batch 1058 Loss 1.5938\n",
      "Epoch 2 Batch 1059 Loss 1.6698\n",
      "Epoch 2 Batch 1060 Loss 1.8174\n",
      "Epoch 2 Batch 1061 Loss 1.6791\n",
      "Epoch 2 Batch 1062 Loss 1.5456\n",
      "Epoch 2 Batch 1063 Loss 1.4573\n",
      "Epoch 2 Batch 1064 Loss 1.4445\n",
      "Epoch 2 Batch 1065 Loss 1.5523\n",
      "Epoch 2 Batch 1066 Loss 1.7494\n",
      "Epoch 2 Batch 1067 Loss 1.0880\n",
      "Epoch 2 Batch 1068 Loss 1.4308\n",
      "Epoch 2 Batch 1069 Loss 1.3284\n",
      "Epoch 2 Batch 1070 Loss 1.4717\n",
      "Epoch 2 Batch 1071 Loss 1.3725\n",
      "Epoch 2 Batch 1072 Loss 1.7899\n",
      "Epoch 2 Batch 1073 Loss 1.8022\n",
      "Epoch 2 Batch 1074 Loss 1.7827\n",
      "Epoch 2 Batch 1075 Loss 1.8728\n",
      "Epoch 2 Batch 1076 Loss 1.8836\n",
      "Epoch 2 Batch 1077 Loss 1.3009\n",
      "Epoch 2 Batch 1078 Loss 1.6159\n",
      "Epoch 2 Batch 1079 Loss 1.6584\n",
      "Epoch 2 Batch 1080 Loss 1.6525\n",
      "Epoch 2 Batch 1081 Loss 2.0516\n",
      "Epoch 2 Batch 1082 Loss 1.4903\n",
      "Epoch 2 Batch 1083 Loss 2.1878\n",
      "Epoch 2 Batch 1084 Loss 2.0066\n",
      "Epoch 2 Batch 1085 Loss 1.2790\n",
      "Epoch 2 Batch 1086 Loss 2.0978\n",
      "Epoch 2 Batch 1087 Loss 1.7714\n",
      "Epoch 2 Batch 1088 Loss 1.6523\n",
      "Epoch 2 Batch 1089 Loss 1.9551\n",
      "Epoch 2 Batch 1090 Loss 1.6719\n",
      "Epoch 2 Batch 1091 Loss 1.8056\n",
      "Epoch 2 Batch 1092 Loss 1.8054\n",
      "Epoch 2 Batch 1093 Loss 2.1552\n",
      "Epoch 2 Batch 1094 Loss 1.8502\n",
      "Epoch 2 Batch 1095 Loss 2.2264\n",
      "Epoch 2 Batch 1096 Loss 1.7849\n",
      "Epoch 2 Batch 1097 Loss 2.3963\n",
      "Epoch 2 Batch 1098 Loss 1.8317\n",
      "Epoch 2 Batch 1099 Loss 1.3921\n",
      "Epoch 2 Batch 1100 Loss 1.6212\n",
      "Epoch 2 Batch 1101 Loss 1.6159\n",
      "Epoch 2 Batch 1102 Loss 1.6336\n",
      "Epoch 2 Batch 1103 Loss 1.6359\n",
      "Epoch 2 Batch 1104 Loss 2.0227\n",
      "Epoch 2 Batch 1105 Loss 2.1628\n",
      "Epoch 2 Batch 1106 Loss 1.6928\n",
      "Epoch 2 Batch 1107 Loss 1.6795\n",
      "Epoch 2 Batch 1108 Loss 2.1943\n",
      "Epoch 2 Batch 1109 Loss 2.0800\n",
      "Epoch 2 Batch 1110 Loss 1.9729\n",
      "Epoch 2 Batch 1111 Loss 1.6126\n",
      "Epoch 2 Batch 1112 Loss 1.6978\n",
      "Epoch 2 Batch 1113 Loss 1.2623\n",
      "Epoch 2 Batch 1114 Loss 1.8485\n",
      "Epoch 2 Batch 1115 Loss 1.6445\n",
      "Epoch 2 Batch 1116 Loss 1.6035\n",
      "Epoch 2 Batch 1117 Loss 1.9465\n",
      "Epoch 2 Batch 1118 Loss 1.5827\n",
      "Epoch 2 Batch 1119 Loss 1.6218\n",
      "Epoch 2 Batch 1120 Loss 1.9362\n",
      "Epoch 2 Batch 1121 Loss 2.3749\n",
      "Epoch 2 Batch 1122 Loss 2.0132\n",
      "Epoch 2 Batch 1123 Loss 1.4746\n",
      "Epoch 2 Batch 1124 Loss 1.6748\n",
      "Epoch 2 Batch 1125 Loss 1.7683\n",
      "Epoch 2 Batch 1126 Loss 2.1023\n",
      "Epoch 2 Batch 1127 Loss 1.9712\n",
      "Epoch 2 Batch 1128 Loss 1.8915\n",
      "Epoch 2 Batch 1129 Loss 1.9285\n",
      "Epoch 2 Batch 1130 Loss 2.1233\n",
      "Epoch 2 Batch 1131 Loss 1.9678\n",
      "Epoch 2 Batch 1132 Loss 2.0678\n",
      "Epoch 2 Batch 1133 Loss 2.2281\n",
      "Epoch 2 Batch 1134 Loss 1.7627\n",
      "Epoch 2 Batch 1135 Loss 2.0096\n",
      "Epoch 2 Batch 1136 Loss 2.0475\n",
      "Epoch 2 Batch 1137 Loss 1.6452\n",
      "Epoch 2 Batch 1138 Loss 2.0009\n",
      "Epoch 2 Batch 1139 Loss 2.0681\n",
      "Epoch 2 Batch 1140 Loss 1.9557\n",
      "Epoch 2 Batch 1141 Loss 2.4248\n",
      "Epoch 2 Batch 1142 Loss 1.6630\n",
      "Epoch 2 Batch 1143 Loss 1.5879\n",
      "Epoch 2 Batch 1144 Loss 1.5722\n",
      "Epoch 2 Batch 1145 Loss 1.4795\n",
      "Epoch 2 Batch 1146 Loss 1.1936\n",
      "Epoch 2 Batch 1147 Loss 1.5112\n",
      "Epoch 2 Batch 1148 Loss 1.6461\n",
      "Epoch 2 Batch 1149 Loss 1.5310\n",
      "Epoch 2 Batch 1150 Loss 1.6978\n",
      "Epoch 2 Batch 1151 Loss 1.8201\n",
      "Epoch 2 Batch 1152 Loss 1.2586\n",
      "Epoch 2 Batch 1153 Loss 1.7789\n",
      "Epoch 2 Batch 1154 Loss 1.9636\n",
      "Epoch 2 Batch 1155 Loss 1.9413\n",
      "Epoch 2 Batch 1156 Loss 2.2883\n",
      "Epoch 2 Batch 1157 Loss 1.9872\n",
      "Epoch 2 Batch 1158 Loss 2.0276\n",
      "Epoch 2 Batch 1159 Loss 1.8376\n",
      "Epoch 2 Batch 1160 Loss 1.3981\n",
      "Epoch 2 Batch 1161 Loss 1.5813\n",
      "Epoch 2 Batch 1162 Loss 1.9709\n",
      "Epoch 2 Batch 1163 Loss 1.3658\n",
      "Epoch 2 Batch 1164 Loss 2.0392\n",
      "Epoch 2 Batch 1165 Loss 1.5711\n",
      "Epoch 2 Batch 1166 Loss 1.9809\n",
      "Epoch 2 Batch 1167 Loss 1.7746\n",
      "Epoch 2 Batch 1168 Loss 1.7589\n",
      "Epoch 2 Batch 1169 Loss 1.4876\n",
      "Epoch 2 Batch 1170 Loss 1.7478\n",
      "Epoch 2 Batch 1171 Loss 1.6023\n",
      "Epoch 2 Batch 1172 Loss 1.3661\n",
      "Epoch 2 Batch 1173 Loss 1.4006\n",
      "Epoch 2 Batch 1174 Loss 1.3387\n",
      "Epoch 2 Batch 1175 Loss 1.1044\n",
      "Epoch 2 Batch 1176 Loss 1.0778\n",
      "Epoch 2 Batch 1177 Loss 1.5829\n",
      "Epoch 2 Batch 1178 Loss 1.4345\n",
      "Epoch 2 Batch 1179 Loss 1.4446\n",
      "Epoch 2 Batch 1180 Loss 1.4609\n",
      "Epoch 2 Batch 1181 Loss 1.4733\n",
      "Epoch 2 Batch 1182 Loss 1.5680\n",
      "Epoch 2 Batch 1183 Loss 1.6858\n",
      "Epoch 2 Batch 1184 Loss 2.5654\n",
      "Epoch 2 Batch 1185 Loss 1.4716\n",
      "Epoch 2 Batch 1186 Loss 1.5884\n",
      "Epoch 2 Batch 1187 Loss 1.8944\n",
      "Epoch 2 Batch 1188 Loss 2.1542\n",
      "Epoch 2 Batch 1189 Loss 2.4145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Batch 1190 Loss 1.7528\n",
      "Epoch 2 Batch 1191 Loss 1.5955\n",
      "Epoch 2 Batch 1192 Loss 1.8334\n",
      "Epoch 2 Batch 1193 Loss 2.0395\n",
      "Epoch 2 Batch 1194 Loss 1.5605\n",
      "Epoch 2 Batch 1195 Loss 1.8096\n",
      "Epoch 2 Batch 1196 Loss 1.7214\n",
      "Epoch 2 Batch 1197 Loss 1.7850\n",
      "Epoch 2 Batch 1198 Loss 1.3455\n",
      "Epoch 2 Batch 1199 Loss 1.4999\n",
      "Epoch 2 Batch 1200 Loss 1.5413\n",
      "Epoch 2 Batch 1201 Loss 1.5179\n",
      "Epoch 2 Batch 1202 Loss 2.0812\n",
      "Epoch 2 Batch 1203 Loss 1.5796\n",
      "Epoch 2 Batch 1204 Loss 1.2040\n",
      "Epoch 2 Batch 1205 Loss 1.8587\n",
      "Epoch 2 Batch 1206 Loss 1.2200\n",
      "Epoch 2 Batch 1207 Loss 1.6254\n",
      "Epoch 2 Batch 1208 Loss 1.8285\n",
      "Epoch 2 Batch 1209 Loss 1.7545\n",
      "Epoch 2 Batch 1210 Loss 1.6746\n",
      "Epoch 2 Batch 1211 Loss 1.9319\n",
      "Epoch 2 Batch 1212 Loss 2.0877\n",
      "Epoch 2 Batch 1213 Loss 1.8622\n",
      "Epoch 2 Batch 1214 Loss 2.0177\n",
      "Epoch 2 Batch 1215 Loss 1.6217\n",
      "Epoch 2 Batch 1216 Loss 1.8074\n",
      "Epoch 2 Batch 1217 Loss 1.4569\n",
      "Epoch 2 Batch 1218 Loss 1.6666\n",
      "Epoch 2 Batch 1219 Loss 1.6782\n",
      "Epoch 2 Batch 1220 Loss 2.0254\n",
      "Epoch 2 Batch 1221 Loss 1.5316\n",
      "Epoch 2 Batch 1222 Loss 1.9111\n",
      "Epoch 2 Batch 1223 Loss 1.5907\n",
      "Epoch 2 Batch 1224 Loss 2.0935\n",
      "Epoch 2 Batch 1225 Loss 1.7128\n",
      "Epoch 2 Batch 1226 Loss 1.3552\n",
      "Epoch 2 Batch 1227 Loss 1.5370\n",
      "Epoch 2 Batch 1228 Loss 1.5014\n",
      "Epoch 2 Batch 1229 Loss 1.7428\n",
      "Epoch 2 Batch 1230 Loss 2.0445\n",
      "Epoch 2 Batch 1231 Loss 2.4221\n",
      "Epoch 2 Batch 1232 Loss 2.1698\n",
      "Epoch 2 Batch 1233 Loss 2.1812\n",
      "Epoch 2 Batch 1234 Loss 1.8754\n",
      "Epoch 2 Batch 1235 Loss 1.5257\n",
      "Epoch 2 Batch 1236 Loss 1.3825\n",
      "Epoch 2 Batch 1237 Loss 1.9033\n",
      "Epoch 2 Batch 1238 Loss 1.8228\n",
      "Epoch 2 Batch 1239 Loss 1.6659\n",
      "Epoch 2 Batch 1240 Loss 1.4150\n",
      "Epoch 2 Batch 1241 Loss 1.6100\n",
      "Epoch 2 Batch 1242 Loss 1.3729\n",
      "Epoch 2 Batch 1243 Loss 1.6554\n",
      "Epoch 2 Batch 1244 Loss 2.0482\n",
      "Epoch 2 Batch 1245 Loss 1.9255\n",
      "Epoch 2 Batch 1246 Loss 1.7905\n",
      "Epoch 2 Batch 1247 Loss 2.0034\n",
      "Epoch 2 Batch 1248 Loss 1.7761\n",
      "Epoch 2 Batch 1249 Loss 1.9402\n",
      "Epoch 2 Batch 1250 Loss 1.6343\n",
      "Epoch 2 Batch 1251 Loss 1.6032\n",
      "Epoch 2 Batch 1252 Loss 1.6475\n",
      "Epoch 2 Batch 1253 Loss 2.0461\n",
      "Epoch 2 Batch 1254 Loss 1.9909\n",
      "Epoch 2 Batch 1255 Loss 1.5995\n",
      "Epoch 2 Batch 1256 Loss 1.7109\n",
      "Epoch 2 Batch 1257 Loss 1.5256\n",
      "Epoch 2 Batch 1258 Loss 1.8084\n",
      "Epoch 2 Batch 1259 Loss 2.0719\n",
      "Epoch 2 Batch 1260 Loss 1.5586\n",
      "Epoch 2 Batch 1261 Loss 1.8821\n",
      "Epoch 2 Batch 1262 Loss 1.7098\n",
      "Epoch 2 Batch 1263 Loss 1.7853\n",
      "Epoch 2 Batch 1264 Loss 1.9006\n",
      "Epoch 2 Batch 1265 Loss 1.7867\n",
      "Epoch 2 Batch 1266 Loss 1.6824\n",
      "Epoch 2 Batch 1267 Loss 1.8915\n",
      "Epoch 2 Batch 1268 Loss 1.7728\n",
      "Epoch 2 Batch 1269 Loss 2.1849\n",
      "Epoch 2 Batch 1270 Loss 1.9781\n",
      "Epoch 2 Batch 1271 Loss 2.0348\n",
      "Epoch 2 Batch 1272 Loss 1.8724\n",
      "Epoch 2 Batch 1273 Loss 2.7075\n",
      "Epoch 2 Batch 1274 Loss 2.2235\n",
      "Epoch 2 Batch 1275 Loss 2.1086\n",
      "Epoch 2 Batch 1276 Loss 2.1421\n",
      "Epoch 2 Batch 1277 Loss 1.7066\n",
      "Epoch 2 Batch 1278 Loss 1.5998\n",
      "Epoch 2 Batch 1279 Loss 1.9294\n",
      "Epoch 2 Batch 1280 Loss 2.0177\n",
      "Epoch 2 Batch 1281 Loss 2.1477\n",
      "Epoch 2 Batch 1282 Loss 1.5140\n",
      "Epoch 2 Batch 1283 Loss 1.2277\n",
      "Epoch 2 Batch 1284 Loss 1.1435\n",
      "Epoch 2 Batch 1285 Loss 1.7263\n",
      "Epoch 2 Batch 1286 Loss 1.4626\n",
      "Epoch 2 Batch 1287 Loss 1.8083\n",
      "Epoch 2 Batch 1288 Loss 1.4956\n",
      "Epoch 2 Batch 1289 Loss 1.5469\n",
      "Epoch 2 Batch 1290 Loss 1.5779\n",
      "Epoch 2 Batch 1291 Loss 1.5248\n",
      "Epoch 2 Batch 1292 Loss 1.3204\n",
      "Epoch 2 Batch 1293 Loss 1.1916\n",
      "Epoch 2 Batch 1294 Loss 1.2445\n",
      "Epoch 2 Batch 1295 Loss 1.1948\n",
      "Epoch 2 Batch 1296 Loss 1.0129\n",
      "Epoch 2 Batch 1297 Loss 1.1635\n",
      "Epoch 2 Batch 1298 Loss 1.2517\n",
      "Epoch 2 Batch 1299 Loss 1.1963\n",
      "Epoch 2 Batch 1300 Loss 1.4916\n",
      "Epoch 2 Batch 1301 Loss 1.3621\n",
      "Epoch 2 Batch 1302 Loss 1.3464\n",
      "Epoch 2 Batch 1303 Loss 1.2548\n",
      "Epoch 2 Batch 1304 Loss 1.7214\n",
      "Epoch 2 Batch 1305 Loss 1.6208\n",
      "Epoch 2 Batch 1306 Loss 1.3217\n",
      "Epoch 2 Batch 1307 Loss 1.5597\n",
      "Epoch 2 Batch 1308 Loss 1.6850\n",
      "Epoch 2 Batch 1309 Loss 1.8821\n",
      "Epoch 2 Batch 1310 Loss 1.8888\n",
      "Epoch 2 Batch 1311 Loss 1.8353\n",
      "Epoch 2 Batch 1312 Loss 1.5805\n",
      "Epoch 2 Batch 1313 Loss 1.4743\n",
      "Epoch 2 Batch 1314 Loss 1.0880\n",
      "Epoch 2 Batch 1315 Loss 1.3083\n",
      "Epoch 2 Batch 1316 Loss 1.5250\n",
      "Epoch 2 Batch 1317 Loss 1.2961\n",
      "Epoch 2 Batch 1318 Loss 1.7011\n",
      "Epoch 2 Batch 1319 Loss 1.6691\n",
      "Epoch 2 Batch 1320 Loss 1.7979\n",
      "Epoch 2 Batch 1321 Loss 1.4542\n",
      "Epoch 2 Batch 1322 Loss 1.7462\n",
      "Epoch 2 Batch 1323 Loss 2.4471\n",
      "Epoch 2 Batch 1324 Loss 1.8233\n",
      "Epoch 2 Batch 1325 Loss 1.7886\n",
      "Epoch 2 Batch 1326 Loss 1.5434\n",
      "Epoch 2 Batch 1327 Loss 1.5936\n",
      "Epoch 2 Batch 1328 Loss 1.7693\n",
      "Epoch 2 Batch 1329 Loss 1.6698\n",
      "Epoch 2 Batch 1330 Loss 1.3526\n",
      "Epoch 2 Batch 1331 Loss 1.1130\n",
      "Epoch 2 Batch 1332 Loss 1.6353\n",
      "Epoch 2 Batch 1333 Loss 1.5971\n",
      "Epoch 2 Batch 1334 Loss 1.8784\n",
      "Epoch 2 Batch 1335 Loss 1.8295\n",
      "Epoch 2 Batch 1336 Loss 1.5644\n",
      "Epoch 2 Batch 1337 Loss 2.1188\n",
      "Epoch 2 Batch 1338 Loss 1.8504\n",
      "Epoch 2 Batch 1339 Loss 2.4437\n",
      "Epoch 2 Batch 1340 Loss 1.6760\n",
      "Epoch 2 Batch 1341 Loss 1.6278\n",
      "Epoch 2 Batch 1342 Loss 2.0120\n",
      "Epoch 2 Batch 1343 Loss 1.5598\n",
      "Epoch 2 Batch 1344 Loss 1.7129\n",
      "Epoch 2 Batch 1345 Loss 1.6006\n",
      "Epoch 2 Batch 1346 Loss 1.3178\n",
      "Epoch 2 Batch 1347 Loss 1.5268\n",
      "Epoch 2 Batch 1348 Loss 2.1685\n",
      "Epoch 2 Batch 1349 Loss 1.5624\n",
      "Epoch 2 Batch 1350 Loss 1.6382\n",
      "Epoch 2 Batch 1351 Loss 1.8061\n",
      "Epoch 2 Batch 1352 Loss 1.4644\n",
      "Epoch 2 Batch 1353 Loss 1.6098\n",
      "Epoch 2 Batch 1354 Loss 1.8763\n",
      "Epoch 2 Batch 1355 Loss 1.8656\n",
      "Epoch 2 Batch 1356 Loss 1.6801\n",
      "Epoch 2 Batch 1357 Loss 1.2502\n",
      "Epoch 2 Batch 1358 Loss 2.1800\n",
      "Epoch 2 Batch 1359 Loss 1.3267\n",
      "Epoch 2 Batch 1360 Loss 1.6163\n",
      "Epoch 2 Batch 1361 Loss 1.8590\n",
      "Epoch 2 Batch 1362 Loss 1.8084\n",
      "Epoch 2 Batch 1363 Loss 1.6329\n",
      "Epoch 2 Batch 1364 Loss 1.7435\n",
      "Epoch 2 Batch 1365 Loss 1.6424\n",
      "Epoch 2 Batch 1366 Loss 1.4889\n",
      "Epoch 2 Batch 1367 Loss 1.3313\n",
      "Epoch 2 Batch 1368 Loss 1.6953\n",
      "Epoch 2 Batch 1369 Loss 1.5550\n",
      "Epoch 2 Batch 1370 Loss 1.7927\n",
      "Epoch 2 Batch 1371 Loss 1.5611\n",
      "Epoch 2 Batch 1372 Loss 1.7343\n",
      "Epoch 2 Batch 1373 Loss 1.7481\n",
      "Epoch 2 Batch 1374 Loss 1.7966\n",
      "Epoch 2 Batch 1375 Loss 1.4590\n",
      "Epoch 2 Batch 1376 Loss 1.2462\n",
      "Epoch 2 Batch 1377 Loss 1.1591\n",
      "Epoch 2 Batch 1378 Loss 1.6286\n",
      "Epoch 2 Batch 1379 Loss 1.8922\n",
      "Epoch 2 Batch 1380 Loss 1.7533\n",
      "Epoch 2 Batch 1381 Loss 1.8509\n",
      "Epoch 2 Batch 1382 Loss 1.5218\n",
      "Epoch 2 Batch 1383 Loss 1.4990\n",
      "Epoch 2 Batch 1384 Loss 1.6520\n",
      "Epoch 2 Batch 1385 Loss 2.1127\n",
      "Epoch 2 Batch 1386 Loss 1.7977\n",
      "Epoch 2 Batch 1387 Loss 1.2188\n",
      "Epoch 2 Batch 1388 Loss 1.5690\n",
      "Epoch 2 Batch 1389 Loss 1.6889\n",
      "Epoch 2 Batch 1390 Loss 1.9615\n",
      "Epoch 2 Batch 1391 Loss 1.6045\n",
      "Epoch 2 Batch 1392 Loss 1.7762\n",
      "Epoch 2 Batch 1393 Loss 2.0130\n",
      "Epoch 2 Batch 1394 Loss 1.5170\n",
      "Epoch 2 Batch 1395 Loss 1.4043\n",
      "Epoch 2 Batch 1396 Loss 1.7674\n",
      "Epoch 2 Batch 1397 Loss 1.2384\n",
      "Epoch 2 Batch 1398 Loss 1.7248\n",
      "Epoch 2 Batch 1399 Loss 1.3416\n",
      "Epoch 2 Batch 1400 Loss 1.5720\n",
      "Epoch 2 Batch 1401 Loss 1.2901\n",
      "Epoch 2 Batch 1402 Loss 1.7715\n",
      "Epoch 2 Batch 1403 Loss 1.4102\n",
      "Epoch 2 Batch 1404 Loss 1.3412\n",
      "Epoch 2 Batch 1405 Loss 1.9489\n",
      "Epoch 2 Batch 1406 Loss 1.8108\n",
      "Epoch 2 Batch 1407 Loss 2.0377\n",
      "Epoch 2 Batch 1408 Loss 2.4731\n",
      "Epoch 2 Batch 1409 Loss 1.9545\n",
      "Epoch 2 Batch 1410 Loss 1.6347\n",
      "Epoch 2 Batch 1411 Loss 1.5599\n",
      "Epoch 2 Batch 1412 Loss 1.2016\n",
      "Epoch 2 Batch 1413 Loss 1.3721\n",
      "Epoch 2 Batch 1414 Loss 1.6092\n",
      "Epoch 2 Batch 1415 Loss 1.3945\n",
      "Epoch 2 Batch 1416 Loss 1.8650\n",
      "Epoch 2 Batch 1417 Loss 1.5344\n",
      "Epoch 2 Batch 1418 Loss 1.7016\n",
      "Epoch 2 Batch 1419 Loss 1.8947\n",
      "Epoch 2 Batch 1420 Loss 1.3030\n",
      "Epoch 2 Batch 1421 Loss 1.8485\n",
      "Epoch 2 Batch 1422 Loss 1.5820\n",
      "Epoch 2 Batch 1423 Loss 1.7154\n",
      "Epoch 2 Batch 1424 Loss 1.6244\n",
      "Epoch 2 Batch 1425 Loss 1.3991\n",
      "Epoch 2 Batch 1426 Loss 2.0497\n",
      "Epoch 2 Batch 1427 Loss 1.6285\n",
      "Epoch 2 Batch 1428 Loss 1.8485\n",
      "Epoch 2 Batch 1429 Loss 1.5346\n",
      "Epoch 2 Batch 1430 Loss 1.2665\n",
      "Epoch 2 Batch 1431 Loss 1.7965\n",
      "Epoch 2 Batch 1432 Loss 1.6550\n",
      "Epoch 2 Batch 1433 Loss 1.6108\n",
      "Epoch 2 Batch 1434 Loss 1.8992\n",
      "Epoch 2 Batch 1435 Loss 2.0372\n",
      "Epoch 2 Batch 1436 Loss 2.0558\n",
      "Epoch 2 Batch 1437 Loss 1.8556\n",
      "Epoch 2 Batch 1438 Loss 1.4977\n",
      "Epoch 2 Batch 1439 Loss 2.1460\n",
      "Epoch 2 Batch 1440 Loss 1.6371\n",
      "Epoch 2 Batch 1441 Loss 1.8149\n",
      "Epoch 2 Batch 1442 Loss 2.1965\n",
      "Epoch 2 Batch 1443 Loss 2.1949\n",
      "Epoch 2 Batch 1444 Loss 1.9077\n",
      "Epoch 2 Batch 1445 Loss 2.4105\n",
      "Epoch 2 Batch 1446 Loss 1.4536\n",
      "Epoch 2 Batch 1447 Loss 1.8514\n",
      "Epoch 2 Batch 1448 Loss 1.8697\n",
      "Epoch 2 Batch 1449 Loss 1.4638\n",
      "Epoch 2 Batch 1450 Loss 1.4691\n",
      "Epoch 2 Batch 1451 Loss 1.3049\n",
      "Epoch 2 Batch 1452 Loss 1.8389\n",
      "Epoch 2 Batch 1453 Loss 1.6528\n",
      "Epoch 2 Batch 1454 Loss 1.8537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Batch 1455 Loss 1.6142\n",
      "Epoch 2 Batch 1456 Loss 1.8257\n",
      "Epoch 2 Batch 1457 Loss 1.4931\n",
      "Epoch 2 Batch 1458 Loss 1.1862\n",
      "Epoch 2 Batch 1459 Loss 1.1145\n",
      "Epoch 2 Batch 1460 Loss 1.2765\n",
      "Epoch 2 Batch 1461 Loss 1.0143\n",
      "Epoch 2 Batch 1462 Loss 1.0054\n",
      "Epoch 2 Batch 1463 Loss 1.7929\n",
      "Epoch 2 Batch 1464 Loss 1.1978\n",
      "Epoch 2 Batch 1465 Loss 1.5673\n",
      "Epoch 2 Batch 1466 Loss 1.4462\n",
      "Epoch 2 Batch 1467 Loss 2.0491\n",
      "Epoch 2 Batch 1468 Loss 2.0292\n",
      "Epoch 2 Batch 1469 Loss 2.0575\n",
      "Epoch 2 Batch 1470 Loss 1.3817\n",
      "Epoch 2 Batch 1471 Loss 1.9084\n",
      "Epoch 2 Batch 1472 Loss 2.1215\n",
      "Epoch 2 Batch 1473 Loss 1.4038\n",
      "Epoch 2 Batch 1474 Loss 1.6969\n",
      "Epoch 2 Batch 1475 Loss 1.6857\n",
      "Epoch 2 Batch 1476 Loss 1.8511\n",
      "Epoch 2 Batch 1477 Loss 2.0597\n",
      "Epoch 2 Batch 1478 Loss 1.4801\n",
      "Epoch 2 Batch 1479 Loss 1.6080\n",
      "Epoch 2 Batch 1480 Loss 1.3468\n",
      "Epoch 2 Batch 1481 Loss 1.5620\n",
      "Epoch 2 Batch 1482 Loss 1.7721\n",
      "Epoch 2 Batch 1483 Loss 1.8492\n",
      "Epoch 2 Batch 1484 Loss 1.5591\n",
      "Epoch 2 Batch 1485 Loss 1.4651\n",
      "Epoch 2 Batch 1486 Loss 1.1129\n",
      "Epoch 2 Batch 1487 Loss 1.4972\n",
      "Epoch 2 Batch 1488 Loss 1.5234\n",
      "Epoch 2 Batch 1489 Loss 1.3769\n",
      "Epoch 2 Batch 1490 Loss 1.6602\n",
      "Epoch 2 Batch 1491 Loss 2.0165\n",
      "Epoch 2 Batch 1492 Loss 1.8799\n",
      "Epoch 2 Batch 1493 Loss 1.9218\n",
      "Epoch 2 Batch 1494 Loss 1.9146\n",
      "Epoch 2 Batch 1495 Loss 1.9665\n",
      "Epoch 2 Batch 1496 Loss 1.4068\n",
      "Epoch 2 Batch 1497 Loss 1.2130\n",
      "Epoch 2 Batch 1498 Loss 1.5162\n",
      "Epoch 2 Batch 1499 Loss 1.6021\n",
      "Epoch 2 Batch 1500 Loss 1.6432\n",
      "Epoch 2 Batch 1501 Loss 1.5865\n",
      "Epoch 2 Batch 1502 Loss 1.7556\n",
      "Epoch 2 Batch 1503 Loss 1.7739\n",
      "Epoch 2 Batch 1504 Loss 1.5939\n",
      "Epoch 2 Batch 1505 Loss 1.6603\n",
      "Epoch 2 Batch 1506 Loss 1.2520\n",
      "Epoch 2 Batch 1507 Loss 1.4174\n",
      "Epoch 2 Batch 1508 Loss 1.8456\n",
      "Epoch 2 Batch 1509 Loss 1.6023\n",
      "Epoch 2 Batch 1510 Loss 1.5761\n",
      "Epoch 2 Batch 1511 Loss 1.9670\n",
      "Epoch 2 Batch 1512 Loss 1.8545\n",
      "Epoch 2 Batch 1513 Loss 1.5843\n",
      "Epoch 2 Batch 1514 Loss 1.8314\n",
      "Epoch 2 Batch 1515 Loss 2.1149\n",
      "Epoch 2 Batch 1516 Loss 2.2516\n",
      "Epoch 2 Batch 1517 Loss 1.8553\n",
      "Epoch 2 Batch 1518 Loss 1.6022\n",
      "Epoch 2 Batch 1519 Loss 1.5491\n",
      "Epoch 2 Batch 1520 Loss 1.5119\n",
      "Epoch 2 Batch 1521 Loss 1.7290\n",
      "Epoch 2 Batch 1522 Loss 2.1324\n",
      "Epoch 2 Batch 1523 Loss 1.7469\n",
      "Epoch 2 Batch 1524 Loss 1.4370\n",
      "Epoch 2 Batch 1525 Loss 1.8129\n",
      "Epoch 2 Batch 1526 Loss 1.7554\n",
      "Epoch 2 Batch 1527 Loss 1.3743\n",
      "Epoch 2 Batch 1528 Loss 1.8594\n",
      "Epoch 2 Batch 1529 Loss 2.3494\n",
      "Epoch 2 Batch 1530 Loss 1.6700\n",
      "Epoch 2 Batch 1531 Loss 1.6030\n",
      "Epoch 2 Batch 1532 Loss 2.0119\n",
      "Epoch 2 Batch 1533 Loss 2.2462\n",
      "Epoch 2 Batch 1534 Loss 2.2964\n",
      "Epoch 2 Batch 1535 Loss 1.9821\n",
      "Epoch 2 Batch 1536 Loss 1.4217\n",
      "Epoch 2 Batch 1537 Loss 1.5635\n",
      "Epoch 2 Batch 1538 Loss 1.4842\n",
      "Epoch 2 Batch 1539 Loss 1.8185\n",
      "Epoch 2 Batch 1540 Loss 1.6790\n",
      "Epoch 2 Batch 1541 Loss 1.4557\n",
      "Epoch 2 Batch 1542 Loss 1.3197\n",
      "Epoch 2 Batch 1543 Loss 1.3934\n",
      "Epoch 2 Batch 1544 Loss 1.6491\n",
      "Epoch 2 Batch 1545 Loss 1.4187\n",
      "Epoch 2 Batch 1546 Loss 1.7834\n",
      "Epoch 2 Batch 1547 Loss 1.9674\n",
      "Epoch 2 Batch 1548 Loss 2.1026\n",
      "Epoch 2 Batch 1549 Loss 1.4887\n",
      "Epoch 2 Batch 1550 Loss 1.5104\n",
      "Epoch 2 Batch 1551 Loss 1.7075\n",
      "Epoch 2 Batch 1552 Loss 1.4898\n",
      "Epoch 2 Batch 1553 Loss 1.6622\n",
      "Epoch 2 Batch 1554 Loss 1.6111\n",
      "Epoch 2 Batch 1555 Loss 2.0885\n",
      "Epoch 2 Batch 1556 Loss 1.8282\n",
      "Epoch 2 Batch 1557 Loss 1.7690\n",
      "Epoch 2 Batch 1558 Loss 1.5540\n",
      "Epoch 2 Batch 1559 Loss 1.2456\n",
      "Epoch 2 Batch 1560 Loss 1.5456\n",
      "Epoch 2 Batch 1561 Loss 1.6683\n",
      "Epoch 2 Batch 1562 Loss 1.5284\n",
      "Epoch 2 Batch 1563 Loss 1.5520\n",
      "Epoch 2 Batch 1564 Loss 1.3534\n",
      "Epoch 2 Batch 1565 Loss 2.1442\n",
      "Epoch 2 Batch 1566 Loss 1.9772\n",
      "Epoch 2 Batch 1567 Loss 2.0358\n",
      "Epoch 2 Batch 1568 Loss 1.5962\n",
      "Epoch 2 Batch 1569 Loss 2.0578\n",
      "Epoch 2 Batch 1570 Loss 1.7180\n",
      "Epoch 2 Batch 1571 Loss 2.2403\n",
      "Epoch 2 Batch 1572 Loss 1.7847\n",
      "Epoch 2 Batch 1573 Loss 2.1866\n",
      "Epoch 2 Batch 1574 Loss 1.8463\n",
      "Epoch 2 Batch 1575 Loss 2.0517\n",
      "Epoch 2 Batch 1576 Loss 1.8971\n",
      "Epoch 2 Batch 1577 Loss 2.1187\n",
      "Epoch 2 Batch 1578 Loss 1.9060\n",
      "Epoch 2 Batch 1579 Loss 1.5826\n",
      "Epoch 2 Batch 1580 Loss 1.2996\n",
      "Epoch 2 Batch 1581 Loss 1.8946\n",
      "Epoch 2 Batch 1582 Loss 1.6154\n",
      "Epoch 2 Batch 1583 Loss 1.4340\n",
      "Epoch 2 Batch 1584 Loss 1.8486\n",
      "Epoch 2 Batch 1585 Loss 2.2031\n",
      "Epoch 2 Batch 1586 Loss 2.0309\n",
      "Epoch 2 Batch 1587 Loss 1.4844\n",
      "Epoch 2 Batch 1588 Loss 1.7488\n",
      "Epoch 2 Batch 1589 Loss 1.2037\n",
      "Epoch 2 Batch 1590 Loss 2.0338\n",
      "Epoch 2 Batch 1591 Loss 1.9081\n",
      "Epoch 2 Batch 1592 Loss 1.8173\n",
      "Epoch 2 Batch 1593 Loss 1.4660\n",
      "Epoch 2 Batch 1594 Loss 1.8817\n",
      "Epoch 2 Batch 1595 Loss 1.5852\n",
      "Epoch 2 Batch 1596 Loss 1.6794\n",
      "Epoch 2 Batch 1597 Loss 2.2040\n",
      "Epoch 2 Batch 1598 Loss 2.4913\n",
      "Epoch 2 Batch 1599 Loss 1.6368\n",
      "Epoch 2 Batch 1600 Loss 2.0096\n",
      "Epoch 2 Batch 1601 Loss 1.5893\n",
      "Epoch 2 Batch 1602 Loss 1.9814\n",
      "Epoch 2 Batch 1603 Loss 2.1759\n",
      "Epoch 2 Batch 1604 Loss 2.1174\n",
      "Epoch 2 Batch 1605 Loss 1.7518\n",
      "Epoch 2 Batch 1606 Loss 2.1307\n",
      "Epoch 2 Batch 1607 Loss 1.9687\n",
      "Epoch 2 Batch 1608 Loss 1.8396\n",
      "Epoch 2 Batch 1609 Loss 2.1987\n",
      "Epoch 2 Batch 1610 Loss 1.4669\n",
      "Epoch 2 Batch 1611 Loss 1.6435\n",
      "Epoch 2 Batch 1612 Loss 1.4430\n",
      "Epoch 2 Batch 1613 Loss 1.9264\n",
      "Epoch 2 Batch 1614 Loss 1.6442\n",
      "Epoch 2 Batch 1615 Loss 1.3968\n",
      "Epoch 2 Batch 1616 Loss 1.5824\n",
      "Epoch 2 Batch 1617 Loss 1.4812\n",
      "Epoch 2 Batch 1618 Loss 1.7638\n",
      "Epoch 2 Batch 1619 Loss 1.8075\n",
      "Epoch 2 Batch 1620 Loss 1.6859\n",
      "Epoch 2 Batch 1621 Loss 1.9810\n",
      "Epoch 2 Batch 1622 Loss 1.4974\n",
      "Epoch 2 Batch 1623 Loss 1.4599\n",
      "Epoch 2 Batch 1624 Loss 1.6986\n",
      "Epoch 2 Batch 1625 Loss 1.7033\n",
      "Epoch 2 Batch 1626 Loss 1.6001\n",
      "Epoch 2 Batch 1627 Loss 2.2134\n",
      "Epoch 2 Batch 1628 Loss 2.0320\n",
      "Epoch 2 Batch 1629 Loss 2.2520\n",
      "Epoch 2 Batch 1630 Loss 2.5962\n",
      "Epoch 2 Batch 1631 Loss 1.9194\n",
      "Epoch 2 Batch 1632 Loss 1.7968\n",
      "Epoch 2 Batch 1633 Loss 1.6011\n",
      "Epoch 2 Batch 1634 Loss 1.8817\n",
      "Epoch 2 Batch 1635 Loss 1.9672\n",
      "Epoch 2 Batch 1636 Loss 1.9917\n",
      "Epoch 2 Batch 1637 Loss 1.6392\n",
      "Epoch 2 Batch 1638 Loss 1.5852\n",
      "Epoch 2 Batch 1639 Loss 2.0356\n",
      "Epoch 2 Batch 1640 Loss 1.9385\n",
      "Epoch 2 Batch 1641 Loss 1.6680\n",
      "Epoch 2 Batch 1642 Loss 1.6981\n",
      "Epoch 2 Batch 1643 Loss 1.3043\n",
      "Epoch 2 Batch 1644 Loss 1.5952\n",
      "Epoch 2 Batch 1645 Loss 1.5277\n",
      "Epoch 2 Batch 1646 Loss 1.0931\n",
      "Epoch 2 Batch 1647 Loss 1.0187\n",
      "Epoch 2 Batch 1648 Loss 1.6286\n",
      "Epoch 2 Batch 1649 Loss 1.8177\n",
      "Epoch 2 Batch 1650 Loss 2.1578\n",
      "Epoch 2 Batch 1651 Loss 2.0294\n",
      "Epoch 2 Batch 1652 Loss 1.0633\n",
      "Epoch 2 Batch 1653 Loss 1.8022\n",
      "Epoch 2 Batch 1654 Loss 1.7125\n",
      "Epoch 2 Batch 1655 Loss 1.4521\n",
      "Epoch 2 Batch 1656 Loss 1.6878\n",
      "Epoch 2 Batch 1657 Loss 1.9578\n",
      "Epoch 2 Batch 1658 Loss 2.2647\n",
      "Epoch 2 Batch 1659 Loss 2.0535\n",
      "Epoch 2 Batch 1660 Loss 1.7068\n",
      "Epoch 2 Batch 1661 Loss 1.3185\n",
      "Epoch 2 Batch 1662 Loss 1.8739\n",
      "Epoch 2 Batch 1663 Loss 2.1711\n",
      "Epoch 2 Batch 1664 Loss 1.8896\n",
      "Epoch 2 Batch 1665 Loss 2.1014\n",
      "Epoch 2 Batch 1666 Loss 1.5785\n",
      "Epoch 2 Batch 1667 Loss 2.0995\n",
      "Epoch 2 Batch 1668 Loss 2.4038\n",
      "Epoch 2 Batch 1669 Loss 1.7539\n",
      "Epoch 2 Batch 1670 Loss 1.6949\n",
      "Epoch 2 Batch 1671 Loss 1.8199\n",
      "Epoch 2 Batch 1672 Loss 1.9396\n",
      "Epoch 2 Batch 1673 Loss 1.5451\n",
      "Epoch 2 Batch 1674 Loss 2.1425\n",
      "Epoch 2 Batch 1675 Loss 1.7496\n",
      "Epoch 2 Batch 1676 Loss 1.5669\n",
      "Epoch 2 Batch 1677 Loss 1.8725\n",
      "Epoch 2 Batch 1678 Loss 2.2676\n",
      "Epoch 2 Batch 1679 Loss 2.0239\n",
      "Epoch 2 Batch 1680 Loss 1.6249\n",
      "Epoch 2 Batch 1681 Loss 1.8929\n",
      "Epoch 2 Batch 1682 Loss 1.3341\n",
      "Epoch 2 Batch 1683 Loss 1.2150\n",
      "Epoch 2 Batch 1684 Loss 1.4487\n",
      "Epoch 2 Batch 1685 Loss 1.6677\n",
      "Epoch 2 Batch 1686 Loss 1.4470\n",
      "Epoch 2 Batch 1687 Loss 1.5637\n",
      "Epoch 2 Batch 1688 Loss 1.3572\n",
      "Epoch 2 Batch 1689 Loss 1.5390\n",
      "Epoch 2 Batch 1690 Loss 1.5602\n",
      "Epoch 2 Batch 1691 Loss 1.8284\n",
      "Epoch 2 Batch 1692 Loss 2.0313\n",
      "Epoch 2 Batch 1693 Loss 1.9864\n",
      "Epoch 2 Batch 1694 Loss 2.0046\n",
      "Epoch 2 Batch 1695 Loss 1.6485\n",
      "Epoch 2 Batch 1696 Loss 1.9073\n",
      "Epoch 2 Batch 1697 Loss 1.8361\n",
      "Epoch 2 Batch 1698 Loss 2.1320\n",
      "Epoch 2 Batch 1699 Loss 1.9007\n",
      "Epoch 2 Batch 1700 Loss 1.8773\n",
      "Epoch 2 Batch 1701 Loss 2.1980\n",
      "Epoch 2 Batch 1702 Loss 1.6652\n",
      "Epoch 2 Batch 1703 Loss 2.0311\n",
      "Epoch 2 Batch 1704 Loss 1.5759\n",
      "Epoch 2 Batch 1705 Loss 1.4992\n",
      "Epoch 2 Batch 1706 Loss 1.6575\n",
      "Epoch 2 Batch 1707 Loss 2.0596\n",
      "Epoch 2 Batch 1708 Loss 1.9631\n",
      "Epoch 2 Batch 1709 Loss 1.9211\n",
      "Epoch 2 Batch 1710 Loss 2.0813\n",
      "Epoch 2 Batch 1711 Loss 2.0152\n",
      "Epoch 2 Batch 1712 Loss 2.1452\n",
      "Epoch 2 Batch 1713 Loss 2.0913\n",
      "Epoch 2 Batch 1714 Loss 2.1610\n",
      "Epoch 2 Batch 1715 Loss 1.5589\n",
      "Epoch 2 Batch 1716 Loss 1.6267\n",
      "Epoch 2 Batch 1717 Loss 1.6745\n",
      "Epoch 2 Batch 1718 Loss 1.7699\n",
      "Epoch 2 Batch 1719 Loss 1.9394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Batch 1720 Loss 1.7597\n",
      "Epoch 2 Batch 1721 Loss 1.9094\n",
      "Epoch 2 Batch 1722 Loss 1.6941\n",
      "Epoch 2 Batch 1723 Loss 1.3135\n",
      "Epoch 2 Batch 1724 Loss 1.4657\n",
      "Epoch 2 Batch 1725 Loss 2.0083\n",
      "Epoch 2 Batch 1726 Loss 1.7255\n",
      "Epoch 2 Batch 1727 Loss 2.1762\n",
      "Epoch 2 Batch 1728 Loss 1.3985\n",
      "Epoch 2 Batch 1729 Loss 2.2558\n",
      "Epoch 2 Batch 1730 Loss 1.8317\n",
      "Epoch 2 Batch 1731 Loss 1.7828\n",
      "Epoch 2 Batch 1732 Loss 1.5872\n",
      "Epoch 2 Batch 1733 Loss 1.5347\n",
      "Epoch 2 Batch 1734 Loss 1.2665\n",
      "Epoch 2 Batch 1735 Loss 1.4794\n",
      "Epoch 2 Batch 1736 Loss 1.8928\n",
      "Epoch 2 Batch 1737 Loss 2.2235\n",
      "Epoch 2 Batch 1738 Loss 2.1511\n",
      "Epoch 2 Batch 1739 Loss 1.5051\n",
      "Epoch 2 Batch 1740 Loss 1.9331\n",
      "Epoch 2 Batch 1741 Loss 1.8718\n",
      "Epoch 2 Batch 1742 Loss 1.4493\n",
      "Epoch 2 Batch 1743 Loss 1.5516\n",
      "Epoch 2 Batch 1744 Loss 1.0387\n",
      "Epoch 2 Batch 1745 Loss 1.3702\n",
      "Epoch 2 Batch 1746 Loss 1.8786\n",
      "Epoch 2 Batch 1747 Loss 1.3677\n",
      "Epoch 2 Batch 1748 Loss 1.9501\n",
      "Epoch 2 Batch 1749 Loss 1.8854\n",
      "Epoch 2 Batch 1750 Loss 1.4524\n",
      "Epoch 2 Batch 1751 Loss 2.0367\n",
      "Epoch 2 Batch 1752 Loss 2.0116\n",
      "Epoch 2 Batch 1753 Loss 1.8282\n",
      "Epoch 2 Batch 1754 Loss 2.0542\n",
      "Epoch 2 Batch 1755 Loss 2.1317\n",
      "Epoch 2 Batch 1756 Loss 1.7077\n",
      "Epoch 2 Batch 1757 Loss 1.8717\n",
      "Epoch 2 Batch 1758 Loss 1.6774\n",
      "Epoch 2 Batch 1759 Loss 1.4525\n",
      "Epoch 2 Batch 1760 Loss 1.4755\n",
      "Epoch 2 Batch 1761 Loss 1.9209\n",
      "Epoch 2 Batch 1762 Loss 2.2098\n",
      "Epoch 2 Batch 1763 Loss 1.9311\n",
      "Epoch 2 Batch 1764 Loss 1.7513\n",
      "Epoch 2 Batch 1765 Loss 1.5310\n",
      "Epoch 2 Batch 1766 Loss 1.8363\n",
      "Epoch 2 Batch 1767 Loss 1.8284\n",
      "Epoch 2 Batch 1768 Loss 1.6804\n",
      "Epoch 2 Batch 1769 Loss 2.4442\n",
      "Epoch 2 Batch 1770 Loss 2.2149\n",
      "Epoch 2 Batch 1771 Loss 1.9155\n",
      "Epoch 2 Batch 1772 Loss 1.7773\n",
      "Epoch 2 Batch 1773 Loss 1.9069\n",
      "Epoch 2 Batch 1774 Loss 1.8316\n",
      "Epoch 2 Batch 1775 Loss 1.9335\n",
      "Epoch 2 Batch 1776 Loss 1.9030\n",
      "Epoch 2 Batch 1777 Loss 1.8826\n",
      "Epoch 2 Batch 1778 Loss 1.6683\n",
      "Epoch 2 Batch 1779 Loss 1.8948\n",
      "Epoch 2 Batch 1780 Loss 1.9678\n",
      "Epoch 2 Batch 1781 Loss 2.3561\n",
      "Epoch 2 Batch 1782 Loss 2.0311\n",
      "Epoch 2 Batch 1783 Loss 2.2581\n",
      "Epoch 2 Batch 1784 Loss 2.0508\n",
      "Epoch 2 Batch 1785 Loss 1.9528\n",
      "Epoch 2 Batch 1786 Loss 1.5756\n",
      "Epoch 2 Batch 1787 Loss 1.9597\n",
      "Epoch 2 Batch 1788 Loss 1.8626\n",
      "Epoch 2 Batch 1789 Loss 2.2805\n",
      "Epoch 2 Batch 1790 Loss 1.8658\n",
      "Epoch 2 Batch 1791 Loss 1.7874\n",
      "Epoch 2 Batch 1792 Loss 1.6384\n",
      "Epoch 2 Batch 1793 Loss 1.8264\n",
      "Epoch 2 Batch 1794 Loss 1.8406\n",
      "Epoch 2 Batch 1795 Loss 1.4749\n",
      "Epoch 2 Batch 1796 Loss 1.5498\n",
      "Epoch 2 Batch 1797 Loss 1.5789\n",
      "Epoch 2 Batch 1798 Loss 1.5758\n",
      "Epoch 2 Batch 1799 Loss 1.4831\n",
      "Epoch 2 Batch 1800 Loss 1.6760\n",
      "Epoch 2 Batch 1801 Loss 1.8656\n",
      "Epoch 2 Batch 1802 Loss 2.0704\n",
      "Epoch 2 Batch 1803 Loss 2.0867\n",
      "Epoch 2 Batch 1804 Loss 1.7658\n",
      "Epoch 2 Batch 1805 Loss 1.3986\n",
      "Epoch 2 Batch 1806 Loss 1.4631\n",
      "Epoch 2 Batch 1807 Loss 1.7218\n",
      "Epoch 2 Batch 1808 Loss 2.4773\n",
      "Epoch 2 Batch 1809 Loss 1.9398\n",
      "Epoch 2 Batch 1810 Loss 2.0546\n",
      "Epoch 2 Batch 1811 Loss 1.9881\n",
      "Epoch 2 Batch 1812 Loss 1.9790\n",
      "Epoch 2 Batch 1813 Loss 2.2607\n",
      "Epoch 2 Batch 1814 Loss 2.1730\n",
      "Epoch 2 Batch 1815 Loss 1.8324\n",
      "Epoch 2 Batch 1816 Loss 1.7195\n",
      "Epoch 2 Batch 1817 Loss 1.6554\n",
      "Epoch 2 Batch 1818 Loss 1.9796\n",
      "Epoch 2 Batch 1819 Loss 1.6656\n",
      "Epoch 2 Batch 1820 Loss 2.0706\n",
      "Epoch 2 Batch 1821 Loss 1.6870\n",
      "Epoch 2 Batch 1822 Loss 1.8213\n",
      "Epoch 2 Batch 1823 Loss 1.8252\n",
      "Epoch 2 Batch 1824 Loss 1.5236\n",
      "Epoch 2 Batch 1825 Loss 1.6939\n",
      "Epoch 2 Batch 1826 Loss 1.9212\n",
      "Epoch 2 Batch 1827 Loss 2.2312\n",
      "Epoch 2 Batch 1828 Loss 1.7812\n",
      "Epoch 2 Batch 1829 Loss 1.8023\n",
      "Epoch 2 Batch 1830 Loss 1.5568\n",
      "Epoch 2 Batch 1831 Loss 2.3776\n",
      "Epoch 2 Batch 1832 Loss 2.0765\n",
      "Epoch 2 Batch 1833 Loss 2.5278\n",
      "Epoch 2 Batch 1834 Loss 2.4308\n",
      "Epoch 2 Batch 1835 Loss 2.0476\n",
      "Epoch 2 Batch 1836 Loss 1.6825\n",
      "Epoch 2 Batch 1837 Loss 2.1974\n",
      "Epoch 2 Batch 1838 Loss 1.8738\n",
      "Epoch 2 Batch 1839 Loss 1.6275\n",
      "Epoch 2 Batch 1840 Loss 1.9187\n",
      "Epoch 2 Batch 1841 Loss 2.0094\n",
      "Epoch 2 Batch 1842 Loss 1.8693\n",
      "Epoch 2 Batch 1843 Loss 1.2652\n",
      "Epoch 2 Batch 1844 Loss 1.3625\n",
      "Epoch 2 Batch 1845 Loss 1.5423\n",
      "Epoch 2 Batch 1846 Loss 1.3106\n",
      "Epoch 2 Batch 1847 Loss 1.7122\n",
      "Epoch 2 Batch 1848 Loss 2.2143\n",
      "Epoch 2 Batch 1849 Loss 1.6146\n",
      "Epoch 2 Batch 1850 Loss 1.9811\n",
      "Epoch 2 Batch 1851 Loss 1.7030\n",
      "Epoch 2 Batch 1852 Loss 1.5483\n",
      "Epoch 2 Batch 1853 Loss 1.5735\n",
      "Epoch 2 Batch 1854 Loss 1.5836\n",
      "Epoch 2 Batch 1855 Loss 1.3389\n",
      "Epoch 2 Batch 1856 Loss 1.4761\n",
      "Epoch 2 Batch 1857 Loss 1.2957\n",
      "Epoch 2 Batch 1858 Loss 1.5530\n",
      "Epoch 2 Batch 1859 Loss 1.6400\n",
      "Epoch 2 Batch 1860 Loss 1.3924\n",
      "Epoch 2 Batch 1861 Loss 1.1671\n",
      "Epoch 2 Batch 1862 Loss 1.6432\n",
      "Epoch 2 Batch 1863 Loss 2.1258\n",
      "Epoch 2 Batch 1864 Loss 1.5437\n",
      "Epoch 2 Batch 1865 Loss 1.8508\n",
      "Epoch 2 Batch 1866 Loss 1.6887\n",
      "Epoch 2 Batch 1867 Loss 1.2584\n",
      "Epoch 2 Batch 1868 Loss 2.0968\n",
      "Epoch 2 Batch 1869 Loss 1.8712\n",
      "Epoch 2 Batch 1870 Loss 1.9791\n",
      "Epoch 2 Batch 1871 Loss 2.0925\n",
      "Epoch 2 Batch 1872 Loss 2.0463\n",
      "Epoch 2 Batch 1873 Loss 2.0378\n",
      "Epoch 2 Batch 1874 Loss 2.1337\n",
      "Epoch 2 Batch 1875 Loss 2.8009\n",
      "Epoch 2 Batch 1876 Loss 2.1351\n",
      "Epoch 2 Batch 1877 Loss 1.6302\n",
      "Epoch 2 Batch 1878 Loss 1.4240\n",
      "Epoch 2 Batch 1879 Loss 1.7491\n",
      "Epoch 2 Batch 1880 Loss 1.6843\n",
      "Epoch 2 Batch 1881 Loss 1.8410\n",
      "Epoch 2 Batch 1882 Loss 1.9201\n",
      "Epoch 2 Batch 1883 Loss 1.6823\n",
      "Epoch 2 Batch 1884 Loss 1.6007\n",
      "Epoch 2 Batch 1885 Loss 1.6560\n",
      "Epoch 2 Batch 1886 Loss 1.6689\n",
      "Epoch 2 Batch 1887 Loss 1.6240\n",
      "Epoch 2 Batch 1888 Loss 1.8074\n",
      "Epoch 2 Batch 1889 Loss 1.4107\n",
      "Epoch 2 Batch 1890 Loss 1.6047\n",
      "Epoch 2 Batch 1891 Loss 1.3070\n",
      "Epoch 2 Batch 1892 Loss 1.8091\n",
      "Epoch 2 Batch 1893 Loss 1.0666\n",
      "Epoch 2 Batch 1894 Loss 1.4601\n",
      "Epoch 2 Batch 1895 Loss 1.8507\n",
      "Epoch 2 Batch 1896 Loss 1.6402\n",
      "Epoch 2 Batch 1897 Loss 1.8647\n",
      "Epoch 2 Batch 1898 Loss 1.8510\n",
      "Epoch 2 Batch 1899 Loss 1.4608\n",
      "Epoch 2 Batch 1900 Loss 1.8102\n",
      "Epoch 2 Batch 1901 Loss 1.8210\n",
      "Epoch 2 Batch 1902 Loss 1.5652\n",
      "Epoch 2 Batch 1903 Loss 1.7327\n",
      "Epoch 2 Batch 1904 Loss 2.1765\n",
      "Epoch 2 Batch 1905 Loss 2.4394\n",
      "Epoch 2 Batch 1906 Loss 2.0075\n",
      "Epoch 2 Batch 1907 Loss 2.1440\n",
      "Epoch 2 Batch 1908 Loss 2.3585\n",
      "Epoch 2 Batch 1909 Loss 2.0648\n",
      "Epoch 2 Batch 1910 Loss 2.0295\n",
      "Epoch 2 Batch 1911 Loss 1.9256\n",
      "Epoch 2 Batch 1912 Loss 2.2063\n",
      "Epoch 2 Batch 1913 Loss 1.6962\n",
      "Epoch 2 Batch 1914 Loss 2.2606\n",
      "Epoch 2 Batch 1915 Loss 2.0483\n",
      "Epoch 2 Batch 1916 Loss 1.9632\n",
      "Epoch 2 Batch 1917 Loss 1.8562\n",
      "Epoch 2 Batch 1918 Loss 1.5821\n",
      "Epoch 2 Batch 1919 Loss 1.4227\n",
      "Epoch 2 Batch 1920 Loss 1.4159\n",
      "Epoch 2 Batch 1921 Loss 1.7192\n",
      "Epoch 2 Batch 1922 Loss 1.9253\n",
      "Epoch 2 Batch 1923 Loss 1.8280\n",
      "Epoch 2 Batch 1924 Loss 2.0244\n",
      "Epoch 2 Batch 1925 Loss 1.8882\n",
      "Epoch 2 Batch 1926 Loss 1.9120\n",
      "Epoch 2 Batch 1927 Loss 1.7779\n",
      "Epoch 2 Batch 1928 Loss 2.1373\n",
      "Epoch 2 Batch 1929 Loss 1.3709\n",
      "Epoch 2 Batch 1930 Loss 1.5151\n",
      "Epoch 2 Batch 1931 Loss 1.5728\n",
      "Epoch 2 Batch 1932 Loss 1.9249\n",
      "Epoch 2 Batch 1933 Loss 2.1777\n",
      "Epoch 2 Batch 1934 Loss 1.7156\n",
      "Epoch 2 Batch 1935 Loss 1.9150\n",
      "Epoch 2 Batch 1936 Loss 2.0307\n",
      "Epoch 2 Batch 1937 Loss 1.7209\n",
      "Epoch 2 Batch 1938 Loss 2.0845\n",
      "Epoch 2 Batch 1939 Loss 1.5580\n",
      "Epoch 2 Batch 1940 Loss 2.0146\n",
      "Epoch 2 Batch 1941 Loss 2.2553\n",
      "Epoch 2 Batch 1942 Loss 1.6316\n",
      "Epoch 2 Batch 1943 Loss 1.8466\n",
      "Epoch 2 Batch 1944 Loss 1.6661\n",
      "Epoch 2 Batch 1945 Loss 1.5819\n",
      "Epoch 2 Batch 1946 Loss 2.3897\n",
      "Epoch 2 Batch 1947 Loss 2.1362\n",
      "Epoch 2 Batch 1948 Loss 1.6838\n",
      "Epoch 2 Batch 1949 Loss 1.9748\n",
      "Epoch 2 Batch 1950 Loss 1.5135\n",
      "Epoch 2 Batch 1951 Loss 1.8916\n",
      "Epoch 2 Batch 1952 Loss 1.4973\n",
      "Epoch 2 Batch 1953 Loss 1.4644\n",
      "Epoch 2 Batch 1954 Loss 1.8846\n",
      "Epoch 2 Batch 1955 Loss 1.9104\n",
      "Epoch 2 Batch 1956 Loss 2.1297\n",
      "Epoch 2 Batch 1957 Loss 1.9195\n",
      "Epoch 2 Batch 1958 Loss 2.6300\n",
      "Epoch 2 Batch 1959 Loss 2.1840\n",
      "Epoch 2 Batch 1960 Loss 1.8053\n",
      "Epoch 2 Batch 1961 Loss 1.7625\n",
      "Epoch 2 Batch 1962 Loss 1.7568\n",
      "Epoch 2 Batch 1963 Loss 2.4592\n",
      "Epoch 2 Batch 1964 Loss 1.5805\n",
      "Epoch 2 Batch 1965 Loss 1.4793\n",
      "Epoch 2 Batch 1966 Loss 1.7104\n",
      "Epoch 2 Batch 1967 Loss 1.5971\n",
      "Epoch 2 Batch 1968 Loss 1.9287\n",
      "Epoch 2 Batch 1969 Loss 1.9612\n",
      "Epoch 2 Batch 1970 Loss 2.1339\n",
      "Epoch 2 Batch 1971 Loss 1.9147\n",
      "Epoch 2 Batch 1972 Loss 2.4171\n",
      "Epoch 2 Batch 1973 Loss 2.0574\n",
      "Epoch 2 Batch 1974 Loss 1.5331\n",
      "Epoch 2 Batch 1975 Loss 1.7879\n",
      "Epoch 2 Batch 1976 Loss 2.0555\n",
      "Epoch 2 Batch 1977 Loss 1.6556\n",
      "Epoch 2 Batch 1978 Loss 1.8821\n",
      "Epoch 2 Batch 1979 Loss 1.2835\n",
      "Epoch 2 Batch 1980 Loss 1.2890\n",
      "Epoch 2 Batch 1981 Loss 1.8468\n",
      "Epoch 2 Batch 1982 Loss 1.6821\n",
      "Epoch 2 Batch 1983 Loss 1.7867\n",
      "Epoch 2 Batch 1984 Loss 1.5173\n",
      "Epoch 2 Batch 1985 Loss 1.9797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Batch 1986 Loss 2.0247\n",
      "Epoch 2 Batch 1987 Loss 2.0269\n",
      "Epoch 2 Batch 1988 Loss 1.5830\n",
      "Epoch 2 Batch 1989 Loss 1.7218\n",
      "Epoch 2 Batch 1990 Loss 1.4834\n",
      "Epoch 2 Batch 1991 Loss 1.6328\n",
      "Epoch 2 Batch 1992 Loss 1.5552\n",
      "Epoch 2 Batch 1993 Loss 1.5965\n",
      "Epoch 2 Batch 1994 Loss 1.8346\n",
      "Epoch 2 Batch 1995 Loss 1.6753\n",
      "Epoch 2 Batch 1996 Loss 1.7879\n",
      "Epoch 2 Batch 1997 Loss 1.5889\n",
      "Epoch 2 Batch 1998 Loss 1.7383\n",
      "Epoch 2 Batch 1999 Loss 1.7541\n",
      "Epoch 2 Batch 2000 Loss 1.9733\n",
      "Epoch 2 Batch 2001 Loss 1.7354\n",
      "Epoch 2 Batch 2002 Loss 2.3654\n",
      "Epoch 2 Batch 2003 Loss 2.0196\n",
      "Epoch 2 Batch 2004 Loss 2.1286\n",
      "Epoch 2 Batch 2005 Loss 1.7934\n",
      "Epoch 2 Batch 2006 Loss 2.0520\n",
      "Epoch 2 Batch 2007 Loss 2.1426\n",
      "Epoch 2 Batch 2008 Loss 1.4881\n",
      "Epoch 2 Batch 2009 Loss 2.1750\n",
      "Epoch 2 Batch 2010 Loss 2.2725\n",
      "Epoch 2 Batch 2011 Loss 2.0345\n",
      "Epoch 2 Batch 2012 Loss 1.8992\n",
      "Epoch 2 Batch 2013 Loss 2.1203\n",
      "Epoch 2 Batch 2014 Loss 1.4842\n",
      "Epoch 2 Batch 2015 Loss 1.6385\n",
      "Epoch 2 Batch 2016 Loss 1.3000\n",
      "Epoch 2 Batch 2017 Loss 1.8198\n",
      "Epoch 2 Batch 2018 Loss 1.7330\n",
      "Epoch 2 Batch 2019 Loss 2.0646\n",
      "Epoch 2 Batch 2020 Loss 1.8341\n",
      "Epoch 2 Batch 2021 Loss 2.0290\n",
      "Epoch 2 Batch 2022 Loss 1.8612\n",
      "Epoch 2 Batch 2023 Loss 1.5517\n",
      "Epoch 2 Batch 2024 Loss 2.0238\n",
      "Epoch 2 Batch 2025 Loss 1.6487\n",
      "Epoch 2 Batch 2026 Loss 1.2199\n",
      "Epoch 2 Batch 2027 Loss 1.4984\n",
      "Epoch 2 Batch 2028 Loss 1.4223\n",
      "Epoch 2 Batch 2029 Loss 1.8147\n",
      "Epoch 2 Batch 2030 Loss 1.8826\n",
      "Epoch 2 Batch 2031 Loss 1.8131\n",
      "Epoch 2 Batch 2032 Loss 1.5912\n",
      "Epoch 2 Batch 2033 Loss 1.5906\n",
      "Epoch 2 Batch 2034 Loss 1.6406\n",
      "Epoch 2 Batch 2035 Loss 1.9590\n",
      "Epoch 2 Batch 2036 Loss 1.6741\n",
      "Epoch 2 Batch 2037 Loss 1.9456\n",
      "Epoch 2 Batch 2038 Loss 1.5732\n",
      "Epoch 2 Batch 2039 Loss 1.6502\n",
      "Epoch 2 Batch 2040 Loss 1.8238\n",
      "Epoch 2 Batch 2041 Loss 1.5901\n",
      "Epoch 2 Batch 2042 Loss 1.7608\n",
      "Epoch 2 Batch 2043 Loss 1.9688\n",
      "Epoch 2 Batch 2044 Loss 1.9585\n",
      "Epoch 2 Batch 2045 Loss 1.8563\n",
      "Epoch 2 Batch 2046 Loss 2.1608\n",
      "Epoch 2 Batch 2047 Loss 1.6151\n",
      "Epoch 2 Batch 2048 Loss 1.5303\n",
      "Epoch 2 Batch 2049 Loss 1.2498\n",
      "Epoch 2 Batch 2050 Loss 1.4993\n",
      "Epoch 2 Batch 2051 Loss 1.7368\n",
      "Epoch 2 Batch 2052 Loss 1.9172\n",
      "Epoch 2 Batch 2053 Loss 2.0030\n",
      "Epoch 2 Batch 2054 Loss 1.9283\n",
      "Epoch 2 Batch 2055 Loss 1.7339\n",
      "Epoch 2 Batch 2056 Loss 1.7807\n",
      "Epoch 2 Batch 2057 Loss 1.7251\n",
      "Epoch 2 Batch 2058 Loss 1.7103\n",
      "Epoch 2 Batch 2059 Loss 1.7791\n",
      "Epoch 2 Batch 2060 Loss 2.0681\n",
      "Epoch 2 Batch 2061 Loss 1.7290\n",
      "Epoch 2 Batch 2062 Loss 2.1146\n",
      "Epoch 2 Batch 2063 Loss 2.2549\n",
      "Epoch 2 Batch 2064 Loss 2.0815\n",
      "Epoch 2 Batch 2065 Loss 1.7157\n",
      "Epoch 2 Batch 2066 Loss 1.8675\n",
      "Epoch 2 Batch 2067 Loss 1.7928\n",
      "Epoch 2 Batch 2068 Loss 2.2414\n",
      "Epoch 2 Batch 2069 Loss 2.1623\n",
      "Epoch 2 Batch 2070 Loss 1.8138\n",
      "Epoch 2 Batch 2071 Loss 1.4685\n",
      "Epoch 2 Batch 2072 Loss 1.9210\n",
      "Epoch 2 Batch 2073 Loss 1.9440\n",
      "Epoch 2 Batch 2074 Loss 2.2558\n",
      "Epoch 2 Batch 2075 Loss 2.2429\n",
      "Epoch 2 Batch 2076 Loss 1.9708\n",
      "Epoch 2 Batch 2077 Loss 1.8699\n",
      "Epoch 2 Batch 2078 Loss 1.6530\n",
      "Epoch 2 Batch 2079 Loss 2.1582\n",
      "Epoch 2 Batch 2080 Loss 1.5929\n",
      "Epoch 2 Batch 2081 Loss 2.0031\n",
      "Epoch 2 Batch 2082 Loss 1.5503\n",
      "Epoch 2 Batch 2083 Loss 1.6529\n",
      "Epoch 2 Batch 2084 Loss 1.8103\n",
      "Epoch 2 Batch 2085 Loss 1.6740\n",
      "Epoch 2 Batch 2086 Loss 1.4306\n",
      "Epoch 2 Batch 2087 Loss 1.5835\n",
      "Epoch 2 Batch 2088 Loss 2.0918\n",
      "Epoch 2 Batch 2089 Loss 1.8830\n",
      "Epoch 2 Batch 2090 Loss 1.3738\n",
      "Epoch 2 Batch 2091 Loss 1.7085\n",
      "Epoch 2 Batch 2092 Loss 2.2111\n",
      "Epoch 2 Batch 2093 Loss 2.4864\n",
      "Epoch 2 Batch 2094 Loss 2.2879\n",
      "Epoch 2 Batch 2095 Loss 1.7575\n",
      "Epoch 2 Batch 2096 Loss 1.9868\n",
      "Epoch 2 Batch 2097 Loss 2.2485\n",
      "Epoch 2 Batch 2098 Loss 2.2219\n",
      "Epoch 2 Batch 2099 Loss 1.7096\n",
      "Epoch 2 Batch 2100 Loss 2.0297\n",
      "Epoch 2 Batch 2101 Loss 1.7807\n",
      "Epoch 2 Batch 2102 Loss 1.8359\n",
      "Epoch 2 Batch 2103 Loss 1.7163\n",
      "Epoch 2 Batch 2104 Loss 2.0353\n",
      "Epoch 2 Batch 2105 Loss 2.1029\n",
      "Epoch 2 Batch 2106 Loss 2.2214\n",
      "Epoch 2 Batch 2107 Loss 2.3318\n",
      "Epoch 2 Batch 2108 Loss 1.9894\n",
      "Epoch 2 Batch 2109 Loss 1.4669\n",
      "Epoch 2 Batch 2110 Loss 1.4044\n",
      "Epoch 2 Batch 2111 Loss 1.9091\n",
      "Epoch 2 Batch 2112 Loss 1.5907\n",
      "Epoch 2 Batch 2113 Loss 1.8563\n",
      "Epoch 2 Batch 2114 Loss 1.3510\n",
      "Epoch 2 Batch 2115 Loss 1.7365\n",
      "Epoch 2 Batch 2116 Loss 1.6528\n",
      "Epoch 2 Batch 2117 Loss 1.4090\n",
      "Epoch 2 Batch 2118 Loss 1.9296\n",
      "Epoch 2 Batch 2119 Loss 1.6760\n",
      "Epoch 2 Batch 2120 Loss 1.6776\n",
      "Epoch 2 Batch 2121 Loss 1.7126\n",
      "Epoch 2 Batch 2122 Loss 1.8303\n",
      "Epoch 2 Batch 2123 Loss 1.7691\n",
      "Epoch 2 Batch 2124 Loss 2.0026\n",
      "Epoch 2 Batch 2125 Loss 2.1467\n",
      "Epoch 2 Batch 2126 Loss 2.5109\n",
      "Epoch 2 Batch 2127 Loss 2.3941\n",
      "Epoch 2 Batch 2128 Loss 2.5913\n",
      "Epoch 2 Batch 2129 Loss 2.1635\n",
      "Epoch 2 Batch 2130 Loss 2.2763\n",
      "Epoch 2 Batch 2131 Loss 2.3017\n",
      "Epoch 2 Batch 2132 Loss 1.6805\n",
      "Epoch 2 Batch 2133 Loss 1.6600\n",
      "Epoch 2 Batch 2134 Loss 1.7387\n",
      "Epoch 2 Batch 2135 Loss 2.0925\n",
      "Epoch 2 Batch 2136 Loss 1.3064\n",
      "Epoch 2 Batch 2137 Loss 1.8121\n",
      "Epoch 2 Batch 2138 Loss 1.9055\n",
      "Epoch 2 Batch 2139 Loss 2.0619\n",
      "Epoch 2 Batch 2140 Loss 2.0846\n",
      "Epoch 2 Batch 2141 Loss 2.8236\n",
      "Epoch 2 Batch 2142 Loss 2.2232\n",
      "Epoch 2 Batch 2143 Loss 2.1747\n",
      "Epoch 2 Batch 2144 Loss 1.6967\n",
      "Epoch 2 Batch 2145 Loss 1.9670\n",
      "Epoch 2 Batch 2146 Loss 1.9938\n",
      "Epoch 2 Batch 2147 Loss 1.6690\n",
      "Epoch 2 Batch 2148 Loss 1.6423\n",
      "Epoch 2 Batch 2149 Loss 2.0501\n",
      "Epoch 2 Batch 2150 Loss 1.8758\n",
      "Epoch 2 Batch 2151 Loss 2.4840\n",
      "Epoch 2 Batch 2152 Loss 2.0770\n",
      "Epoch 2 Batch 2153 Loss 1.8118\n",
      "Epoch 2 Batch 2154 Loss 2.6888\n",
      "Epoch 2 Batch 2155 Loss 2.6621\n",
      "Epoch 2 Batch 2156 Loss 1.9740\n",
      "Epoch 2 Batch 2157 Loss 2.3004\n",
      "Epoch 2 Batch 2158 Loss 2.1098\n",
      "Epoch 2 Batch 2159 Loss 1.8408\n",
      "Epoch 2 Batch 2160 Loss 2.1247\n",
      "Epoch 2 Batch 2161 Loss 1.6608\n",
      "Epoch 2 Batch 2162 Loss 1.8340\n",
      "Epoch 2 Batch 2163 Loss 1.7949\n",
      "Epoch 2 Batch 2164 Loss 2.2591\n",
      "Epoch 2 Batch 2165 Loss 2.3172\n",
      "Epoch 2 Batch 2166 Loss 1.6832\n",
      "Epoch 2 Batch 2167 Loss 1.5088\n",
      "Epoch 2 Batch 2168 Loss 1.8745\n",
      "Epoch 2 Batch 2169 Loss 1.7673\n",
      "Epoch 2 Batch 2170 Loss 1.8001\n",
      "Epoch 2 Batch 2171 Loss 1.6360\n",
      "Epoch 2 Batch 2172 Loss 1.6299\n",
      "Epoch 2 Batch 2173 Loss 1.5307\n",
      "Epoch 2 Batch 2174 Loss 1.3460\n",
      "Epoch 2 Batch 2175 Loss 1.4975\n",
      "Epoch 2 Batch 2176 Loss 2.1820\n",
      "Epoch 2 Batch 2177 Loss 2.1449\n",
      "Epoch 2 Batch 2178 Loss 1.6576\n",
      "Epoch 2 Batch 2179 Loss 2.1051\n",
      "Epoch 2 Batch 2180 Loss 2.1365\n",
      "Epoch 2 Batch 2181 Loss 1.6420\n",
      "Epoch 2 Batch 2182 Loss 1.7814\n",
      "Epoch 2 Batch 2183 Loss 2.0859\n",
      "Epoch 2 Batch 2184 Loss 1.6293\n",
      "Epoch 2 Batch 2185 Loss 2.0992\n",
      "Epoch 2 Batch 2186 Loss 1.4499\n",
      "Epoch 2 Batch 2187 Loss 1.9570\n",
      "Epoch 2 Batch 2188 Loss 1.6688\n",
      "Epoch 2 Batch 2189 Loss 1.8224\n",
      "Epoch 2 Batch 2190 Loss 2.0078\n",
      "Epoch 2 Batch 2191 Loss 1.8826\n",
      "Epoch 2 Batch 2192 Loss 1.6795\n",
      "Epoch 2 Batch 2193 Loss 1.8924\n",
      "Epoch 2 Batch 2194 Loss 1.6752\n",
      "Epoch 2 Batch 2195 Loss 1.7055\n",
      "Epoch 2 Batch 2196 Loss 1.5468\n",
      "Epoch 2 Batch 2197 Loss 1.4986\n",
      "Epoch 2 Batch 2198 Loss 1.4268\n",
      "Epoch 2 Batch 2199 Loss 1.5653\n",
      "Epoch 2 Batch 2200 Loss 1.5140\n",
      "Epoch 2 Batch 2201 Loss 1.4882\n",
      "Epoch 2 Batch 2202 Loss 1.7359\n",
      "Epoch 2 Batch 2203 Loss 1.8203\n",
      "Epoch 2 Batch 2204 Loss 1.5266\n",
      "Epoch 2 Batch 2205 Loss 1.7707\n",
      "Epoch 2 Batch 2206 Loss 1.7047\n",
      "Epoch 2 Batch 2207 Loss 1.7329\n",
      "Epoch 2 Batch 2208 Loss 1.7639\n",
      "Epoch 2 Batch 2209 Loss 1.6373\n",
      "Epoch 2 Batch 2210 Loss 2.5082\n",
      "Epoch 2 Batch 2211 Loss 2.2536\n",
      "Epoch 2 Batch 2212 Loss 2.1369\n",
      "Epoch 2 Batch 2213 Loss 2.1441\n",
      "Epoch 2 Batch 2214 Loss 2.0295\n",
      "Epoch 2 Batch 2215 Loss 2.0645\n",
      "Epoch 2 Batch 2216 Loss 1.8767\n",
      "Epoch 2 Batch 2217 Loss 1.9538\n",
      "Epoch 2 Batch 2218 Loss 1.4037\n",
      "Epoch 2 Batch 2219 Loss 2.2888\n",
      "Epoch 2 Batch 2220 Loss 1.5249\n",
      "Epoch 2 Batch 2221 Loss 1.2265\n",
      "Epoch 2 Batch 2222 Loss 1.4688\n",
      "Epoch 2 Batch 2223 Loss 1.4620\n",
      "Epoch 2 Batch 2224 Loss 1.8341\n",
      "Epoch 2 Batch 2225 Loss 1.7392\n",
      "Epoch 2 Batch 2226 Loss 1.9484\n",
      "Epoch 2 Batch 2227 Loss 1.6824\n",
      "Epoch 2 Batch 2228 Loss 2.2857\n",
      "Epoch 2 Batch 2229 Loss 1.1634\n",
      "Epoch 2 Batch 2230 Loss 1.6812\n",
      "Epoch 2 Batch 2231 Loss 1.7751\n",
      "Epoch 2 Batch 2232 Loss 1.2720\n",
      "Epoch 2 Batch 2233 Loss 1.3652\n",
      "Epoch 2 Batch 2234 Loss 2.1786\n",
      "Epoch 2 Batch 2235 Loss 1.7235\n",
      "Epoch 2 Batch 2236 Loss 2.3423\n",
      "Epoch 2 Batch 2237 Loss 2.2462\n",
      "Epoch 2 Batch 2238 Loss 2.0323\n",
      "Epoch 2 Batch 2239 Loss 1.8045\n",
      "Epoch 2 Batch 2240 Loss 1.9952\n",
      "Epoch 2 Batch 2241 Loss 1.3415\n",
      "Epoch 2 Batch 2242 Loss 2.0415\n",
      "Epoch 2 Batch 2243 Loss 1.7806\n",
      "Epoch 2 Batch 2244 Loss 1.5108\n",
      "Epoch 2 Batch 2245 Loss 1.1677\n",
      "Epoch 2 Batch 2246 Loss 1.3802\n",
      "Epoch 2 Batch 2247 Loss 1.4965\n",
      "Epoch 2 Batch 2248 Loss 1.9551\n",
      "Epoch 2 Batch 2249 Loss 1.3851\n",
      "Epoch 2 Batch 2250 Loss 1.5854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Batch 2251 Loss 1.6549\n",
      "Epoch 2 Batch 2252 Loss 1.3086\n",
      "Epoch 2 Batch 2253 Loss 1.9197\n",
      "Epoch 2 Batch 2254 Loss 1.4056\n",
      "Epoch 2 Batch 2255 Loss 1.4351\n",
      "Epoch 2 Batch 2256 Loss 1.8914\n",
      "Epoch 2 Batch 2257 Loss 1.7419\n",
      "Epoch 2 Batch 2258 Loss 1.7202\n",
      "Epoch 2 Batch 2259 Loss 1.7413\n",
      "Epoch 2 Batch 2260 Loss 1.6415\n",
      "Epoch 2 Batch 2261 Loss 1.8891\n",
      "Epoch 2 Batch 2262 Loss 1.3178\n",
      "Epoch 2 Batch 2263 Loss 1.7888\n",
      "Epoch 2 Batch 2264 Loss 1.4017\n",
      "Epoch 2 Batch 2265 Loss 1.5952\n",
      "Epoch 2 Batch 2266 Loss 1.4479\n",
      "Epoch 2 Batch 2267 Loss 1.6588\n",
      "Epoch 2 Batch 2268 Loss 1.7328\n",
      "Epoch 2 Batch 2269 Loss 1.2187\n",
      "Epoch 2 Batch 2270 Loss 1.6975\n",
      "Epoch 2 Batch 2271 Loss 1.6195\n",
      "Epoch 2 Batch 2272 Loss 1.5666\n",
      "Epoch 2 Batch 2273 Loss 1.7706\n",
      "Epoch 2 Batch 2274 Loss 1.6348\n",
      "Epoch 2 Batch 2275 Loss 1.7446\n",
      "Epoch 2 Batch 2276 Loss 2.1587\n",
      "Epoch 2 Batch 2277 Loss 1.7463\n",
      "Epoch 2 Batch 2278 Loss 1.6967\n",
      "Epoch 2 Batch 2279 Loss 1.4270\n",
      "Epoch 2 Batch 2280 Loss 1.3905\n",
      "Epoch 2 Batch 2281 Loss 1.5818\n",
      "Epoch 2 Batch 2282 Loss 1.3984\n",
      "Epoch 2 Batch 2283 Loss 1.4521\n",
      "Epoch 2 Batch 2284 Loss 1.6125\n",
      "Epoch 2 Batch 2285 Loss 1.5795\n",
      "Epoch 2 Batch 2286 Loss 1.3337\n",
      "Epoch 2 Batch 2287 Loss 1.3090\n",
      "Epoch 2 Batch 2288 Loss 1.7771\n",
      "Epoch 2 Batch 2289 Loss 1.5341\n",
      "Epoch 2 Batch 2290 Loss 2.0183\n",
      "Epoch 2 Batch 2291 Loss 1.8852\n",
      "Epoch 2 Batch 2292 Loss 1.5169\n",
      "Epoch 2 Batch 2293 Loss 1.5126\n",
      "Epoch 2 Batch 2294 Loss 1.7984\n",
      "Epoch 2 Batch 2295 Loss 1.8382\n",
      "Epoch 2 Batch 2296 Loss 2.3468\n",
      "Epoch 2 Batch 2297 Loss 1.8608\n",
      "Epoch 2 Batch 2298 Loss 1.3141\n",
      "Epoch 2 Batch 2299 Loss 1.5345\n",
      "Epoch 2 Batch 2300 Loss 1.6329\n",
      "Epoch 2 Batch 2301 Loss 1.4419\n",
      "Epoch 2 Batch 2302 Loss 1.9376\n",
      "Epoch 2 Batch 2303 Loss 1.7279\n",
      "Epoch 2 Batch 2304 Loss 1.8660\n",
      "Epoch 2 Batch 2305 Loss 1.6762\n",
      "Epoch 2 Batch 2306 Loss 1.9605\n",
      "Epoch 2 Batch 2307 Loss 1.7476\n",
      "Epoch 2 Batch 2308 Loss 1.7131\n",
      "Epoch 2 Batch 2309 Loss 1.5475\n",
      "Epoch 2 Batch 2310 Loss 1.5063\n",
      "Epoch 2 Batch 2311 Loss 1.4538\n",
      "Epoch 2 Batch 2312 Loss 1.6977\n",
      "Epoch 2 Batch 2313 Loss 1.7609\n",
      "Epoch 2 Batch 2314 Loss 1.9241\n",
      "Epoch 2 Batch 2315 Loss 1.9250\n",
      "Epoch 2 Batch 2316 Loss 1.6917\n",
      "Epoch 2 Batch 2317 Loss 2.1601\n",
      "Epoch 2 Batch 2318 Loss 1.8391\n",
      "Epoch 2 Batch 2319 Loss 1.6153\n",
      "Epoch 2 Batch 2320 Loss 2.2520\n",
      "Epoch 2 Batch 2321 Loss 1.3953\n",
      "Epoch 2 Batch 2322 Loss 1.5258\n",
      "Epoch 2 Batch 2323 Loss 1.8550\n",
      "Epoch 2 Batch 2324 Loss 1.6188\n",
      "Epoch 2 Batch 2325 Loss 1.7126\n",
      "Epoch 2 Batch 2326 Loss 1.7152\n",
      "Epoch 2 Batch 2327 Loss 1.4839\n",
      "Epoch 2 Batch 2328 Loss 1.4980\n",
      "Epoch 2 Batch 2329 Loss 1.3325\n",
      "Epoch 2 Batch 2330 Loss 1.3313\n",
      "Epoch 2 Batch 2331 Loss 1.3594\n",
      "Epoch 2 Batch 2332 Loss 1.1637\n",
      "Epoch 2 Batch 2333 Loss 0.9797\n",
      "Epoch 2 Batch 2334 Loss 1.2690\n",
      "Epoch 2 Batch 2335 Loss 1.5590\n",
      "Epoch 2 Batch 2336 Loss 1.3993\n",
      "Epoch 2 Batch 2337 Loss 1.5668\n",
      "Epoch 2 Batch 2338 Loss 1.4304\n",
      "Epoch 2 Batch 2339 Loss 1.4395\n",
      "Epoch 2 Batch 2340 Loss 1.7327\n",
      "Epoch 2 Batch 2341 Loss 1.3519\n",
      "Epoch 2 Batch 2342 Loss 1.5233\n",
      "Epoch 2 Batch 2343 Loss 1.7632\n",
      "Epoch 2 Batch 2344 Loss 1.7007\n",
      "Epoch 2 Batch 2345 Loss 1.8250\n",
      "Epoch 2 Batch 2346 Loss 1.7303\n",
      "Epoch 2 Batch 2347 Loss 1.5536\n",
      "Epoch 2 Batch 2348 Loss 1.4378\n",
      "Epoch 2 Batch 2349 Loss 1.7115\n",
      "Epoch 2 Batch 2350 Loss 1.5730\n",
      "Epoch 2 Batch 2351 Loss 2.0954\n",
      "Epoch 2 Batch 2352 Loss 1.6631\n",
      "Epoch 2 Batch 2353 Loss 1.6101\n",
      "Epoch 2 Batch 2354 Loss 1.2472\n",
      "Epoch 2 Batch 2355 Loss 1.7627\n",
      "Epoch 2 Batch 2356 Loss 1.2641\n",
      "Epoch 2 Batch 2357 Loss 1.2027\n",
      "Epoch 2 Batch 2358 Loss 1.6320\n",
      "Epoch 2 Batch 2359 Loss 1.5543\n",
      "Epoch 2 Batch 2360 Loss 1.3604\n",
      "Epoch 2 Batch 2361 Loss 1.0903\n",
      "Epoch 2 Batch 2362 Loss 1.6943\n",
      "Epoch 2 Batch 2363 Loss 1.8915\n",
      "Epoch 2 Batch 2364 Loss 1.7608\n",
      "Epoch 2 Batch 2365 Loss 1.7640\n",
      "Epoch 2 Batch 2366 Loss 1.5085\n",
      "Epoch 2 Batch 2367 Loss 1.3210\n",
      "Epoch 2 Batch 2368 Loss 1.3342\n",
      "Epoch 2 Batch 2369 Loss 1.5237\n",
      "Epoch 2 Batch 2370 Loss 1.6087\n",
      "Epoch 2 Batch 2371 Loss 1.1398\n",
      "Epoch 2 Batch 2372 Loss 1.2712\n",
      "Epoch 2 Batch 2373 Loss 1.4626\n",
      "Epoch 2 Batch 2374 Loss 1.0093\n",
      "Epoch 2 Batch 2375 Loss 1.2553\n",
      "Epoch 2 Batch 2376 Loss 1.4138\n",
      "Epoch 2 Batch 2377 Loss 1.2113\n",
      "Epoch 2 Batch 2378 Loss 1.6906\n",
      "Epoch 2 Batch 2379 Loss 1.6187\n",
      "Epoch 2 Batch 2380 Loss 1.4259\n",
      "Epoch 2 Batch 2381 Loss 1.6250\n",
      "Epoch 2 Batch 2382 Loss 1.7777\n",
      "Epoch 2 Batch 2383 Loss 1.9932\n",
      "Epoch 2 Batch 2384 Loss 1.7670\n",
      "Epoch 2 Batch 2385 Loss 1.7222\n",
      "Epoch 2 Batch 2386 Loss 1.7655\n",
      "Epoch 2 Batch 2387 Loss 1.6890\n",
      "Epoch 2 Batch 2388 Loss 1.7698\n",
      "Epoch 2 Batch 2389 Loss 2.0090\n",
      "Epoch 2 Batch 2390 Loss 1.7640\n",
      "Epoch 2 Batch 2391 Loss 1.7012\n",
      "Epoch 2 Batch 2392 Loss 1.4399\n",
      "Epoch 2 Batch 2393 Loss 1.4492\n",
      "Epoch 2 Batch 2394 Loss 1.3412\n",
      "Epoch 2 Batch 2395 Loss 1.6969\n",
      "Epoch 2 Batch 2396 Loss 1.8631\n",
      "Epoch 2 Batch 2397 Loss 1.5334\n",
      "Epoch 2 Batch 2398 Loss 1.5515\n",
      "Epoch 2 Batch 2399 Loss 1.1478\n",
      "Epoch 2 Batch 2400 Loss 1.2225\n",
      "Epoch 2 Batch 2401 Loss 1.5363\n",
      "Epoch 2 Batch 2402 Loss 1.2980\n",
      "Epoch 2 Batch 2403 Loss 1.5622\n",
      "Epoch 2 Batch 2404 Loss 2.0827\n",
      "Epoch 2 Batch 2405 Loss 1.6889\n",
      "Epoch 2 Batch 2406 Loss 1.5769\n",
      "Epoch 2 Batch 2407 Loss 1.7900\n",
      "Epoch 2 Batch 2408 Loss 2.0855\n",
      "Epoch 2 Batch 2409 Loss 1.7925\n",
      "Epoch 2 Batch 2410 Loss 1.4553\n",
      "Epoch 2 Batch 2411 Loss 1.3737\n",
      "Epoch 2 Batch 2412 Loss 1.4094\n",
      "Epoch 2 Batch 2413 Loss 1.4539\n",
      "Epoch 2 Batch 2414 Loss 1.5164\n",
      "Epoch 2 Batch 2415 Loss 1.7111\n",
      "Epoch 2 Batch 2416 Loss 1.5091\n",
      "Epoch 2 Batch 2417 Loss 1.5813\n",
      "Epoch 2 Batch 2418 Loss 1.4630\n",
      "Epoch 2 Batch 2419 Loss 1.0638\n",
      "Epoch 2 Batch 2420 Loss 1.3609\n",
      "Epoch 2 Batch 2421 Loss 1.1537\n",
      "Epoch 2 Batch 2422 Loss 1.2528\n",
      "Epoch 2 Batch 2423 Loss 1.7958\n",
      "Epoch 2 Batch 2424 Loss 1.3799\n",
      "Epoch 2 Batch 2425 Loss 1.5378\n",
      "Epoch 2 Batch 2426 Loss 2.1643\n",
      "Epoch 2 Batch 2427 Loss 1.9798\n",
      "Epoch 2 Batch 2428 Loss 1.0328\n",
      "Epoch 2 Batch 2429 Loss 1.2178\n",
      "Epoch 2 Batch 2430 Loss 1.5282\n",
      "Epoch 2 Batch 2431 Loss 1.5388\n",
      "Epoch 2 Batch 2432 Loss 1.2169\n",
      "Epoch 2 Batch 2433 Loss 1.4765\n",
      "Epoch 2 Batch 2434 Loss 1.5750\n",
      "Epoch 2 Batch 2435 Loss 1.2092\n",
      "Epoch 2 Batch 2436 Loss 1.3126\n",
      "Epoch 2 Batch 2437 Loss 1.5215\n",
      "Epoch 2 Batch 2438 Loss 1.4699\n",
      "Epoch 2 Batch 2439 Loss 1.3936\n",
      "Epoch 2 Batch 2440 Loss 1.2406\n",
      "Epoch 2 Batch 2441 Loss 1.3218\n",
      "Epoch 2 Batch 2442 Loss 1.1974\n",
      "Epoch 2 Batch 2443 Loss 1.0142\n",
      "Epoch 2 Batch 2444 Loss 1.3897\n",
      "Epoch 2 Batch 2445 Loss 1.3977\n",
      "Epoch 2 Batch 2446 Loss 1.5263\n",
      "Epoch 2 Batch 2447 Loss 1.5105\n",
      "Epoch 2 Batch 2448 Loss 1.2492\n",
      "Epoch 2 Batch 2449 Loss 1.5007\n",
      "Epoch 2 Batch 2450 Loss 1.6719\n",
      "Epoch 2 Batch 2451 Loss 2.0867\n",
      "Epoch 2 Batch 2452 Loss 1.9079\n",
      "Epoch 2 Batch 2453 Loss 2.2754\n",
      "Epoch 2 Batch 2454 Loss 1.5195\n",
      "Epoch 2 Batch 2455 Loss 1.8328\n",
      "Epoch 2 Batch 2456 Loss 1.8606\n",
      "Epoch 2 Batch 2457 Loss 1.7305\n",
      "Epoch 2 Batch 2458 Loss 1.7841\n",
      "Epoch 2 Batch 2459 Loss 1.6906\n",
      "Epoch 2 Batch 2460 Loss 1.6208\n",
      "Epoch 2 Batch 2461 Loss 1.3254\n",
      "Epoch 2 Batch 2462 Loss 1.3330\n",
      "Epoch 2 Batch 2463 Loss 1.4743\n",
      "Epoch 2 Batch 2464 Loss 1.3520\n",
      "Epoch 2 Batch 2465 Loss 1.4613\n",
      "Epoch 2 Batch 2466 Loss 1.3933\n",
      "Epoch 2 Batch 2467 Loss 1.5722\n",
      "Epoch 2 Batch 2468 Loss 1.6630\n",
      "Epoch 2 Batch 2469 Loss 1.3872\n",
      "Epoch 2 Batch 2470 Loss 1.8194\n",
      "Epoch 2 Batch 2471 Loss 2.1355\n",
      "Epoch 2 Batch 2472 Loss 1.7476\n",
      "Epoch 2 Batch 2473 Loss 1.8708\n",
      "Epoch 2 Batch 2474 Loss 1.5042\n",
      "Epoch 2 Batch 2475 Loss 1.5712\n",
      "Epoch 2 Batch 2476 Loss 1.0244\n",
      "Epoch 2 Batch 2477 Loss 1.2553\n",
      "Epoch 2 Batch 2478 Loss 1.2342\n",
      "Epoch 2 Batch 2479 Loss 1.2676\n",
      "Epoch 2 Batch 2480 Loss 1.0483\n",
      "Epoch 2 Batch 2481 Loss 1.2889\n",
      "Epoch 2 Batch 2482 Loss 1.2276\n",
      "Epoch 2 Batch 2483 Loss 1.5512\n",
      "Epoch 2 Batch 2484 Loss 1.9322\n",
      "Epoch 2 Batch 2485 Loss 1.3303\n",
      "Epoch 2 Batch 2486 Loss 1.6181\n",
      "Epoch 2 Batch 2487 Loss 1.6982\n",
      "Epoch 2 Batch 2488 Loss 1.4867\n",
      "Epoch 2 Batch 2489 Loss 1.8554\n",
      "Epoch 2 Batch 2490 Loss 1.7116\n",
      "Epoch 2 Batch 2491 Loss 1.4676\n",
      "Epoch 2 Batch 2492 Loss 1.7575\n",
      "Epoch 2 Batch 2493 Loss 1.4944\n",
      "Epoch 2 Batch 2494 Loss 1.7186\n",
      "Epoch 2 Batch 2495 Loss 1.6441\n",
      "Epoch 2 Batch 2496 Loss 1.5670\n",
      "Epoch 2 Batch 2497 Loss 1.8086\n",
      "Epoch 2 Batch 2498 Loss 1.9318\n",
      "Epoch 2 Batch 2499 Loss 1.6680\n",
      "Epoch 2 Batch 2500 Loss 1.3522\n",
      "Epoch 2 Batch 2501 Loss 1.3240\n",
      "Epoch 2 Batch 2502 Loss 1.2195\n",
      "Epoch 2 Batch 2503 Loss 0.9639\n",
      "Epoch 2 Batch 2504 Loss 1.2847\n",
      "Epoch 2 Batch 2505 Loss 1.4410\n",
      "Epoch 2 Batch 2506 Loss 1.9325\n",
      "Epoch 2 Batch 2507 Loss 1.3717\n",
      "Epoch 2 Batch 2508 Loss 1.3658\n",
      "Epoch 2 Batch 2509 Loss 1.4599\n",
      "Epoch 2 Batch 2510 Loss 1.5822\n",
      "Epoch 2 Batch 2511 Loss 1.6569\n",
      "Epoch 2 Batch 2512 Loss 1.4772\n",
      "Epoch 2 Batch 2513 Loss 1.8303\n",
      "Epoch 2 Batch 2514 Loss 1.5458\n",
      "Epoch 2 Batch 2515 Loss 1.8442\n",
      "Epoch 2 Batch 2516 Loss 1.4026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Batch 2517 Loss 1.2910\n",
      "Epoch 2 Batch 2518 Loss 1.1626\n",
      "Epoch 2 Batch 2519 Loss 2.0844\n",
      "Epoch 2 Batch 2520 Loss 1.7096\n",
      "Epoch 2 Batch 2521 Loss 1.7449\n",
      "Epoch 2 Batch 2522 Loss 1.8362\n",
      "Epoch 2 Batch 2523 Loss 1.7869\n",
      "Epoch 2 Batch 2524 Loss 1.8777\n",
      "Epoch 2 Batch 2525 Loss 1.6830\n",
      "Epoch 2 Batch 2526 Loss 1.5908\n",
      "Epoch 2 Batch 2527 Loss 2.3014\n",
      "Epoch 2 Batch 2528 Loss 1.8916\n",
      "Epoch 2 Batch 2529 Loss 2.0211\n",
      "Epoch 2 Batch 2530 Loss 1.7027\n",
      "Epoch 2 Batch 2531 Loss 1.6312\n",
      "Epoch 2 Batch 2532 Loss 1.5231\n",
      "Epoch 2 Batch 2533 Loss 1.3710\n",
      "Epoch 2 Batch 2534 Loss 1.3560\n",
      "Epoch 2 Batch 2535 Loss 1.7345\n",
      "Epoch 2 Batch 2536 Loss 1.4188\n",
      "Epoch 2 Batch 2537 Loss 1.2768\n",
      "Epoch 2 Batch 2538 Loss 1.2969\n",
      "Epoch 2 Batch 2539 Loss 1.8825\n",
      "Epoch 2 Batch 2540 Loss 1.4253\n",
      "Epoch 2 Batch 2541 Loss 1.4017\n",
      "Epoch 2 Batch 2542 Loss 1.2193\n",
      "Epoch 2 Batch 2543 Loss 1.4038\n",
      "Epoch 2 Batch 2544 Loss 1.3100\n",
      "Epoch 2 Batch 2545 Loss 1.6577\n",
      "Epoch 2 Batch 2546 Loss 1.6178\n",
      "Epoch 2 Batch 2547 Loss 1.6066\n",
      "Epoch 2 Batch 2548 Loss 1.4903\n",
      "Epoch 2 Batch 2549 Loss 1.8793\n",
      "Epoch 2 Batch 2550 Loss 2.1143\n",
      "Epoch 2 Batch 2551 Loss 1.3165\n",
      "Epoch 2 Batch 2552 Loss 1.6957\n",
      "Epoch 2 Batch 2553 Loss 1.7939\n",
      "Epoch 2 Batch 2554 Loss 1.8528\n",
      "Epoch 2 Batch 2555 Loss 1.7377\n",
      "Epoch 2 Batch 2556 Loss 1.5836\n",
      "Epoch 2 Batch 2557 Loss 1.3862\n",
      "Epoch 2 Batch 2558 Loss 1.6699\n",
      "Epoch 2 Batch 2559 Loss 1.4322\n",
      "Epoch 2 Batch 2560 Loss 1.2759\n",
      "Epoch 2 Batch 2561 Loss 1.4783\n",
      "Epoch 2 Batch 2562 Loss 1.3068\n",
      "Epoch 2 Batch 2563 Loss 1.1860\n",
      "Epoch 2 Batch 2564 Loss 1.1934\n",
      "Epoch 2 Batch 2565 Loss 1.4030\n",
      "Epoch 2 Batch 2566 Loss 1.1979\n",
      "Epoch 2 Batch 2567 Loss 1.3773\n",
      "Epoch 2 Batch 2568 Loss 1.4862\n",
      "Epoch 2 Batch 2569 Loss 1.4984\n",
      "Epoch 2 Batch 2570 Loss 1.8174\n",
      "Epoch 2 Batch 2571 Loss 1.6519\n",
      "Epoch 2 Batch 2572 Loss 1.5569\n",
      "Epoch 2 Batch 2573 Loss 1.6951\n",
      "Epoch 2 Batch 2574 Loss 1.6552\n",
      "Epoch 2 Batch 2575 Loss 1.8466\n",
      "Epoch 2 Batch 2576 Loss 1.2959\n",
      "Epoch 2 Batch 2577 Loss 1.2600\n",
      "Epoch 2 Batch 2578 Loss 1.2050\n",
      "Epoch 2 Batch 2579 Loss 1.6957\n",
      "Epoch 2 Batch 2580 Loss 1.5269\n",
      "Epoch 2 Batch 2581 Loss 1.6906\n",
      "Epoch 2 Batch 2582 Loss 1.6945\n",
      "Epoch 2 Batch 2583 Loss 1.7344\n",
      "Epoch 2 Batch 2584 Loss 1.6787\n",
      "Epoch 2 Batch 2585 Loss 1.5723\n",
      "Epoch 2 Batch 2586 Loss 1.3612\n",
      "Epoch 2 Batch 2587 Loss 1.6187\n",
      "Epoch 2 Batch 2588 Loss 1.2549\n",
      "Epoch 2 Batch 2589 Loss 1.4460\n",
      "Epoch 2 Batch 2590 Loss 1.4795\n",
      "Saving checkpoint for epoch 2 at E:\\GitHub\\QA-abstract-and-reasoning\\data\\checkpoints\\training_checkpoints\\ckpt-1\n",
      "Epoch 2 Loss 1.7333\n",
      "Time taken for 1 epoch 528.8291599750519 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = params[\"epochs\"]\n",
    "# 如果检查点存在，则恢复最新的检查点。\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print ('Latest checkpoint restored!!')\n",
    "    \n",
    "for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch+1)):\n",
    "        batch_loss = train_step(inp, targ)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        if batch % 1 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                         batch,\n",
    "                                                         batch_loss.numpy()))\n",
    "    # saving (checkpoint) the model every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        ckpt_save_path = ckpt_manager.save()\n",
    "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
    "                                                             ckpt_save_path))\n",
    "\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 载入模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model restored\n"
     ]
    }
   ],
   "source": [
    "# 如果检查点存在，则恢复最新的检查点。\n",
    "ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "print(\"Model restored\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model,inputs):\n",
    "    attention_plot = np.zeros((params[\"max_dec_len\"], params[\"max_enc_len\"]))\n",
    "    \n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "    result = ''\n",
    "    \n",
    "    hidden = [tf.zeros((1, params[\"enc_units\"]))]\n",
    "    enc_output, enc_hidden = model.encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    \n",
    "    dec_input = tf.expand_dims([vocab['<START>']], 0)\n",
    "    \n",
    "    context_vector, _ = model.attention(dec_hidden, enc_output)\n",
    "\n",
    "    for t in range(params[\"max_dec_len\"]):\n",
    "        \n",
    "        context_vector, attention_weights = model.attention(dec_hidden, enc_output)\n",
    "        \n",
    "        predictions, dec_hidden = model.decoder(dec_input,\n",
    "                                         dec_hidden,\n",
    "                                         enc_output,\n",
    "                                         context_vector)\n",
    "\n",
    "        # storing the attention weights to plot later on\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        \n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += vocab_reversed[predicted_id] + ' '\n",
    "        if vocab_reversed[predicted_id] == '<STOP>':\n",
    "            return result, attention_plot\n",
    "\n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    st = preproc.sentence_proc(sentence)\n",
    "    sentence = preproc.sentence_proc_eval(sentence,params[\"max_enc_len\"]-2,vocab)\n",
    "    result, attention_plot = evaluate(model,sentence)\n",
    "\n",
    "    print('Input: %s' % (st))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "\n",
    "    attention_plot = attention_plot[:len(result.split(' ')), :len(st.split(' '))]\n",
    "    plot_attention(attention_plot, st.split(' '), result.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\Light\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.678 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'方向机 重 助力 泵 方向机 都 换'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = '方向机重，助力泵，方向机都换了还是一样'\n",
    "preproc = Preprocess()\n",
    "preproc.sentence_proc(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib as mpl\n",
    "# mpl.rcParams['font.family'] = 'STSong'  # 显示中文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 方向机 重 助力 泵 方向机 都 换\n",
      "Predicted translation: 方向机 助力 泵 <STOP> \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1440 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "translate(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下半部分\n",
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_predict(inps):\n",
    "    # 判断输入长度\n",
    "    batch_size=len(inps)\n",
    "    # 开辟结果存储list\n",
    "    preidicts=[''] * batch_size\n",
    "    \n",
    "    inps = tf.convert_to_tensor(inps)\n",
    "    # 0. 初始化隐藏层输入\n",
    "    hidden = [tf.zeros((batch_size, params[\"enc_units\"]))]\n",
    "    # 1. 构建encoder\n",
    "    enc_output, enc_hidden = model.encoder(inps, hidden)\n",
    "    # 2. 复制\n",
    "    dec_hidden = enc_hidden\n",
    "    # 3. <START> * BATCH_SIZE \n",
    "    dec_input = tf.expand_dims([vocab['<START>']] * batch_size, 1)\n",
    "    \n",
    "    context_vector, _ = model.attention(dec_hidden, enc_output)\n",
    "    # Teacher forcing - feeding the target as the next input\n",
    "    for t in range(params[\"max_dec_len\"]):\n",
    "        # 计算上下文\n",
    "        context_vector, attention_weights = model.attention(dec_hidden, enc_output)\n",
    "        # 单步预测\n",
    "        predictions, dec_hidden = model.decoder(dec_input,\n",
    "                                         dec_hidden,\n",
    "                                         enc_output,\n",
    "                                         context_vector)\n",
    "        \n",
    "        # id转换 贪婪搜索\n",
    "        predicted_ids = tf.argmax(predictions,axis=1).numpy()\n",
    "        \n",
    "        \n",
    "        for index,predicted_id in enumerate(predicted_ids):\n",
    "            preidicts[index]+= vocab_reversed[predicted_id] + ' '\n",
    "        \n",
    "        # using teacher forcing\n",
    "        dec_input = tf.expand_dims(predicted_ids, 1)\n",
    "\n",
    "    results=[]\n",
    "    for preidict in preidicts:\n",
    "        # 去掉句子前后空格\n",
    "        preidict=preidict.strip()\n",
    "        # 句子小于max len就结束了 截断\n",
    "        if '<STOP>' in preidict:\n",
    "            # 截断stop\n",
    "            preidict=preidict[:preidict.index('<STOP>')]\n",
    "        # 保存结果\n",
    "        results.append(preidict)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试代码\n",
    "# ds = iter(dataset)\n",
    "# x,y = ds.next()\n",
    "# batch_predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict(data_X, batch_size):\n",
    "    # 存储结果\n",
    "    results=[]\n",
    "    # 样本数量\n",
    "    sample_size=len(data_X)\n",
    "    # batch 操作轮数 math.ceil向上取整 小数 +1\n",
    "    # 因为最后一个batch可能不足一个batch size 大小 ,但是依然需要计算  \n",
    "    steps_epoch = math.ceil(sample_size/batch_size)\n",
    "    # [0,steps_epoch)\n",
    "    for i in tqdm(range(steps_epoch)):\n",
    "        batch_data = data_X[i*batch_size:(i+1)*batch_size]\n",
    "        results+=batch_predict(batch_data)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  2.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.37 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results=model_predict(test_x[:10],batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['这种 情况 分析 维修 ',\n",
       " '修理厂 进行 抛光 处理 一下 ',\n",
       " '更换 喇叭 ',\n",
       " '发动机 排气管 漏气 ',\n",
       " '描述 情况 下 轮胎 动平衡 问题 导致 ',\n",
       " '描述 情况 分析 应该 排气管 滴水 正常 ',\n",
       " '防冻液 添加 颜色 添加 颜色 防冻液 颜色 添加 颜色 防冻液 颜色 添加 颜色 防冻液 颜色 添加 颜色 防冻液 颜色 添加 颜色 防冻液 颜色 添加 颜色 防冻液 颜色 添加 颜色 防冻液 颜色 添加 颜色',\n",
       " '机油 问题 ',\n",
       " '这种 情况 需要 更换 轮胎 花纹 深度 不 影响 车辆 行驶 中 高速行驶 过程 中 轮胎 地面 裂纹 扩散 裂纹 没有 影响 正常 使用 没有 影响 正常 使用 没有 影响 正常 使用 没有',\n",
       " '这种 情况 应该 天气 冷 机油 润滑 ']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf2.0]",
   "language": "python",
   "name": "conda-env-tf2.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
