{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import sys\n",
    "sys.path.append('../')  # 返回notebook的上一级目录\n",
    "# sys.path.append('E:\\GitHub\\QA-abstract-and-reasoning')  # 效果同上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no colab\n"
     ]
    }
   ],
   "source": [
    "# 在google colab运行则执行以下代码\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive_path = '/content/drive'\n",
    "    working_path = drive_path + \"/My Drive/QA\" # 工作路径\n",
    "    drive.mount(drive_path)\n",
    "    os.chdir(working_path)\n",
    "    sys.path.append(working_path)  # 环境变量\n",
    "    print(\"current working directory: \", os.getcwd())\n",
    "    \n",
    "    # %tensorflow_version 仅存在于 Colab\n",
    "    %tensorflow_version 2.x\n",
    "    print(\"run notebook in colab\")\n",
    "except:\n",
    "    print(\"no colab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "np.set_printoptions(suppress=True)\n",
    "from utils.plot import plot_attention\n",
    "from utils.saveLoader import *\n",
    "from utils.config import *\n",
    "from layers import *\n",
    "from preprocess import Preprocess\n",
    "from gensim.models.word2vec import LineSentence, Word2Vec\n",
    "import tensorflow as tf\n",
    "# from model_layer import seq2seq_model\n",
    "import time\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[限制gpu内存增长](https://tensorflow.google.cn/guide/gpu#limiting_gpu_memory_growth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "from utils.config_gpu import config_gpu\n",
    "config_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x,train_y,test_x = load_train_dataset()  # 数据集\n",
    "vocab,vocab_reversed = load_vocab(VOCAB_PAD)  # vocab\n",
    "embedding_matrix = np.loadtxt(EMBEDDING_MATRIX_PAD)  # 预训练层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 设置参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'vocab_size': 32566, 'max_enc_len': 260, 'max_dec_len': 33, 'embed_size': 300, 'enc_units': 256, 'attn_units': 10, 'dec_units': 256, 'batch_size': 32, 'epochs': 2}\n"
     ]
    }
   ],
   "source": [
    "params = {}\n",
    "params[\"vocab_size\"] = len(vocab)\n",
    "params[\"max_enc_len\"] = train_x.shape[1]  # 260\n",
    "params[\"max_dec_len\"] = train_y.shape[1]  # 33\n",
    "params[\"embed_size\"] = embedding_matrix.shape[1]\n",
    "params[\"enc_units\"] = 256\n",
    "params[\"attn_units\"] = 10\n",
    "params[\"dec_units\"] = params[\"enc_units\"]\n",
    "params[\"batch_size\"] = 32\n",
    "params[\"epochs\"] = 2\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取部分数据进行训练\n",
    "# sample_num=64\n",
    "sample_num = train_x.shape[0]\n",
    "dataset = tf.data.Dataset.from_tensor_slices((train_x[:sample_num], train_y[:sample_num])).shuffle(params[\"batch_size\"]*2+1)\n",
    "dataset = dataset.batch(params[\"batch_size\"], drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = sample_num//params[\"batch_size\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seq2seq import *\n",
    "model=Seq2Seq(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存点设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.config import CKPT_DIR, CKPT_PREFIX\n",
    "from utils.saveLoader import del_all_files_of_dir\n",
    "ckpt = tf.train.Checkpoint(Seq2Seq=model)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, CKPT_DIR, max_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there no files in this path\n",
      "Model restored\n"
     ]
    }
   ],
   "source": [
    "del_all_files_of_dir(CKPT_DIR)\n",
    "ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "print(\"Model restored\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[SparseCategoricalCrossentropy](https://tensorflow.google.cn/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(name='Adam',learning_rate=0.001)\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "pad_index=vocab['<PAD>']\n",
    "unk_index=vocab['<UNK>']\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    pad_mask = tf.math.equal(real, pad_index)\n",
    "    unk_mask = tf.math.equal(real, unk_index)\n",
    "    # <PAD> 和 <UNK> 的损失都不算\n",
    "    mask = tf.math.logical_not(tf.math.logical_or(pad_mask,unk_mask))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_mean(loss_)\n",
    "    # return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 调试train_step()\n",
    "# inp, targ = next(iter(dataset))\n",
    "# pad_index=vocab['<PAD>']\n",
    "# unk_index=vocab['<UNK>']\n",
    "# enc_output, enc_hidden = model.call_encoder(inp)\n",
    "# dec_hidden = enc_hidden\n",
    "# dec_input = tf.expand_dims([vocab['<START>']] * params[\"batch_size\"], 1)\n",
    "# predictions, _ = model(dec_input, dec_hidden, enc_output, targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ):\n",
    "    pad_index=vocab['<PAD>']\n",
    "    unk_index=vocab['<UNK>']\n",
    "    loss = 0\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        # 1. 构建encoder\n",
    "        enc_output, enc_hidden = model.call_encoder(inp)\n",
    "        # 2. 复制\n",
    "        dec_hidden = enc_hidden\n",
    "        # 3. <START> * BATCH_SIZE \n",
    "        dec_input = tf.expand_dims([vocab['<START>']] * params[\"batch_size\"], 1)\n",
    "        \n",
    "        # 逐个预测序列\n",
    "        predictions, _ = model(dec_input, dec_hidden, enc_output, targ)\n",
    "        \n",
    "        batch_loss = loss_function(targ[:, 1:], predictions)\n",
    "\n",
    "        variables = model.encoder.trainable_variables + model.decoder.trainable_variables+ model.attention.trainable_variables\n",
    "    \n",
    "        gradients = tape.gradient(batch_loss, variables)\n",
    "\n",
    "        optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "        return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 5.3902\n",
      "Epoch 1 Batch 1 Loss 5.6638\n",
      "Epoch 1 Batch 2 Loss 5.2197\n",
      "Epoch 1 Batch 3 Loss 5.5543\n",
      "Epoch 1 Batch 4 Loss 5.8169\n",
      "Epoch 1 Batch 5 Loss 4.5870\n",
      "Epoch 1 Batch 6 Loss 5.0880\n",
      "Epoch 1 Batch 7 Loss 4.8601\n",
      "Epoch 1 Batch 8 Loss 4.3980\n",
      "Epoch 1 Batch 9 Loss 3.2799\n",
      "Epoch 1 Batch 10 Loss 5.2775\n",
      "Epoch 1 Batch 11 Loss 3.6353\n",
      "Epoch 1 Batch 12 Loss 2.7170\n",
      "Epoch 1 Batch 13 Loss 2.6421\n",
      "Epoch 1 Batch 14 Loss 2.1167\n",
      "Epoch 1 Batch 15 Loss 2.9356\n",
      "Epoch 1 Batch 16 Loss 2.4914\n",
      "Epoch 1 Batch 17 Loss 2.3527\n",
      "Epoch 1 Batch 18 Loss 2.4934\n",
      "Epoch 1 Batch 19 Loss 3.1829\n",
      "Epoch 1 Batch 20 Loss 3.3698\n",
      "Epoch 1 Batch 21 Loss 3.2792\n",
      "Epoch 1 Batch 22 Loss 3.1758\n",
      "Epoch 1 Batch 23 Loss 3.3565\n",
      "Epoch 1 Batch 24 Loss 2.8799\n",
      "Epoch 1 Batch 25 Loss 2.5533\n",
      "Epoch 1 Batch 26 Loss 2.5327\n",
      "Epoch 1 Batch 27 Loss 3.1751\n",
      "Epoch 1 Batch 28 Loss 3.1140\n",
      "Epoch 1 Batch 29 Loss 2.9614\n",
      "Epoch 1 Batch 30 Loss 3.0480\n",
      "Epoch 1 Batch 31 Loss 3.4621\n",
      "Epoch 1 Batch 32 Loss 3.3694\n",
      "Epoch 1 Batch 33 Loss 2.5887\n",
      "Epoch 1 Batch 34 Loss 1.7413\n",
      "Epoch 1 Batch 35 Loss 2.1052\n",
      "Epoch 1 Batch 36 Loss 2.1231\n",
      "Epoch 1 Batch 37 Loss 1.6203\n",
      "Epoch 1 Batch 38 Loss 1.9480\n",
      "Epoch 1 Batch 39 Loss 2.0578\n",
      "Epoch 1 Batch 40 Loss 2.0394\n",
      "Epoch 1 Batch 41 Loss 2.6192\n",
      "Epoch 1 Batch 42 Loss 2.0071\n",
      "Epoch 1 Batch 43 Loss 2.1149\n",
      "Epoch 1 Batch 44 Loss 2.5986\n",
      "Epoch 1 Batch 45 Loss 2.8199\n",
      "Epoch 1 Batch 46 Loss 3.2689\n",
      "Epoch 1 Batch 47 Loss 2.7101\n",
      "Epoch 1 Batch 48 Loss 2.4153\n",
      "Epoch 1 Batch 49 Loss 2.4914\n",
      "Epoch 1 Batch 50 Loss 2.6334\n",
      "Epoch 1 Batch 51 Loss 2.6871\n",
      "Epoch 1 Batch 52 Loss 3.0354\n",
      "Epoch 1 Batch 53 Loss 2.7590\n",
      "Epoch 1 Batch 54 Loss 3.3114\n",
      "Epoch 1 Batch 55 Loss 3.1814\n",
      "Epoch 1 Batch 56 Loss 3.7608\n",
      "Epoch 1 Batch 57 Loss 3.8574\n",
      "Epoch 1 Batch 58 Loss 4.1828\n",
      "Epoch 1 Batch 59 Loss 3.0734\n",
      "Epoch 1 Batch 60 Loss 2.7548\n",
      "Epoch 1 Batch 61 Loss 2.6958\n",
      "Epoch 1 Batch 62 Loss 2.7401\n",
      "Epoch 1 Batch 63 Loss 2.9483\n",
      "Epoch 1 Batch 64 Loss 2.3175\n",
      "Epoch 1 Batch 65 Loss 2.4642\n",
      "Epoch 1 Batch 66 Loss 2.5425\n",
      "Epoch 1 Batch 67 Loss 2.9211\n",
      "Epoch 1 Batch 68 Loss 2.5065\n",
      "Epoch 1 Batch 69 Loss 2.9689\n",
      "Epoch 1 Batch 70 Loss 2.7883\n",
      "Epoch 1 Batch 71 Loss 3.0603\n",
      "Epoch 1 Batch 72 Loss 2.8202\n",
      "Epoch 1 Batch 73 Loss 3.4948\n",
      "Epoch 1 Batch 74 Loss 3.5883\n",
      "Epoch 1 Batch 75 Loss 3.4207\n",
      "Epoch 1 Batch 76 Loss 3.3605\n",
      "Epoch 1 Batch 77 Loss 2.8247\n",
      "Epoch 1 Batch 78 Loss 3.2950\n",
      "Epoch 1 Batch 79 Loss 3.2665\n",
      "Epoch 1 Batch 80 Loss 3.0373\n",
      "Epoch 1 Batch 81 Loss 2.8295\n",
      "Epoch 1 Batch 82 Loss 2.5969\n",
      "Epoch 1 Batch 83 Loss 2.5897\n",
      "Epoch 1 Batch 84 Loss 3.2455\n",
      "Epoch 1 Batch 85 Loss 3.2307\n",
      "Epoch 1 Batch 86 Loss 3.4268\n",
      "Epoch 1 Batch 87 Loss 3.2165\n",
      "Epoch 1 Batch 88 Loss 2.7619\n",
      "Epoch 1 Batch 89 Loss 3.0768\n",
      "Epoch 1 Batch 90 Loss 3.0649\n",
      "Epoch 1 Batch 91 Loss 2.4568\n",
      "Epoch 1 Batch 92 Loss 3.0979\n",
      "Epoch 1 Batch 93 Loss 2.1221\n",
      "Epoch 1 Batch 94 Loss 2.0194\n",
      "Epoch 1 Batch 95 Loss 1.7367\n",
      "Epoch 1 Batch 96 Loss 2.3849\n",
      "Epoch 1 Batch 97 Loss 2.8051\n",
      "Epoch 1 Batch 98 Loss 3.2872\n",
      "Epoch 1 Batch 99 Loss 3.8254\n",
      "Epoch 1 Batch 100 Loss 4.2381\n",
      "Epoch 1 Batch 101 Loss 3.7809\n",
      "Epoch 1 Batch 102 Loss 2.9281\n",
      "Epoch 1 Batch 103 Loss 2.8255\n",
      "Epoch 1 Batch 104 Loss 2.8410\n",
      "Epoch 1 Batch 105 Loss 3.3247\n",
      "Epoch 1 Batch 106 Loss 2.5788\n",
      "Epoch 1 Batch 107 Loss 3.4830\n",
      "Epoch 1 Batch 108 Loss 3.5723\n",
      "Epoch 1 Batch 109 Loss 2.7296\n",
      "Epoch 1 Batch 110 Loss 2.8703\n",
      "Epoch 1 Batch 111 Loss 2.0184\n",
      "Epoch 1 Batch 112 Loss 2.4584\n",
      "Epoch 1 Batch 113 Loss 2.6152\n",
      "Epoch 1 Batch 114 Loss 2.6472\n",
      "Epoch 1 Batch 115 Loss 2.7472\n",
      "Epoch 1 Batch 116 Loss 2.8306\n",
      "Epoch 1 Batch 117 Loss 2.8870\n",
      "Epoch 1 Batch 118 Loss 2.8602\n",
      "Epoch 1 Batch 119 Loss 2.3996\n",
      "Epoch 1 Batch 120 Loss 2.5895\n",
      "Epoch 1 Batch 121 Loss 3.1986\n",
      "Epoch 1 Batch 122 Loss 2.5711\n",
      "Epoch 1 Batch 123 Loss 2.5145\n",
      "Epoch 1 Batch 124 Loss 2.2236\n",
      "Epoch 1 Batch 125 Loss 1.9991\n",
      "Epoch 1 Batch 126 Loss 2.2327\n",
      "Epoch 1 Batch 127 Loss 2.4315\n",
      "Epoch 1 Batch 128 Loss 1.8755\n",
      "Epoch 1 Batch 129 Loss 2.4874\n",
      "Epoch 1 Batch 130 Loss 2.9522\n",
      "Epoch 1 Batch 131 Loss 2.9418\n",
      "Epoch 1 Batch 132 Loss 2.6236\n",
      "Epoch 1 Batch 133 Loss 3.2565\n",
      "Epoch 1 Batch 134 Loss 3.2114\n",
      "Epoch 1 Batch 135 Loss 2.7034\n",
      "Epoch 1 Batch 136 Loss 2.5936\n",
      "Epoch 1 Batch 137 Loss 2.8020\n",
      "Epoch 1 Batch 138 Loss 2.9648\n",
      "Epoch 1 Batch 139 Loss 2.8406\n",
      "Epoch 1 Batch 140 Loss 2.5036\n",
      "Epoch 1 Batch 141 Loss 2.2758\n",
      "Epoch 1 Batch 142 Loss 2.7403\n",
      "Epoch 1 Batch 143 Loss 2.3130\n",
      "Epoch 1 Batch 144 Loss 2.7424\n",
      "Epoch 1 Batch 145 Loss 2.8741\n",
      "Epoch 1 Batch 146 Loss 2.7752\n",
      "Epoch 1 Batch 147 Loss 3.1695\n",
      "Epoch 1 Batch 148 Loss 3.2862\n",
      "Epoch 1 Batch 149 Loss 3.5620\n",
      "Epoch 1 Batch 150 Loss 2.2938\n",
      "Epoch 1 Batch 151 Loss 2.7315\n",
      "Epoch 1 Batch 152 Loss 3.2000\n",
      "Epoch 1 Batch 153 Loss 3.0762\n",
      "Epoch 1 Batch 154 Loss 2.1902\n",
      "Epoch 1 Batch 155 Loss 2.8025\n",
      "Epoch 1 Batch 156 Loss 3.3217\n",
      "Epoch 1 Batch 157 Loss 3.7658\n",
      "Epoch 1 Batch 158 Loss 3.3751\n",
      "Epoch 1 Batch 159 Loss 3.3136\n",
      "Epoch 1 Batch 160 Loss 2.9501\n",
      "Epoch 1 Batch 161 Loss 2.4670\n",
      "Epoch 1 Batch 162 Loss 2.4228\n",
      "Epoch 1 Batch 163 Loss 2.0666\n",
      "Epoch 1 Batch 164 Loss 1.7592\n",
      "Epoch 1 Batch 165 Loss 2.3776\n",
      "Epoch 1 Batch 166 Loss 3.1320\n",
      "Epoch 1 Batch 167 Loss 3.0078\n",
      "Epoch 1 Batch 168 Loss 2.6889\n",
      "Epoch 1 Batch 169 Loss 2.7223\n",
      "Epoch 1 Batch 170 Loss 2.6799\n",
      "Epoch 1 Batch 171 Loss 2.4379\n",
      "Epoch 1 Batch 172 Loss 2.3113\n",
      "Epoch 1 Batch 173 Loss 2.8215\n",
      "Epoch 1 Batch 174 Loss 2.6059\n",
      "Epoch 1 Batch 175 Loss 2.6432\n",
      "Epoch 1 Batch 176 Loss 2.3513\n",
      "Epoch 1 Batch 177 Loss 2.8453\n",
      "Epoch 1 Batch 178 Loss 2.5837\n",
      "Epoch 1 Batch 179 Loss 2.2726\n",
      "Epoch 1 Batch 180 Loss 2.1194\n",
      "Epoch 1 Batch 181 Loss 2.1744\n",
      "Epoch 1 Batch 182 Loss 2.0413\n",
      "Epoch 1 Batch 183 Loss 1.8558\n",
      "Epoch 1 Batch 184 Loss 1.8125\n",
      "Epoch 1 Batch 185 Loss 1.9466\n",
      "Epoch 1 Batch 186 Loss 2.1311\n",
      "Epoch 1 Batch 187 Loss 2.1136\n",
      "Epoch 1 Batch 188 Loss 1.9491\n",
      "Epoch 1 Batch 189 Loss 2.4922\n",
      "Epoch 1 Batch 190 Loss 2.7133\n",
      "Epoch 1 Batch 191 Loss 2.8314\n",
      "Epoch 1 Batch 192 Loss 3.0526\n",
      "Epoch 1 Batch 193 Loss 3.0522\n",
      "Epoch 1 Batch 194 Loss 2.5966\n",
      "Epoch 1 Batch 195 Loss 2.5867\n",
      "Epoch 1 Batch 196 Loss 3.3329\n",
      "Epoch 1 Batch 197 Loss 3.0309\n",
      "Epoch 1 Batch 198 Loss 2.3152\n",
      "Epoch 1 Batch 199 Loss 2.5482\n",
      "Epoch 1 Batch 200 Loss 2.8602\n",
      "Epoch 1 Batch 201 Loss 2.1062\n",
      "Epoch 1 Batch 202 Loss 2.1994\n",
      "Epoch 1 Batch 203 Loss 2.0502\n",
      "Epoch 1 Batch 204 Loss 2.1043\n",
      "Epoch 1 Batch 205 Loss 2.3741\n",
      "Epoch 1 Batch 206 Loss 2.8702\n",
      "Epoch 1 Batch 207 Loss 2.9256\n",
      "Epoch 1 Batch 208 Loss 2.5158\n",
      "Epoch 1 Batch 209 Loss 2.1614\n",
      "Epoch 1 Batch 210 Loss 2.7359\n",
      "Epoch 1 Batch 211 Loss 2.4392\n",
      "Epoch 1 Batch 212 Loss 2.5633\n",
      "Epoch 1 Batch 213 Loss 2.4807\n",
      "Epoch 1 Batch 214 Loss 2.5585\n",
      "Epoch 1 Batch 215 Loss 2.9571\n",
      "Epoch 1 Batch 216 Loss 2.4981\n",
      "Epoch 1 Batch 217 Loss 2.4663\n",
      "Epoch 1 Batch 218 Loss 1.4804\n",
      "Epoch 1 Batch 219 Loss 2.1263\n",
      "Epoch 1 Batch 220 Loss 2.9785\n",
      "Epoch 1 Batch 221 Loss 2.1023\n",
      "Epoch 1 Batch 222 Loss 2.3787\n",
      "Epoch 1 Batch 223 Loss 2.2947\n",
      "Epoch 1 Batch 224 Loss 2.4435\n",
      "Epoch 1 Batch 225 Loss 2.3161\n",
      "Epoch 1 Batch 226 Loss 2.6469\n",
      "Epoch 1 Batch 227 Loss 2.3697\n",
      "Epoch 1 Batch 228 Loss 2.8273\n",
      "Epoch 1 Batch 229 Loss 2.2675\n",
      "Epoch 1 Batch 230 Loss 2.0508\n",
      "Epoch 1 Batch 231 Loss 2.3235\n",
      "Epoch 1 Batch 232 Loss 1.9769\n",
      "Epoch 1 Batch 233 Loss 2.2207\n",
      "Epoch 1 Batch 234 Loss 1.5929\n",
      "Epoch 1 Batch 235 Loss 2.0676\n",
      "Epoch 1 Batch 236 Loss 2.3832\n",
      "Epoch 1 Batch 237 Loss 2.4098\n",
      "Epoch 1 Batch 238 Loss 2.0383\n",
      "Epoch 1 Batch 239 Loss 2.2007\n",
      "Epoch 1 Batch 240 Loss 2.0768\n",
      "Epoch 1 Batch 241 Loss 2.5313\n",
      "Epoch 1 Batch 242 Loss 2.9232\n",
      "Epoch 1 Batch 243 Loss 2.3542\n",
      "Epoch 1 Batch 244 Loss 2.5897\n",
      "Epoch 1 Batch 245 Loss 2.6997\n",
      "Epoch 1 Batch 246 Loss 2.5944\n",
      "Epoch 1 Batch 247 Loss 2.0906\n",
      "Epoch 1 Batch 248 Loss 2.5644\n",
      "Epoch 1 Batch 249 Loss 2.7564\n",
      "Epoch 1 Batch 250 Loss 2.0965\n",
      "Epoch 1 Batch 251 Loss 2.4813\n",
      "Epoch 1 Batch 252 Loss 3.0839\n",
      "Epoch 1 Batch 253 Loss 3.1102\n",
      "Epoch 1 Batch 254 Loss 2.2369\n",
      "Epoch 1 Batch 255 Loss 2.5319\n",
      "Epoch 1 Batch 256 Loss 2.3615\n",
      "Epoch 1 Batch 257 Loss 2.7021\n",
      "Epoch 1 Batch 258 Loss 2.0691\n",
      "Epoch 1 Batch 259 Loss 2.2562\n",
      "Epoch 1 Batch 260 Loss 2.1806\n",
      "Epoch 1 Batch 261 Loss 2.1964\n",
      "Epoch 1 Batch 262 Loss 1.8498\n",
      "Epoch 1 Batch 263 Loss 2.0302\n",
      "Epoch 1 Batch 264 Loss 2.0898\n",
      "Epoch 1 Batch 265 Loss 2.1634\n",
      "Epoch 1 Batch 266 Loss 2.2413\n",
      "Epoch 1 Batch 267 Loss 2.4801\n",
      "Epoch 1 Batch 268 Loss 2.4181\n",
      "Epoch 1 Batch 269 Loss 2.8004\n",
      "Epoch 1 Batch 270 Loss 3.2501\n",
      "Epoch 1 Batch 271 Loss 3.0315\n",
      "Epoch 1 Batch 272 Loss 3.3526\n",
      "Epoch 1 Batch 273 Loss 2.7694\n",
      "Epoch 1 Batch 274 Loss 2.7485\n",
      "Epoch 1 Batch 275 Loss 2.6867\n",
      "Epoch 1 Batch 276 Loss 2.5919\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 277 Loss 2.5332\n",
      "Epoch 1 Batch 278 Loss 2.7266\n",
      "Epoch 1 Batch 279 Loss 2.2328\n",
      "Epoch 1 Batch 280 Loss 2.3626\n",
      "Epoch 1 Batch 281 Loss 2.8816\n",
      "Epoch 1 Batch 282 Loss 2.6482\n",
      "Epoch 1 Batch 283 Loss 2.3508\n",
      "Epoch 1 Batch 284 Loss 2.4148\n",
      "Epoch 1 Batch 285 Loss 2.1364\n",
      "Epoch 1 Batch 286 Loss 1.7620\n",
      "Epoch 1 Batch 287 Loss 2.5446\n",
      "Epoch 1 Batch 288 Loss 1.9034\n",
      "Epoch 1 Batch 289 Loss 2.3294\n",
      "Epoch 1 Batch 290 Loss 2.4867\n",
      "Epoch 1 Batch 291 Loss 2.5074\n",
      "Epoch 1 Batch 292 Loss 1.7173\n",
      "Epoch 1 Batch 293 Loss 2.1615\n",
      "Epoch 1 Batch 294 Loss 2.1000\n",
      "Epoch 1 Batch 295 Loss 2.4031\n",
      "Epoch 1 Batch 296 Loss 1.9260\n",
      "Epoch 1 Batch 297 Loss 2.8261\n",
      "Epoch 1 Batch 298 Loss 2.3688\n",
      "Epoch 1 Batch 299 Loss 2.1287\n",
      "Epoch 1 Batch 300 Loss 2.3126\n",
      "Epoch 1 Batch 301 Loss 2.2815\n",
      "Epoch 1 Batch 302 Loss 1.7520\n",
      "Epoch 1 Batch 303 Loss 2.2485\n",
      "Epoch 1 Batch 304 Loss 1.6329\n",
      "Epoch 1 Batch 305 Loss 2.2861\n",
      "Epoch 1 Batch 306 Loss 2.0162\n",
      "Epoch 1 Batch 307 Loss 2.9728\n",
      "Epoch 1 Batch 308 Loss 2.4058\n",
      "Epoch 1 Batch 309 Loss 2.5699\n",
      "Epoch 1 Batch 310 Loss 2.0918\n",
      "Epoch 1 Batch 311 Loss 2.7916\n",
      "Epoch 1 Batch 312 Loss 1.9978\n",
      "Epoch 1 Batch 313 Loss 2.4355\n",
      "Epoch 1 Batch 314 Loss 2.5204\n",
      "Epoch 1 Batch 315 Loss 2.4591\n",
      "Epoch 1 Batch 316 Loss 2.1635\n",
      "Epoch 1 Batch 317 Loss 2.4706\n",
      "Epoch 1 Batch 318 Loss 1.8999\n",
      "Epoch 1 Batch 319 Loss 1.5219\n",
      "Epoch 1 Batch 320 Loss 2.2400\n",
      "Epoch 1 Batch 321 Loss 2.4123\n",
      "Epoch 1 Batch 322 Loss 3.3645\n",
      "Epoch 1 Batch 323 Loss 2.4655\n",
      "Epoch 1 Batch 324 Loss 2.0875\n",
      "Epoch 1 Batch 325 Loss 2.0768\n",
      "Epoch 1 Batch 326 Loss 2.4470\n",
      "Epoch 1 Batch 327 Loss 2.1219\n",
      "Epoch 1 Batch 328 Loss 3.0059\n",
      "Epoch 1 Batch 329 Loss 2.2662\n",
      "Epoch 1 Batch 330 Loss 2.1487\n",
      "Epoch 1 Batch 331 Loss 2.3571\n",
      "Epoch 1 Batch 332 Loss 2.0877\n",
      "Epoch 1 Batch 333 Loss 2.0407\n",
      "Epoch 1 Batch 334 Loss 2.1134\n",
      "Epoch 1 Batch 335 Loss 1.8264\n",
      "Epoch 1 Batch 336 Loss 2.2049\n",
      "Epoch 1 Batch 337 Loss 2.3071\n",
      "Epoch 1 Batch 338 Loss 2.5702\n",
      "Epoch 1 Batch 339 Loss 2.7576\n",
      "Epoch 1 Batch 340 Loss 2.5724\n",
      "Epoch 1 Batch 341 Loss 2.1694\n",
      "Epoch 1 Batch 342 Loss 2.4148\n",
      "Epoch 1 Batch 343 Loss 2.4045\n",
      "Epoch 1 Batch 344 Loss 2.1458\n",
      "Epoch 1 Batch 345 Loss 2.2302\n",
      "Epoch 1 Batch 346 Loss 2.3773\n",
      "Epoch 1 Batch 347 Loss 2.3829\n",
      "Epoch 1 Batch 348 Loss 2.5015\n",
      "Epoch 1 Batch 349 Loss 2.5560\n",
      "Epoch 1 Batch 350 Loss 2.4687\n",
      "Epoch 1 Batch 351 Loss 1.7655\n",
      "Epoch 1 Batch 352 Loss 1.7364\n",
      "Epoch 1 Batch 353 Loss 2.0336\n",
      "Epoch 1 Batch 354 Loss 1.7124\n",
      "Epoch 1 Batch 355 Loss 2.2998\n",
      "Epoch 1 Batch 356 Loss 2.2625\n",
      "Epoch 1 Batch 357 Loss 2.5436\n",
      "Epoch 1 Batch 358 Loss 2.4013\n",
      "Epoch 1 Batch 359 Loss 2.6664\n",
      "Epoch 1 Batch 360 Loss 2.3057\n",
      "Epoch 1 Batch 361 Loss 2.2291\n",
      "Epoch 1 Batch 362 Loss 2.3264\n",
      "Epoch 1 Batch 363 Loss 2.5398\n",
      "Epoch 1 Batch 364 Loss 2.3479\n",
      "Epoch 1 Batch 365 Loss 2.9126\n",
      "Epoch 1 Batch 366 Loss 2.4842\n",
      "Epoch 1 Batch 367 Loss 2.1619\n",
      "Epoch 1 Batch 368 Loss 2.3842\n",
      "Epoch 1 Batch 369 Loss 2.4876\n",
      "Epoch 1 Batch 370 Loss 3.0875\n",
      "Epoch 1 Batch 371 Loss 2.5535\n",
      "Epoch 1 Batch 372 Loss 2.1986\n",
      "Epoch 1 Batch 373 Loss 2.3750\n",
      "Epoch 1 Batch 374 Loss 2.0278\n",
      "Epoch 1 Batch 375 Loss 1.6727\n",
      "Epoch 1 Batch 376 Loss 1.5079\n",
      "Epoch 1 Batch 377 Loss 2.1954\n",
      "Epoch 1 Batch 378 Loss 2.0253\n",
      "Epoch 1 Batch 379 Loss 2.1142\n",
      "Epoch 1 Batch 380 Loss 1.8532\n",
      "Epoch 1 Batch 381 Loss 2.3771\n",
      "Epoch 1 Batch 382 Loss 1.9553\n",
      "Epoch 1 Batch 383 Loss 2.1341\n",
      "Epoch 1 Batch 384 Loss 1.8893\n",
      "Epoch 1 Batch 385 Loss 2.3114\n",
      "Epoch 1 Batch 386 Loss 1.9859\n",
      "Epoch 1 Batch 387 Loss 2.1746\n",
      "Epoch 1 Batch 388 Loss 2.2136\n",
      "Epoch 1 Batch 389 Loss 2.4818\n",
      "Epoch 1 Batch 390 Loss 2.3266\n",
      "Epoch 1 Batch 391 Loss 2.5229\n",
      "Epoch 1 Batch 392 Loss 1.9732\n",
      "Epoch 1 Batch 393 Loss 2.2386\n",
      "Epoch 1 Batch 394 Loss 2.2073\n",
      "Epoch 1 Batch 395 Loss 1.9445\n",
      "Epoch 1 Batch 396 Loss 1.5764\n",
      "Epoch 1 Batch 397 Loss 1.6278\n",
      "Epoch 1 Batch 398 Loss 2.3878\n",
      "Epoch 1 Batch 399 Loss 2.7971\n",
      "Epoch 1 Batch 400 Loss 1.9263\n",
      "Epoch 1 Batch 401 Loss 2.0033\n",
      "Epoch 1 Batch 402 Loss 1.9393\n",
      "Epoch 1 Batch 403 Loss 2.4715\n",
      "Epoch 1 Batch 404 Loss 1.4479\n",
      "Epoch 1 Batch 405 Loss 2.3966\n",
      "Epoch 1 Batch 406 Loss 2.2774\n",
      "Epoch 1 Batch 407 Loss 2.7712\n",
      "Epoch 1 Batch 408 Loss 2.2994\n",
      "Epoch 1 Batch 409 Loss 2.6066\n",
      "Epoch 1 Batch 410 Loss 1.9052\n",
      "Epoch 1 Batch 411 Loss 2.5866\n",
      "Epoch 1 Batch 412 Loss 2.6389\n",
      "Epoch 1 Batch 413 Loss 2.6033\n",
      "Epoch 1 Batch 414 Loss 1.9001\n",
      "Epoch 1 Batch 415 Loss 2.2826\n",
      "Epoch 1 Batch 416 Loss 2.6494\n",
      "Epoch 1 Batch 417 Loss 2.3308\n",
      "Epoch 1 Batch 418 Loss 2.4106\n",
      "Epoch 1 Batch 419 Loss 2.2600\n",
      "Epoch 1 Batch 420 Loss 1.8991\n",
      "Epoch 1 Batch 421 Loss 2.5408\n",
      "Epoch 1 Batch 422 Loss 2.6528\n",
      "Epoch 1 Batch 423 Loss 2.7065\n",
      "Epoch 1 Batch 424 Loss 2.5008\n",
      "Epoch 1 Batch 425 Loss 2.1180\n",
      "Epoch 1 Batch 426 Loss 3.0007\n",
      "Epoch 1 Batch 427 Loss 2.9065\n",
      "Epoch 1 Batch 428 Loss 2.8368\n",
      "Epoch 1 Batch 429 Loss 2.2959\n",
      "Epoch 1 Batch 430 Loss 2.7159\n",
      "Epoch 1 Batch 431 Loss 2.4552\n",
      "Epoch 1 Batch 432 Loss 1.8722\n",
      "Epoch 1 Batch 433 Loss 2.6489\n",
      "Epoch 1 Batch 434 Loss 1.9466\n",
      "Epoch 1 Batch 435 Loss 2.0689\n",
      "Epoch 1 Batch 436 Loss 2.2267\n",
      "Epoch 1 Batch 437 Loss 2.8142\n",
      "Epoch 1 Batch 438 Loss 1.8962\n",
      "Epoch 1 Batch 439 Loss 2.5018\n",
      "Epoch 1 Batch 440 Loss 2.6592\n",
      "Epoch 1 Batch 441 Loss 2.6274\n",
      "Epoch 1 Batch 442 Loss 2.7549\n",
      "Epoch 1 Batch 443 Loss 1.9244\n",
      "Epoch 1 Batch 444 Loss 2.2778\n",
      "Epoch 1 Batch 445 Loss 1.9666\n",
      "Epoch 1 Batch 446 Loss 2.2378\n",
      "Epoch 1 Batch 447 Loss 2.8127\n",
      "Epoch 1 Batch 448 Loss 2.7156\n",
      "Epoch 1 Batch 449 Loss 2.2145\n",
      "Epoch 1 Batch 450 Loss 2.4448\n",
      "Epoch 1 Batch 451 Loss 2.8721\n",
      "Epoch 1 Batch 452 Loss 2.6561\n",
      "Epoch 1 Batch 453 Loss 2.3862\n",
      "Epoch 1 Batch 454 Loss 2.7931\n",
      "Epoch 1 Batch 455 Loss 2.2247\n",
      "Epoch 1 Batch 456 Loss 2.3214\n",
      "Epoch 1 Batch 457 Loss 2.3777\n",
      "Epoch 1 Batch 458 Loss 1.8196\n",
      "Epoch 1 Batch 459 Loss 2.0933\n",
      "Epoch 1 Batch 460 Loss 1.7953\n",
      "Epoch 1 Batch 461 Loss 1.6186\n",
      "Epoch 1 Batch 462 Loss 2.0236\n",
      "Epoch 1 Batch 463 Loss 2.8463\n",
      "Epoch 1 Batch 464 Loss 2.9027\n",
      "Epoch 1 Batch 465 Loss 2.5285\n",
      "Epoch 1 Batch 466 Loss 2.7888\n",
      "Epoch 1 Batch 467 Loss 2.4353\n",
      "Epoch 1 Batch 468 Loss 2.6006\n",
      "Epoch 1 Batch 469 Loss 2.3728\n",
      "Epoch 1 Batch 470 Loss 2.3264\n",
      "Epoch 1 Batch 471 Loss 2.4287\n",
      "Epoch 1 Batch 472 Loss 2.1148\n",
      "Epoch 1 Batch 473 Loss 1.9376\n",
      "Epoch 1 Batch 474 Loss 2.0490\n",
      "Epoch 1 Batch 475 Loss 1.9952\n",
      "Epoch 1 Batch 476 Loss 2.0909\n",
      "Epoch 1 Batch 477 Loss 2.1046\n",
      "Epoch 1 Batch 478 Loss 1.6882\n",
      "Epoch 1 Batch 479 Loss 1.8164\n",
      "Epoch 1 Batch 480 Loss 2.5344\n",
      "Epoch 1 Batch 481 Loss 2.1469\n",
      "Epoch 1 Batch 482 Loss 2.1726\n",
      "Epoch 1 Batch 483 Loss 2.7029\n",
      "Epoch 1 Batch 484 Loss 2.7376\n",
      "Epoch 1 Batch 485 Loss 2.5416\n",
      "Epoch 1 Batch 486 Loss 2.1781\n",
      "Epoch 1 Batch 487 Loss 1.7728\n",
      "Epoch 1 Batch 488 Loss 1.5850\n",
      "Epoch 1 Batch 489 Loss 1.7160\n",
      "Epoch 1 Batch 490 Loss 1.9257\n",
      "Epoch 1 Batch 491 Loss 1.9170\n",
      "Epoch 1 Batch 492 Loss 2.7170\n",
      "Epoch 1 Batch 493 Loss 2.0662\n",
      "Epoch 1 Batch 494 Loss 2.1597\n",
      "Epoch 1 Batch 495 Loss 2.0780\n",
      "Epoch 1 Batch 496 Loss 1.5792\n",
      "Epoch 1 Batch 497 Loss 2.2658\n",
      "Epoch 1 Batch 498 Loss 1.7912\n",
      "Epoch 1 Batch 499 Loss 1.9836\n",
      "Epoch 1 Batch 500 Loss 2.2081\n",
      "Epoch 1 Batch 501 Loss 1.9202\n",
      "Epoch 1 Batch 502 Loss 1.4794\n",
      "Epoch 1 Batch 503 Loss 1.4732\n",
      "Epoch 1 Batch 504 Loss 1.8266\n",
      "Epoch 1 Batch 505 Loss 1.6935\n",
      "Epoch 1 Batch 506 Loss 1.8583\n",
      "Epoch 1 Batch 507 Loss 2.0932\n",
      "Epoch 1 Batch 508 Loss 2.0076\n",
      "Epoch 1 Batch 509 Loss 2.9455\n",
      "Epoch 1 Batch 510 Loss 2.6306\n",
      "Epoch 1 Batch 511 Loss 2.0361\n",
      "Epoch 1 Batch 512 Loss 2.1565\n",
      "Epoch 1 Batch 513 Loss 1.9165\n",
      "Epoch 1 Batch 514 Loss 1.9627\n",
      "Epoch 1 Batch 515 Loss 2.3468\n",
      "Epoch 1 Batch 516 Loss 2.4171\n",
      "Epoch 1 Batch 517 Loss 2.4151\n",
      "Epoch 1 Batch 518 Loss 2.0979\n",
      "Epoch 1 Batch 519 Loss 2.0550\n",
      "Epoch 1 Batch 520 Loss 1.4894\n",
      "Epoch 1 Batch 521 Loss 1.8032\n",
      "Epoch 1 Batch 522 Loss 1.9968\n",
      "Epoch 1 Batch 523 Loss 1.9412\n",
      "Epoch 1 Batch 524 Loss 1.9476\n",
      "Epoch 1 Batch 525 Loss 1.8444\n",
      "Epoch 1 Batch 526 Loss 1.8580\n",
      "Epoch 1 Batch 527 Loss 1.9818\n",
      "Epoch 1 Batch 528 Loss 2.3960\n",
      "Epoch 1 Batch 529 Loss 2.0090\n",
      "Epoch 1 Batch 530 Loss 1.8717\n",
      "Epoch 1 Batch 531 Loss 2.5128\n",
      "Epoch 1 Batch 532 Loss 2.7656\n",
      "Epoch 1 Batch 533 Loss 2.4214\n",
      "Epoch 1 Batch 534 Loss 2.4146\n",
      "Epoch 1 Batch 535 Loss 2.2648\n",
      "Epoch 1 Batch 536 Loss 2.0922\n",
      "Epoch 1 Batch 537 Loss 2.3995\n",
      "Epoch 1 Batch 538 Loss 2.2854\n",
      "Epoch 1 Batch 539 Loss 1.9662\n",
      "Epoch 1 Batch 540 Loss 1.8690\n",
      "Epoch 1 Batch 541 Loss 1.7318\n",
      "Epoch 1 Batch 542 Loss 2.0868\n",
      "Epoch 1 Batch 543 Loss 1.7066\n",
      "Epoch 1 Batch 544 Loss 2.1427\n",
      "Epoch 1 Batch 545 Loss 2.3797\n",
      "Epoch 1 Batch 546 Loss 2.7412\n",
      "Epoch 1 Batch 547 Loss 2.3634\n",
      "Epoch 1 Batch 548 Loss 2.7768\n",
      "Epoch 1 Batch 549 Loss 2.7284\n",
      "Epoch 1 Batch 550 Loss 2.3988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 551 Loss 2.6761\n",
      "Epoch 1 Batch 552 Loss 1.9607\n",
      "Epoch 1 Batch 553 Loss 2.0225\n",
      "Epoch 1 Batch 554 Loss 2.1884\n",
      "Epoch 1 Batch 555 Loss 2.5218\n",
      "Epoch 1 Batch 556 Loss 2.4330\n",
      "Epoch 1 Batch 557 Loss 1.8679\n",
      "Epoch 1 Batch 558 Loss 2.0639\n",
      "Epoch 1 Batch 559 Loss 1.8311\n",
      "Epoch 1 Batch 560 Loss 2.3148\n",
      "Epoch 1 Batch 561 Loss 2.3147\n",
      "Epoch 1 Batch 562 Loss 2.2757\n",
      "Epoch 1 Batch 563 Loss 2.4726\n",
      "Epoch 1 Batch 564 Loss 2.0322\n",
      "Epoch 1 Batch 565 Loss 1.7358\n",
      "Epoch 1 Batch 566 Loss 2.1908\n",
      "Epoch 1 Batch 567 Loss 2.1499\n",
      "Epoch 1 Batch 568 Loss 2.1539\n",
      "Epoch 1 Batch 569 Loss 2.2235\n",
      "Epoch 1 Batch 570 Loss 1.9656\n",
      "Epoch 1 Batch 571 Loss 1.3884\n",
      "Epoch 1 Batch 572 Loss 1.8819\n",
      "Epoch 1 Batch 573 Loss 2.0066\n",
      "Epoch 1 Batch 574 Loss 1.9259\n",
      "Epoch 1 Batch 575 Loss 2.5262\n",
      "Epoch 1 Batch 576 Loss 1.9042\n",
      "Epoch 1 Batch 577 Loss 2.5569\n",
      "Epoch 1 Batch 578 Loss 2.8194\n",
      "Epoch 1 Batch 579 Loss 2.4888\n",
      "Epoch 1 Batch 580 Loss 2.0708\n",
      "Epoch 1 Batch 581 Loss 2.2200\n",
      "Epoch 1 Batch 582 Loss 2.2083\n",
      "Epoch 1 Batch 583 Loss 2.0884\n",
      "Epoch 1 Batch 584 Loss 2.2150\n",
      "Epoch 1 Batch 585 Loss 1.8741\n",
      "Epoch 1 Batch 586 Loss 1.9918\n",
      "Epoch 1 Batch 587 Loss 2.0470\n",
      "Epoch 1 Batch 588 Loss 2.0209\n",
      "Epoch 1 Batch 589 Loss 2.1165\n",
      "Epoch 1 Batch 590 Loss 2.2268\n",
      "Epoch 1 Batch 591 Loss 2.0389\n",
      "Epoch 1 Batch 592 Loss 2.6612\n",
      "Epoch 1 Batch 593 Loss 2.3385\n",
      "Epoch 1 Batch 594 Loss 2.4981\n",
      "Epoch 1 Batch 595 Loss 1.9851\n",
      "Epoch 1 Batch 596 Loss 2.1025\n",
      "Epoch 1 Batch 597 Loss 2.5842\n",
      "Epoch 1 Batch 598 Loss 2.4041\n",
      "Epoch 1 Batch 599 Loss 2.0198\n",
      "Epoch 1 Batch 600 Loss 2.4396\n",
      "Epoch 1 Batch 601 Loss 2.3982\n",
      "Epoch 1 Batch 602 Loss 2.2986\n",
      "Epoch 1 Batch 603 Loss 1.6672\n",
      "Epoch 1 Batch 604 Loss 2.1787\n",
      "Epoch 1 Batch 605 Loss 2.4463\n",
      "Epoch 1 Batch 606 Loss 2.4509\n",
      "Epoch 1 Batch 607 Loss 2.5763\n",
      "Epoch 1 Batch 608 Loss 2.5314\n",
      "Epoch 1 Batch 609 Loss 2.0499\n",
      "Epoch 1 Batch 610 Loss 2.6274\n",
      "Epoch 1 Batch 611 Loss 2.1280\n",
      "Epoch 1 Batch 612 Loss 1.8765\n",
      "Epoch 1 Batch 613 Loss 2.3210\n",
      "Epoch 1 Batch 614 Loss 2.2410\n",
      "Epoch 1 Batch 615 Loss 2.4339\n",
      "Epoch 1 Batch 616 Loss 1.8887\n",
      "Epoch 1 Batch 617 Loss 2.3955\n",
      "Epoch 1 Batch 618 Loss 2.4608\n",
      "Epoch 1 Batch 619 Loss 2.1556\n",
      "Epoch 1 Batch 620 Loss 2.0133\n",
      "Epoch 1 Batch 621 Loss 1.9529\n",
      "Epoch 1 Batch 622 Loss 1.9237\n",
      "Epoch 1 Batch 623 Loss 2.0158\n",
      "Epoch 1 Batch 624 Loss 1.4296\n",
      "Epoch 1 Batch 625 Loss 2.0558\n",
      "Epoch 1 Batch 626 Loss 2.3704\n",
      "Epoch 1 Batch 627 Loss 2.4609\n",
      "Epoch 1 Batch 628 Loss 2.0823\n",
      "Epoch 1 Batch 629 Loss 2.5463\n",
      "Epoch 1 Batch 630 Loss 2.9226\n",
      "Epoch 1 Batch 631 Loss 2.3754\n",
      "Epoch 1 Batch 632 Loss 2.0653\n",
      "Epoch 1 Batch 633 Loss 2.3834\n",
      "Epoch 1 Batch 634 Loss 2.2667\n",
      "Epoch 1 Batch 635 Loss 2.1139\n",
      "Epoch 1 Batch 636 Loss 2.1166\n",
      "Epoch 1 Batch 637 Loss 2.2951\n",
      "Epoch 1 Batch 638 Loss 2.1461\n",
      "Epoch 1 Batch 639 Loss 2.2741\n",
      "Epoch 1 Batch 640 Loss 2.1083\n",
      "Epoch 1 Batch 641 Loss 2.0449\n",
      "Epoch 1 Batch 642 Loss 2.0052\n",
      "Epoch 1 Batch 643 Loss 1.6677\n",
      "Epoch 1 Batch 644 Loss 1.9701\n",
      "Epoch 1 Batch 645 Loss 1.9698\n",
      "Epoch 1 Batch 646 Loss 2.2240\n",
      "Epoch 1 Batch 647 Loss 2.4423\n",
      "Epoch 1 Batch 648 Loss 2.2329\n",
      "Epoch 1 Batch 649 Loss 2.2323\n",
      "Epoch 1 Batch 650 Loss 2.0000\n",
      "Epoch 1 Batch 651 Loss 1.9120\n",
      "Epoch 1 Batch 652 Loss 2.4305\n",
      "Epoch 1 Batch 653 Loss 2.0889\n",
      "Epoch 1 Batch 654 Loss 2.3818\n",
      "Epoch 1 Batch 655 Loss 2.3715\n",
      "Epoch 1 Batch 656 Loss 2.4961\n",
      "Epoch 1 Batch 657 Loss 2.9661\n",
      "Epoch 1 Batch 658 Loss 2.4539\n",
      "Epoch 1 Batch 659 Loss 2.5242\n",
      "Epoch 1 Batch 660 Loss 2.6557\n",
      "Epoch 1 Batch 661 Loss 2.6289\n",
      "Epoch 1 Batch 662 Loss 2.4852\n",
      "Epoch 1 Batch 663 Loss 1.9646\n",
      "Epoch 1 Batch 664 Loss 2.3797\n",
      "Epoch 1 Batch 665 Loss 2.3344\n",
      "Epoch 1 Batch 666 Loss 2.1238\n",
      "Epoch 1 Batch 667 Loss 2.5040\n",
      "Epoch 1 Batch 668 Loss 2.2370\n",
      "Epoch 1 Batch 669 Loss 2.4139\n",
      "Epoch 1 Batch 670 Loss 1.7369\n",
      "Epoch 1 Batch 671 Loss 2.0424\n",
      "Epoch 1 Batch 672 Loss 2.3673\n",
      "Epoch 1 Batch 673 Loss 2.4320\n",
      "Epoch 1 Batch 674 Loss 1.8652\n",
      "Epoch 1 Batch 675 Loss 1.8801\n",
      "Epoch 1 Batch 676 Loss 2.1978\n",
      "Epoch 1 Batch 677 Loss 1.9374\n",
      "Epoch 1 Batch 678 Loss 2.1934\n",
      "Epoch 1 Batch 679 Loss 2.0067\n",
      "Epoch 1 Batch 680 Loss 2.3009\n",
      "Epoch 1 Batch 681 Loss 2.4654\n",
      "Epoch 1 Batch 682 Loss 2.4378\n",
      "Epoch 1 Batch 683 Loss 2.1232\n",
      "Epoch 1 Batch 684 Loss 2.4846\n",
      "Epoch 1 Batch 685 Loss 1.9092\n",
      "Epoch 1 Batch 686 Loss 1.5172\n",
      "Epoch 1 Batch 687 Loss 1.7826\n",
      "Epoch 1 Batch 688 Loss 1.9304\n",
      "Epoch 1 Batch 689 Loss 2.6566\n",
      "Epoch 1 Batch 690 Loss 2.3547\n",
      "Epoch 1 Batch 691 Loss 2.9371\n",
      "Epoch 1 Batch 692 Loss 2.5432\n",
      "Epoch 1 Batch 693 Loss 2.1974\n",
      "Epoch 1 Batch 694 Loss 2.3375\n",
      "Epoch 1 Batch 695 Loss 2.4896\n",
      "Epoch 1 Batch 696 Loss 1.7180\n",
      "Epoch 1 Batch 697 Loss 2.2090\n",
      "Epoch 1 Batch 698 Loss 2.3991\n",
      "Epoch 1 Batch 699 Loss 2.1304\n",
      "Epoch 1 Batch 700 Loss 1.6165\n",
      "Epoch 1 Batch 701 Loss 2.3501\n",
      "Epoch 1 Batch 702 Loss 2.1045\n",
      "Epoch 1 Batch 703 Loss 2.1649\n",
      "Epoch 1 Batch 704 Loss 1.7120\n",
      "Epoch 1 Batch 705 Loss 1.6846\n",
      "Epoch 1 Batch 706 Loss 2.2321\n",
      "Epoch 1 Batch 707 Loss 2.3061\n",
      "Epoch 1 Batch 708 Loss 2.7418\n",
      "Epoch 1 Batch 709 Loss 2.7679\n",
      "Epoch 1 Batch 710 Loss 2.2253\n",
      "Epoch 1 Batch 711 Loss 1.9667\n",
      "Epoch 1 Batch 712 Loss 2.1086\n",
      "Epoch 1 Batch 713 Loss 2.0613\n",
      "Epoch 1 Batch 714 Loss 1.9108\n",
      "Epoch 1 Batch 715 Loss 2.0011\n",
      "Epoch 1 Batch 716 Loss 2.4669\n",
      "Epoch 1 Batch 717 Loss 2.0948\n",
      "Epoch 1 Batch 718 Loss 2.0226\n",
      "Epoch 1 Batch 719 Loss 2.4599\n",
      "Epoch 1 Batch 720 Loss 1.8771\n",
      "Epoch 1 Batch 721 Loss 3.0559\n",
      "Epoch 1 Batch 722 Loss 2.6623\n",
      "Epoch 1 Batch 723 Loss 2.1543\n",
      "Epoch 1 Batch 724 Loss 2.3949\n",
      "Epoch 1 Batch 725 Loss 2.8693\n",
      "Epoch 1 Batch 726 Loss 2.2245\n",
      "Epoch 1 Batch 727 Loss 1.7065\n",
      "Epoch 1 Batch 728 Loss 1.8270\n",
      "Epoch 1 Batch 729 Loss 1.6891\n",
      "Epoch 1 Batch 730 Loss 1.9079\n",
      "Epoch 1 Batch 731 Loss 1.8095\n",
      "Epoch 1 Batch 732 Loss 1.6623\n",
      "Epoch 1 Batch 733 Loss 1.8573\n",
      "Epoch 1 Batch 734 Loss 1.4487\n",
      "Epoch 1 Batch 735 Loss 1.3990\n",
      "Epoch 1 Batch 736 Loss 1.6531\n",
      "Epoch 1 Batch 737 Loss 1.6079\n",
      "Epoch 1 Batch 738 Loss 1.5352\n",
      "Epoch 1 Batch 739 Loss 1.7278\n",
      "Epoch 1 Batch 740 Loss 1.5312\n",
      "Epoch 1 Batch 741 Loss 2.0827\n",
      "Epoch 1 Batch 742 Loss 2.3379\n",
      "Epoch 1 Batch 743 Loss 1.8954\n",
      "Epoch 1 Batch 744 Loss 2.6281\n",
      "Epoch 1 Batch 745 Loss 1.5988\n",
      "Epoch 1 Batch 746 Loss 2.4343\n",
      "Epoch 1 Batch 747 Loss 2.2088\n",
      "Epoch 1 Batch 748 Loss 2.2667\n",
      "Epoch 1 Batch 749 Loss 1.9474\n",
      "Epoch 1 Batch 750 Loss 2.4672\n",
      "Epoch 1 Batch 751 Loss 2.2373\n",
      "Epoch 1 Batch 752 Loss 1.8206\n",
      "Epoch 1 Batch 753 Loss 1.5993\n",
      "Epoch 1 Batch 754 Loss 1.8585\n",
      "Epoch 1 Batch 755 Loss 1.8872\n",
      "Epoch 1 Batch 756 Loss 1.9221\n",
      "Epoch 1 Batch 757 Loss 2.2222\n",
      "Epoch 1 Batch 758 Loss 2.2000\n",
      "Epoch 1 Batch 759 Loss 2.0574\n",
      "Epoch 1 Batch 760 Loss 2.5316\n",
      "Epoch 1 Batch 761 Loss 2.1723\n",
      "Epoch 1 Batch 762 Loss 2.2010\n",
      "Epoch 1 Batch 763 Loss 2.8611\n",
      "Epoch 1 Batch 764 Loss 2.3039\n",
      "Epoch 1 Batch 765 Loss 2.0018\n",
      "Epoch 1 Batch 766 Loss 2.3068\n",
      "Epoch 1 Batch 767 Loss 2.3263\n",
      "Epoch 1 Batch 768 Loss 2.6105\n",
      "Epoch 1 Batch 769 Loss 3.1185\n",
      "Epoch 1 Batch 770 Loss 2.4238\n",
      "Epoch 1 Batch 771 Loss 2.6941\n",
      "Epoch 1 Batch 772 Loss 2.9692\n",
      "Epoch 1 Batch 773 Loss 2.5039\n",
      "Epoch 1 Batch 774 Loss 2.6735\n",
      "Epoch 1 Batch 775 Loss 2.1647\n",
      "Epoch 1 Batch 776 Loss 2.7740\n",
      "Epoch 1 Batch 777 Loss 2.3224\n",
      "Epoch 1 Batch 778 Loss 1.9864\n",
      "Epoch 1 Batch 779 Loss 2.1003\n",
      "Epoch 1 Batch 780 Loss 1.9501\n",
      "Epoch 1 Batch 781 Loss 2.3913\n",
      "Epoch 1 Batch 782 Loss 1.7600\n",
      "Epoch 1 Batch 783 Loss 2.3132\n",
      "Epoch 1 Batch 784 Loss 1.6699\n",
      "Epoch 1 Batch 785 Loss 2.4909\n",
      "Epoch 1 Batch 786 Loss 2.5176\n",
      "Epoch 1 Batch 787 Loss 2.0483\n",
      "Epoch 1 Batch 788 Loss 2.2950\n",
      "Epoch 1 Batch 789 Loss 1.9750\n",
      "Epoch 1 Batch 790 Loss 2.0936\n",
      "Epoch 1 Batch 791 Loss 2.1269\n",
      "Epoch 1 Batch 792 Loss 2.0863\n",
      "Epoch 1 Batch 793 Loss 1.6878\n",
      "Epoch 1 Batch 794 Loss 1.2262\n",
      "Epoch 1 Batch 795 Loss 1.9444\n",
      "Epoch 1 Batch 796 Loss 1.7576\n",
      "Epoch 1 Batch 797 Loss 1.7189\n",
      "Epoch 1 Batch 798 Loss 1.6714\n",
      "Epoch 1 Batch 799 Loss 1.7010\n",
      "Epoch 1 Batch 800 Loss 2.0454\n",
      "Epoch 1 Batch 801 Loss 2.2655\n",
      "Epoch 1 Batch 802 Loss 2.0986\n",
      "Epoch 1 Batch 803 Loss 1.8292\n",
      "Epoch 1 Batch 804 Loss 1.7255\n",
      "Epoch 1 Batch 805 Loss 1.5283\n",
      "Epoch 1 Batch 806 Loss 2.0569\n",
      "Epoch 1 Batch 807 Loss 2.5772\n",
      "Epoch 1 Batch 808 Loss 2.2636\n",
      "Epoch 1 Batch 809 Loss 2.3521\n",
      "Epoch 1 Batch 810 Loss 2.3567\n",
      "Epoch 1 Batch 811 Loss 2.3847\n",
      "Epoch 1 Batch 812 Loss 1.9047\n",
      "Epoch 1 Batch 813 Loss 2.2927\n",
      "Epoch 1 Batch 814 Loss 2.2046\n",
      "Epoch 1 Batch 815 Loss 2.5850\n",
      "Epoch 1 Batch 816 Loss 2.3636\n",
      "Epoch 1 Batch 817 Loss 2.6072\n",
      "Epoch 1 Batch 818 Loss 2.5278\n",
      "Epoch 1 Batch 819 Loss 2.6267\n",
      "Epoch 1 Batch 820 Loss 2.7244\n",
      "Epoch 1 Batch 821 Loss 1.8743\n",
      "Epoch 1 Batch 822 Loss 1.9230\n",
      "Epoch 1 Batch 823 Loss 1.8877\n",
      "Epoch 1 Batch 824 Loss 2.0914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 825 Loss 1.9037\n",
      "Epoch 1 Batch 826 Loss 1.8563\n",
      "Epoch 1 Batch 827 Loss 2.2698\n",
      "Epoch 1 Batch 828 Loss 2.5655\n",
      "Epoch 1 Batch 829 Loss 2.5596\n",
      "Epoch 1 Batch 830 Loss 2.7183\n",
      "Epoch 1 Batch 831 Loss 2.0226\n",
      "Epoch 1 Batch 832 Loss 2.5239\n",
      "Epoch 1 Batch 833 Loss 2.3136\n",
      "Epoch 1 Batch 834 Loss 1.6843\n",
      "Epoch 1 Batch 835 Loss 1.6242\n",
      "Epoch 1 Batch 836 Loss 1.9309\n",
      "Epoch 1 Batch 837 Loss 2.4837\n",
      "Epoch 1 Batch 838 Loss 1.6524\n",
      "Epoch 1 Batch 839 Loss 2.0247\n",
      "Epoch 1 Batch 840 Loss 2.6685\n",
      "Epoch 1 Batch 841 Loss 1.9771\n",
      "Epoch 1 Batch 842 Loss 2.3563\n",
      "Epoch 1 Batch 843 Loss 1.8486\n",
      "Epoch 1 Batch 844 Loss 2.1735\n",
      "Epoch 1 Batch 845 Loss 2.1388\n",
      "Epoch 1 Batch 846 Loss 1.9321\n",
      "Epoch 1 Batch 847 Loss 2.4017\n",
      "Epoch 1 Batch 848 Loss 1.9556\n",
      "Epoch 1 Batch 849 Loss 2.3698\n",
      "Epoch 1 Batch 850 Loss 1.8454\n",
      "Epoch 1 Batch 851 Loss 1.6663\n",
      "Epoch 1 Batch 852 Loss 2.5039\n",
      "Epoch 1 Batch 853 Loss 2.2818\n",
      "Epoch 1 Batch 854 Loss 1.8737\n",
      "Epoch 1 Batch 855 Loss 1.7481\n",
      "Epoch 1 Batch 856 Loss 2.0495\n",
      "Epoch 1 Batch 857 Loss 1.8471\n",
      "Epoch 1 Batch 858 Loss 2.1485\n",
      "Epoch 1 Batch 859 Loss 2.1852\n",
      "Epoch 1 Batch 860 Loss 2.2605\n",
      "Epoch 1 Batch 861 Loss 2.5214\n",
      "Epoch 1 Batch 862 Loss 1.9073\n",
      "Epoch 1 Batch 863 Loss 2.4102\n",
      "Epoch 1 Batch 864 Loss 2.2511\n",
      "Epoch 1 Batch 865 Loss 2.0778\n",
      "Epoch 1 Batch 866 Loss 2.7054\n",
      "Epoch 1 Batch 867 Loss 2.1890\n",
      "Epoch 1 Batch 868 Loss 2.1280\n",
      "Epoch 1 Batch 869 Loss 2.0747\n",
      "Epoch 1 Batch 870 Loss 2.1332\n",
      "Epoch 1 Batch 871 Loss 1.9047\n",
      "Epoch 1 Batch 872 Loss 1.6532\n",
      "Epoch 1 Batch 873 Loss 1.7630\n",
      "Epoch 1 Batch 874 Loss 1.5971\n",
      "Epoch 1 Batch 875 Loss 1.5441\n",
      "Epoch 1 Batch 876 Loss 1.7611\n",
      "Epoch 1 Batch 877 Loss 2.1850\n",
      "Epoch 1 Batch 878 Loss 1.7381\n",
      "Epoch 1 Batch 879 Loss 2.3057\n",
      "Epoch 1 Batch 880 Loss 2.0577\n",
      "Epoch 1 Batch 881 Loss 1.6619\n",
      "Epoch 1 Batch 882 Loss 2.0457\n",
      "Epoch 1 Batch 883 Loss 2.1269\n",
      "Epoch 1 Batch 884 Loss 2.1179\n",
      "Epoch 1 Batch 885 Loss 2.1927\n",
      "Epoch 1 Batch 886 Loss 2.1182\n",
      "Epoch 1 Batch 887 Loss 1.9205\n",
      "Epoch 1 Batch 888 Loss 1.6619\n",
      "Epoch 1 Batch 889 Loss 1.8494\n",
      "Epoch 1 Batch 890 Loss 1.6459\n",
      "Epoch 1 Batch 891 Loss 1.9235\n",
      "Epoch 1 Batch 892 Loss 1.8717\n",
      "Epoch 1 Batch 893 Loss 1.7009\n",
      "Epoch 1 Batch 894 Loss 1.5259\n",
      "Epoch 1 Batch 895 Loss 1.5554\n",
      "Epoch 1 Batch 896 Loss 2.0680\n",
      "Epoch 1 Batch 897 Loss 2.1123\n",
      "Epoch 1 Batch 898 Loss 2.3779\n",
      "Epoch 1 Batch 899 Loss 1.5815\n",
      "Epoch 1 Batch 900 Loss 1.8825\n",
      "Epoch 1 Batch 901 Loss 1.8388\n",
      "Epoch 1 Batch 902 Loss 1.9998\n",
      "Epoch 1 Batch 903 Loss 1.9168\n",
      "Epoch 1 Batch 904 Loss 2.4896\n",
      "Epoch 1 Batch 905 Loss 2.4734\n",
      "Epoch 1 Batch 906 Loss 2.7840\n",
      "Epoch 1 Batch 907 Loss 2.5803\n",
      "Epoch 1 Batch 908 Loss 2.4340\n",
      "Epoch 1 Batch 909 Loss 2.4820\n",
      "Epoch 1 Batch 910 Loss 1.8346\n",
      "Epoch 1 Batch 911 Loss 2.2425\n",
      "Epoch 1 Batch 912 Loss 2.4125\n",
      "Epoch 1 Batch 913 Loss 2.4608\n",
      "Epoch 1 Batch 914 Loss 2.2669\n",
      "Epoch 1 Batch 915 Loss 2.1855\n",
      "Epoch 1 Batch 916 Loss 2.1356\n",
      "Epoch 1 Batch 917 Loss 2.0718\n",
      "Epoch 1 Batch 918 Loss 2.5282\n",
      "Epoch 1 Batch 919 Loss 2.4161\n",
      "Epoch 1 Batch 920 Loss 2.1403\n",
      "Epoch 1 Batch 921 Loss 2.2241\n",
      "Epoch 1 Batch 922 Loss 2.2192\n",
      "Epoch 1 Batch 923 Loss 2.3593\n",
      "Epoch 1 Batch 924 Loss 1.9174\n",
      "Epoch 1 Batch 925 Loss 2.5176\n",
      "Epoch 1 Batch 926 Loss 2.4485\n",
      "Epoch 1 Batch 927 Loss 2.4772\n",
      "Epoch 1 Batch 928 Loss 2.5051\n",
      "Epoch 1 Batch 929 Loss 2.3908\n",
      "Epoch 1 Batch 930 Loss 2.6918\n",
      "Epoch 1 Batch 931 Loss 2.2110\n",
      "Epoch 1 Batch 932 Loss 2.3556\n",
      "Epoch 1 Batch 933 Loss 2.5283\n",
      "Epoch 1 Batch 934 Loss 2.3106\n",
      "Epoch 1 Batch 935 Loss 2.5786\n",
      "Epoch 1 Batch 936 Loss 2.3880\n",
      "Epoch 1 Batch 937 Loss 2.7775\n",
      "Epoch 1 Batch 938 Loss 2.4932\n",
      "Epoch 1 Batch 939 Loss 2.1462\n",
      "Epoch 1 Batch 940 Loss 2.0680\n",
      "Epoch 1 Batch 941 Loss 2.1200\n",
      "Epoch 1 Batch 942 Loss 1.8837\n",
      "Epoch 1 Batch 943 Loss 1.6261\n",
      "Epoch 1 Batch 944 Loss 1.8954\n",
      "Epoch 1 Batch 945 Loss 1.7056\n",
      "Epoch 1 Batch 946 Loss 2.4142\n",
      "Epoch 1 Batch 947 Loss 3.0857\n",
      "Epoch 1 Batch 948 Loss 1.6661\n",
      "Epoch 1 Batch 949 Loss 2.3532\n",
      "Epoch 1 Batch 950 Loss 2.2513\n",
      "Epoch 1 Batch 951 Loss 2.1347\n",
      "Epoch 1 Batch 952 Loss 1.9504\n",
      "Epoch 1 Batch 953 Loss 2.0904\n",
      "Epoch 1 Batch 954 Loss 1.9070\n",
      "Epoch 1 Batch 955 Loss 2.0872\n",
      "Epoch 1 Batch 956 Loss 2.7082\n",
      "Epoch 1 Batch 957 Loss 2.1588\n",
      "Epoch 1 Batch 958 Loss 2.1564\n",
      "Epoch 1 Batch 959 Loss 2.4487\n",
      "Epoch 1 Batch 960 Loss 1.9826\n",
      "Epoch 1 Batch 961 Loss 1.9176\n",
      "Epoch 1 Batch 962 Loss 2.2671\n",
      "Epoch 1 Batch 963 Loss 2.1324\n",
      "Epoch 1 Batch 964 Loss 2.2981\n",
      "Epoch 1 Batch 965 Loss 2.3023\n",
      "Epoch 1 Batch 966 Loss 2.7833\n",
      "Epoch 1 Batch 967 Loss 2.4628\n",
      "Epoch 1 Batch 968 Loss 2.2363\n",
      "Epoch 1 Batch 969 Loss 1.6853\n",
      "Epoch 1 Batch 970 Loss 1.9633\n",
      "Epoch 1 Batch 971 Loss 2.0664\n",
      "Epoch 1 Batch 972 Loss 2.0263\n",
      "Epoch 1 Batch 973 Loss 2.1539\n",
      "Epoch 1 Batch 974 Loss 2.3927\n",
      "Epoch 1 Batch 975 Loss 1.4656\n",
      "Epoch 1 Batch 976 Loss 1.4572\n",
      "Epoch 1 Batch 977 Loss 1.7885\n",
      "Epoch 1 Batch 978 Loss 1.7332\n",
      "Epoch 1 Batch 979 Loss 2.2011\n",
      "Epoch 1 Batch 980 Loss 1.8124\n",
      "Epoch 1 Batch 981 Loss 1.6374\n",
      "Epoch 1 Batch 982 Loss 1.8361\n",
      "Epoch 1 Batch 983 Loss 2.2314\n",
      "Epoch 1 Batch 984 Loss 2.0137\n",
      "Epoch 1 Batch 985 Loss 2.1911\n",
      "Epoch 1 Batch 986 Loss 2.4801\n",
      "Epoch 1 Batch 987 Loss 2.0455\n",
      "Epoch 1 Batch 988 Loss 1.9282\n",
      "Epoch 1 Batch 989 Loss 1.9357\n",
      "Epoch 1 Batch 990 Loss 2.2021\n",
      "Epoch 1 Batch 991 Loss 2.2474\n",
      "Epoch 1 Batch 992 Loss 1.8954\n",
      "Epoch 1 Batch 993 Loss 2.0554\n",
      "Epoch 1 Batch 994 Loss 1.8405\n",
      "Epoch 1 Batch 995 Loss 1.5862\n",
      "Epoch 1 Batch 996 Loss 1.7396\n",
      "Epoch 1 Batch 997 Loss 1.9673\n",
      "Epoch 1 Batch 998 Loss 2.3793\n",
      "Epoch 1 Batch 999 Loss 2.3224\n",
      "Epoch 1 Batch 1000 Loss 2.1753\n",
      "Epoch 1 Batch 1001 Loss 2.1821\n",
      "Epoch 1 Batch 1002 Loss 1.6306\n",
      "Epoch 1 Batch 1003 Loss 2.3688\n",
      "Epoch 1 Batch 1004 Loss 2.2750\n",
      "Epoch 1 Batch 1005 Loss 2.1907\n",
      "Epoch 1 Batch 1006 Loss 1.9526\n",
      "Epoch 1 Batch 1007 Loss 2.1936\n",
      "Epoch 1 Batch 1008 Loss 2.2863\n",
      "Epoch 1 Batch 1009 Loss 1.9018\n",
      "Epoch 1 Batch 1010 Loss 1.7805\n",
      "Epoch 1 Batch 1011 Loss 2.1934\n",
      "Epoch 1 Batch 1012 Loss 2.1617\n",
      "Epoch 1 Batch 1013 Loss 2.1443\n",
      "Epoch 1 Batch 1014 Loss 2.2691\n",
      "Epoch 1 Batch 1015 Loss 2.2684\n",
      "Epoch 1 Batch 1016 Loss 2.2201\n",
      "Epoch 1 Batch 1017 Loss 1.5975\n",
      "Epoch 1 Batch 1018 Loss 2.1646\n",
      "Epoch 1 Batch 1019 Loss 1.8117\n",
      "Epoch 1 Batch 1020 Loss 1.5887\n",
      "Epoch 1 Batch 1021 Loss 1.8393\n",
      "Epoch 1 Batch 1022 Loss 2.4652\n",
      "Epoch 1 Batch 1023 Loss 2.3162\n",
      "Epoch 1 Batch 1024 Loss 2.5310\n",
      "Epoch 1 Batch 1025 Loss 2.0671\n",
      "Epoch 1 Batch 1026 Loss 1.9058\n",
      "Epoch 1 Batch 1027 Loss 1.8308\n",
      "Epoch 1 Batch 1028 Loss 2.1511\n",
      "Epoch 1 Batch 1029 Loss 1.6281\n",
      "Epoch 1 Batch 1030 Loss 1.8898\n",
      "Epoch 1 Batch 1031 Loss 1.9555\n",
      "Epoch 1 Batch 1032 Loss 2.0832\n",
      "Epoch 1 Batch 1033 Loss 2.0683\n",
      "Epoch 1 Batch 1034 Loss 1.9808\n",
      "Epoch 1 Batch 1035 Loss 1.7252\n",
      "Epoch 1 Batch 1036 Loss 1.8022\n",
      "Epoch 1 Batch 1037 Loss 1.8933\n",
      "Epoch 1 Batch 1038 Loss 2.2095\n",
      "Epoch 1 Batch 1039 Loss 2.2001\n",
      "Epoch 1 Batch 1040 Loss 2.0790\n",
      "Epoch 1 Batch 1041 Loss 1.3883\n",
      "Epoch 1 Batch 1042 Loss 1.4805\n",
      "Epoch 1 Batch 1043 Loss 1.4197\n",
      "Epoch 1 Batch 1044 Loss 1.5109\n",
      "Epoch 1 Batch 1045 Loss 1.7626\n",
      "Epoch 1 Batch 1046 Loss 1.7289\n",
      "Epoch 1 Batch 1047 Loss 1.8309\n",
      "Epoch 1 Batch 1048 Loss 1.9618\n",
      "Epoch 1 Batch 1049 Loss 2.3262\n",
      "Epoch 1 Batch 1050 Loss 2.0030\n",
      "Epoch 1 Batch 1051 Loss 2.8812\n",
      "Epoch 1 Batch 1052 Loss 2.1286\n",
      "Epoch 1 Batch 1053 Loss 2.6002\n",
      "Epoch 1 Batch 1054 Loss 2.2467\n",
      "Epoch 1 Batch 1055 Loss 2.0215\n",
      "Epoch 1 Batch 1056 Loss 1.9781\n",
      "Epoch 1 Batch 1057 Loss 2.0094\n",
      "Epoch 1 Batch 1058 Loss 1.6335\n",
      "Epoch 1 Batch 1059 Loss 2.0608\n",
      "Epoch 1 Batch 1060 Loss 1.8386\n",
      "Epoch 1 Batch 1061 Loss 2.3763\n",
      "Epoch 1 Batch 1062 Loss 1.6591\n",
      "Epoch 1 Batch 1063 Loss 1.6471\n",
      "Epoch 1 Batch 1064 Loss 1.6650\n",
      "Epoch 1 Batch 1065 Loss 1.8307\n",
      "Epoch 1 Batch 1066 Loss 2.0017\n",
      "Epoch 1 Batch 1067 Loss 1.7509\n",
      "Epoch 1 Batch 1068 Loss 1.6257\n",
      "Epoch 1 Batch 1069 Loss 1.8339\n",
      "Epoch 1 Batch 1070 Loss 1.5280\n",
      "Epoch 1 Batch 1071 Loss 2.2054\n",
      "Epoch 1 Batch 1072 Loss 1.8182\n",
      "Epoch 1 Batch 1073 Loss 2.2811\n",
      "Epoch 1 Batch 1074 Loss 2.5597\n",
      "Epoch 1 Batch 1075 Loss 2.0806\n",
      "Epoch 1 Batch 1076 Loss 2.1776\n",
      "Epoch 1 Batch 1077 Loss 2.0864\n",
      "Epoch 1 Batch 1078 Loss 1.6149\n",
      "Epoch 1 Batch 1079 Loss 1.7271\n",
      "Epoch 1 Batch 1080 Loss 2.3170\n",
      "Epoch 1 Batch 1081 Loss 2.0692\n",
      "Epoch 1 Batch 1082 Loss 2.3308\n",
      "Epoch 1 Batch 1083 Loss 2.1783\n",
      "Epoch 1 Batch 1084 Loss 1.9290\n",
      "Epoch 1 Batch 1085 Loss 1.6871\n",
      "Epoch 1 Batch 1086 Loss 2.3903\n",
      "Epoch 1 Batch 1087 Loss 1.8509\n",
      "Epoch 1 Batch 1088 Loss 2.3135\n",
      "Epoch 1 Batch 1089 Loss 2.5632\n",
      "Epoch 1 Batch 1090 Loss 2.2792\n",
      "Epoch 1 Batch 1091 Loss 2.0165\n",
      "Epoch 1 Batch 1092 Loss 2.0790\n",
      "Epoch 1 Batch 1093 Loss 2.4482\n",
      "Epoch 1 Batch 1094 Loss 2.6942\n",
      "Epoch 1 Batch 1095 Loss 2.6370\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 1096 Loss 1.7915\n",
      "Epoch 1 Batch 1097 Loss 2.6636\n",
      "Epoch 1 Batch 1098 Loss 2.0838\n",
      "Epoch 1 Batch 1099 Loss 1.6486\n",
      "Epoch 1 Batch 1100 Loss 1.9225\n",
      "Epoch 1 Batch 1101 Loss 2.3268\n",
      "Epoch 1 Batch 1102 Loss 2.0157\n",
      "Epoch 1 Batch 1103 Loss 1.8252\n",
      "Epoch 1 Batch 1104 Loss 1.6602\n",
      "Epoch 1 Batch 1105 Loss 2.9506\n",
      "Epoch 1 Batch 1106 Loss 2.1040\n",
      "Epoch 1 Batch 1107 Loss 2.4910\n",
      "Epoch 1 Batch 1108 Loss 2.3540\n",
      "Epoch 1 Batch 1109 Loss 2.4920\n",
      "Epoch 1 Batch 1110 Loss 2.1440\n",
      "Epoch 1 Batch 1111 Loss 2.1032\n",
      "Epoch 1 Batch 1112 Loss 2.1167\n",
      "Epoch 1 Batch 1113 Loss 2.0426\n",
      "Epoch 1 Batch 1114 Loss 1.8764\n",
      "Epoch 1 Batch 1115 Loss 1.9492\n",
      "Epoch 1 Batch 1116 Loss 1.5873\n",
      "Epoch 1 Batch 1117 Loss 2.4339\n",
      "Epoch 1 Batch 1118 Loss 2.0819\n",
      "Epoch 1 Batch 1119 Loss 2.4672\n",
      "Epoch 1 Batch 1120 Loss 2.6624\n",
      "Epoch 1 Batch 1121 Loss 2.3443\n",
      "Epoch 1 Batch 1122 Loss 2.2766\n",
      "Epoch 1 Batch 1123 Loss 1.8444\n",
      "Epoch 1 Batch 1124 Loss 1.6928\n",
      "Epoch 1 Batch 1125 Loss 3.0692\n",
      "Epoch 1 Batch 1126 Loss 2.2562\n",
      "Epoch 1 Batch 1127 Loss 2.1034\n",
      "Epoch 1 Batch 1128 Loss 2.2684\n",
      "Epoch 1 Batch 1129 Loss 2.2325\n",
      "Epoch 1 Batch 1130 Loss 2.3421\n",
      "Epoch 1 Batch 1131 Loss 2.2837\n",
      "Epoch 1 Batch 1132 Loss 2.4888\n",
      "Epoch 1 Batch 1133 Loss 2.2899\n",
      "Epoch 1 Batch 1134 Loss 2.9338\n",
      "Epoch 1 Batch 1135 Loss 2.0178\n",
      "Epoch 1 Batch 1136 Loss 2.4411\n",
      "Epoch 1 Batch 1137 Loss 2.0996\n",
      "Epoch 1 Batch 1138 Loss 2.1788\n",
      "Epoch 1 Batch 1139 Loss 2.2726\n",
      "Epoch 1 Batch 1140 Loss 2.3434\n",
      "Epoch 1 Batch 1141 Loss 2.6729\n",
      "Epoch 1 Batch 1142 Loss 2.2471\n",
      "Epoch 1 Batch 1143 Loss 1.5493\n",
      "Epoch 1 Batch 1144 Loss 1.5791\n",
      "Epoch 1 Batch 1145 Loss 2.0043\n",
      "Epoch 1 Batch 1146 Loss 1.4873\n",
      "Epoch 1 Batch 1147 Loss 2.0042\n",
      "Epoch 1 Batch 1148 Loss 1.5854\n",
      "Epoch 1 Batch 1149 Loss 1.9001\n",
      "Epoch 1 Batch 1150 Loss 2.3566\n",
      "Epoch 1 Batch 1151 Loss 2.0163\n",
      "Epoch 1 Batch 1152 Loss 1.5560\n",
      "Epoch 1 Batch 1153 Loss 2.7186\n",
      "Epoch 1 Batch 1154 Loss 2.7398\n",
      "Epoch 1 Batch 1155 Loss 2.6353\n",
      "Epoch 1 Batch 1156 Loss 2.5582\n",
      "Epoch 1 Batch 1157 Loss 2.1162\n",
      "Epoch 1 Batch 1158 Loss 2.0020\n",
      "Epoch 1 Batch 1159 Loss 2.3616\n",
      "Epoch 1 Batch 1160 Loss 1.6016\n",
      "Epoch 1 Batch 1161 Loss 1.7845\n",
      "Epoch 1 Batch 1162 Loss 1.6809\n",
      "Epoch 1 Batch 1163 Loss 2.3139\n",
      "Epoch 1 Batch 1164 Loss 2.2263\n",
      "Epoch 1 Batch 1165 Loss 2.1806\n",
      "Epoch 1 Batch 1166 Loss 2.1229\n",
      "Epoch 1 Batch 1167 Loss 2.0083\n",
      "Epoch 1 Batch 1168 Loss 2.4001\n",
      "Epoch 1 Batch 1169 Loss 1.8312\n",
      "Epoch 1 Batch 1170 Loss 1.5235\n",
      "Epoch 1 Batch 1171 Loss 1.8224\n",
      "Epoch 1 Batch 1172 Loss 1.5921\n",
      "Epoch 1 Batch 1173 Loss 1.8781\n",
      "Epoch 1 Batch 1174 Loss 1.8523\n",
      "Epoch 1 Batch 1175 Loss 1.2585\n",
      "Epoch 1 Batch 1176 Loss 1.4611\n",
      "Epoch 1 Batch 1177 Loss 1.6720\n",
      "Epoch 1 Batch 1178 Loss 1.4523\n",
      "Epoch 1 Batch 1179 Loss 2.0314\n",
      "Epoch 1 Batch 1180 Loss 1.6481\n",
      "Epoch 1 Batch 1181 Loss 2.2672\n",
      "Epoch 1 Batch 1182 Loss 2.0717\n",
      "Epoch 1 Batch 1183 Loss 2.0831\n",
      "Epoch 1 Batch 1184 Loss 2.2035\n",
      "Epoch 1 Batch 1185 Loss 1.8840\n",
      "Epoch 1 Batch 1186 Loss 2.0586\n",
      "Epoch 1 Batch 1187 Loss 2.0350\n",
      "Epoch 1 Batch 1188 Loss 2.4992\n",
      "Epoch 1 Batch 1189 Loss 2.2218\n",
      "Epoch 1 Batch 1190 Loss 2.4525\n",
      "Epoch 1 Batch 1191 Loss 2.4375\n",
      "Epoch 1 Batch 1192 Loss 2.2487\n",
      "Epoch 1 Batch 1193 Loss 2.5199\n",
      "Epoch 1 Batch 1194 Loss 1.9827\n",
      "Epoch 1 Batch 1195 Loss 2.2582\n",
      "Epoch 1 Batch 1196 Loss 1.7941\n",
      "Epoch 1 Batch 1197 Loss 1.9565\n",
      "Epoch 1 Batch 1198 Loss 1.7856\n",
      "Epoch 1 Batch 1199 Loss 1.2761\n",
      "Epoch 1 Batch 1200 Loss 1.6342\n",
      "Epoch 1 Batch 1201 Loss 2.1562\n",
      "Epoch 1 Batch 1202 Loss 2.1303\n",
      "Epoch 1 Batch 1203 Loss 2.0211\n",
      "Epoch 1 Batch 1204 Loss 2.2338\n",
      "Epoch 1 Batch 1205 Loss 1.8412\n",
      "Epoch 1 Batch 1206 Loss 1.6754\n",
      "Epoch 1 Batch 1207 Loss 1.9448\n",
      "Epoch 1 Batch 1208 Loss 2.3632\n",
      "Epoch 1 Batch 1209 Loss 1.8825\n",
      "Epoch 1 Batch 1210 Loss 2.0235\n",
      "Epoch 1 Batch 1211 Loss 2.3324\n",
      "Epoch 1 Batch 1212 Loss 1.7113\n",
      "Epoch 1 Batch 1213 Loss 2.2070\n",
      "Epoch 1 Batch 1214 Loss 1.8982\n",
      "Epoch 1 Batch 1215 Loss 2.2534\n",
      "Epoch 1 Batch 1216 Loss 2.2073\n",
      "Epoch 1 Batch 1217 Loss 1.9701\n",
      "Epoch 1 Batch 1218 Loss 1.5825\n",
      "Epoch 1 Batch 1219 Loss 2.2975\n",
      "Epoch 1 Batch 1220 Loss 1.6358\n",
      "Epoch 1 Batch 1221 Loss 2.2498\n",
      "Epoch 1 Batch 1222 Loss 1.6274\n",
      "Epoch 1 Batch 1223 Loss 1.7744\n",
      "Epoch 1 Batch 1224 Loss 2.1949\n",
      "Epoch 1 Batch 1225 Loss 2.3197\n",
      "Epoch 1 Batch 1226 Loss 1.9588\n",
      "Epoch 1 Batch 1227 Loss 2.1111\n",
      "Epoch 1 Batch 1228 Loss 1.9131\n",
      "Epoch 1 Batch 1229 Loss 2.2239\n",
      "Epoch 1 Batch 1230 Loss 2.0406\n",
      "Epoch 1 Batch 1231 Loss 2.5755\n",
      "Epoch 1 Batch 1232 Loss 2.4423\n",
      "Epoch 1 Batch 1233 Loss 2.5398\n",
      "Epoch 1 Batch 1234 Loss 2.1700\n",
      "Epoch 1 Batch 1235 Loss 1.9207\n",
      "Epoch 1 Batch 1236 Loss 2.1888\n",
      "Epoch 1 Batch 1237 Loss 2.0803\n",
      "Epoch 1 Batch 1238 Loss 1.9349\n",
      "Epoch 1 Batch 1239 Loss 1.8882\n",
      "Epoch 1 Batch 1240 Loss 2.0309\n",
      "Epoch 1 Batch 1241 Loss 1.9974\n",
      "Epoch 1 Batch 1242 Loss 1.8143\n",
      "Epoch 1 Batch 1243 Loss 1.9302\n",
      "Epoch 1 Batch 1244 Loss 2.2240\n",
      "Epoch 1 Batch 1245 Loss 1.9995\n",
      "Epoch 1 Batch 1246 Loss 2.2320\n",
      "Epoch 1 Batch 1247 Loss 1.8842\n",
      "Epoch 1 Batch 1248 Loss 2.0403\n",
      "Epoch 1 Batch 1249 Loss 2.0883\n",
      "Epoch 1 Batch 1250 Loss 1.8169\n",
      "Epoch 1 Batch 1251 Loss 1.9865\n",
      "Epoch 1 Batch 1252 Loss 2.1368\n",
      "Epoch 1 Batch 1253 Loss 2.6190\n",
      "Epoch 1 Batch 1254 Loss 2.1046\n",
      "Epoch 1 Batch 1255 Loss 2.1141\n",
      "Epoch 1 Batch 1256 Loss 1.9257\n",
      "Epoch 1 Batch 1257 Loss 1.7113\n",
      "Epoch 1 Batch 1258 Loss 2.0773\n",
      "Epoch 1 Batch 1259 Loss 2.2148\n",
      "Epoch 1 Batch 1260 Loss 1.6466\n",
      "Epoch 1 Batch 1261 Loss 2.3562\n",
      "Epoch 1 Batch 1262 Loss 2.1559\n",
      "Epoch 1 Batch 1263 Loss 2.3091\n",
      "Epoch 1 Batch 1264 Loss 2.3522\n",
      "Epoch 1 Batch 1265 Loss 1.9998\n",
      "Epoch 1 Batch 1266 Loss 2.1135\n",
      "Epoch 1 Batch 1267 Loss 2.1440\n",
      "Epoch 1 Batch 1268 Loss 2.5417\n",
      "Epoch 1 Batch 1269 Loss 2.6776\n",
      "Epoch 1 Batch 1270 Loss 2.2647\n",
      "Epoch 1 Batch 1271 Loss 2.0909\n",
      "Epoch 1 Batch 1272 Loss 2.3694\n",
      "Epoch 1 Batch 1273 Loss 3.0003\n",
      "Epoch 1 Batch 1274 Loss 2.2050\n",
      "Epoch 1 Batch 1275 Loss 2.4864\n",
      "Epoch 1 Batch 1276 Loss 2.4051\n",
      "Epoch 1 Batch 1277 Loss 2.0636\n",
      "Epoch 1 Batch 1278 Loss 1.9872\n",
      "Epoch 1 Batch 1279 Loss 1.9829\n",
      "Epoch 1 Batch 1280 Loss 2.4787\n",
      "Epoch 1 Batch 1281 Loss 2.3732\n",
      "Epoch 1 Batch 1282 Loss 2.0106\n",
      "Epoch 1 Batch 1283 Loss 1.5156\n",
      "Epoch 1 Batch 1284 Loss 1.9395\n",
      "Epoch 1 Batch 1285 Loss 1.8235\n",
      "Epoch 1 Batch 1286 Loss 2.2590\n",
      "Epoch 1 Batch 1287 Loss 1.5386\n",
      "Epoch 1 Batch 1288 Loss 1.7256\n",
      "Epoch 1 Batch 1289 Loss 1.9501\n",
      "Epoch 1 Batch 1290 Loss 2.4260\n",
      "Epoch 1 Batch 1291 Loss 1.3220\n",
      "Epoch 1 Batch 1292 Loss 1.6945\n",
      "Epoch 1 Batch 1293 Loss 1.3324\n",
      "Epoch 1 Batch 1294 Loss 1.7523\n",
      "Epoch 1 Batch 1295 Loss 1.4678\n",
      "Epoch 1 Batch 1296 Loss 1.1890\n",
      "Epoch 1 Batch 1297 Loss 1.3243\n",
      "Epoch 1 Batch 1298 Loss 1.4899\n",
      "Epoch 1 Batch 1299 Loss 1.4475\n",
      "Epoch 1 Batch 1300 Loss 1.5578\n",
      "Epoch 1 Batch 1301 Loss 1.4632\n",
      "Epoch 1 Batch 1302 Loss 1.4195\n",
      "Epoch 1 Batch 1303 Loss 1.2910\n",
      "Epoch 1 Batch 1304 Loss 1.7994\n",
      "Epoch 1 Batch 1305 Loss 1.6660\n",
      "Epoch 1 Batch 1306 Loss 1.7861\n",
      "Epoch 1 Batch 1307 Loss 1.9230\n",
      "Epoch 1 Batch 1308 Loss 2.3181\n",
      "Epoch 1 Batch 1309 Loss 2.3820\n",
      "Epoch 1 Batch 1310 Loss 2.4322\n",
      "Epoch 1 Batch 1311 Loss 1.8641\n",
      "Epoch 1 Batch 1312 Loss 1.6421\n",
      "Epoch 1 Batch 1313 Loss 1.5942\n",
      "Epoch 1 Batch 1314 Loss 1.4860\n",
      "Epoch 1 Batch 1315 Loss 1.8896\n",
      "Epoch 1 Batch 1316 Loss 1.4098\n",
      "Epoch 1 Batch 1317 Loss 1.8177\n",
      "Epoch 1 Batch 1318 Loss 2.0974\n",
      "Epoch 1 Batch 1319 Loss 1.9769\n",
      "Epoch 1 Batch 1320 Loss 1.8518\n",
      "Epoch 1 Batch 1321 Loss 1.8444\n",
      "Epoch 1 Batch 1322 Loss 2.1874\n",
      "Epoch 1 Batch 1323 Loss 2.0619\n",
      "Epoch 1 Batch 1324 Loss 2.4472\n",
      "Epoch 1 Batch 1325 Loss 2.4030\n",
      "Epoch 1 Batch 1326 Loss 2.1542\n",
      "Epoch 1 Batch 1327 Loss 2.1107\n",
      "Epoch 1 Batch 1328 Loss 1.9897\n",
      "Epoch 1 Batch 1329 Loss 1.9843\n",
      "Epoch 1 Batch 1330 Loss 1.2616\n",
      "Epoch 1 Batch 1331 Loss 1.7662\n",
      "Epoch 1 Batch 1332 Loss 1.7296\n",
      "Epoch 1 Batch 1333 Loss 2.0546\n",
      "Epoch 1 Batch 1334 Loss 2.0301\n",
      "Epoch 1 Batch 1335 Loss 1.8973\n",
      "Epoch 1 Batch 1336 Loss 2.1418\n",
      "Epoch 1 Batch 1337 Loss 2.0770\n",
      "Epoch 1 Batch 1338 Loss 2.4253\n",
      "Epoch 1 Batch 1339 Loss 2.9822\n",
      "Epoch 1 Batch 1340 Loss 1.8115\n",
      "Epoch 1 Batch 1341 Loss 2.1972\n",
      "Epoch 1 Batch 1342 Loss 1.8194\n",
      "Epoch 1 Batch 1343 Loss 1.9915\n",
      "Epoch 1 Batch 1344 Loss 1.8237\n",
      "Epoch 1 Batch 1345 Loss 2.1691\n",
      "Epoch 1 Batch 1346 Loss 1.8363\n",
      "Epoch 1 Batch 1347 Loss 1.8839\n",
      "Epoch 1 Batch 1348 Loss 2.2517\n",
      "Epoch 1 Batch 1349 Loss 1.7851\n",
      "Epoch 1 Batch 1350 Loss 1.8881\n",
      "Epoch 1 Batch 1351 Loss 1.7139\n",
      "Epoch 1 Batch 1352 Loss 1.7250\n",
      "Epoch 1 Batch 1353 Loss 2.2099\n",
      "Epoch 1 Batch 1354 Loss 1.9635\n",
      "Epoch 1 Batch 1355 Loss 1.8787\n",
      "Epoch 1 Batch 1356 Loss 1.8646\n",
      "Epoch 1 Batch 1357 Loss 2.0946\n",
      "Epoch 1 Batch 1358 Loss 2.0807\n",
      "Epoch 1 Batch 1359 Loss 1.7137\n",
      "Epoch 1 Batch 1360 Loss 1.6865\n",
      "Epoch 1 Batch 1361 Loss 2.9219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 1362 Loss 1.9632\n",
      "Epoch 1 Batch 1363 Loss 1.6795\n",
      "Epoch 1 Batch 1364 Loss 1.9489\n",
      "Epoch 1 Batch 1365 Loss 2.0493\n",
      "Epoch 1 Batch 1366 Loss 1.6988\n",
      "Epoch 1 Batch 1367 Loss 1.8401\n",
      "Epoch 1 Batch 1368 Loss 1.9432\n",
      "Epoch 1 Batch 1369 Loss 1.7475\n",
      "Epoch 1 Batch 1370 Loss 2.1460\n",
      "Epoch 1 Batch 1371 Loss 1.8751\n",
      "Epoch 1 Batch 1372 Loss 1.9177\n",
      "Epoch 1 Batch 1373 Loss 2.3582\n",
      "Epoch 1 Batch 1374 Loss 1.7523\n",
      "Epoch 1 Batch 1375 Loss 1.7484\n",
      "Epoch 1 Batch 1376 Loss 1.3145\n",
      "Epoch 1 Batch 1377 Loss 1.3935\n",
      "Epoch 1 Batch 1378 Loss 2.1444\n",
      "Epoch 1 Batch 1379 Loss 1.7559\n",
      "Epoch 1 Batch 1380 Loss 2.3757\n",
      "Epoch 1 Batch 1381 Loss 1.8689\n",
      "Epoch 1 Batch 1382 Loss 2.1919\n",
      "Epoch 1 Batch 1383 Loss 1.6612\n",
      "Epoch 1 Batch 1384 Loss 1.9096\n",
      "Epoch 1 Batch 1385 Loss 1.9356\n",
      "Epoch 1 Batch 1386 Loss 2.3608\n",
      "Epoch 1 Batch 1387 Loss 1.9628\n",
      "Epoch 1 Batch 1388 Loss 1.8582\n",
      "Epoch 1 Batch 1389 Loss 1.9530\n",
      "Epoch 1 Batch 1390 Loss 2.3008\n",
      "Epoch 1 Batch 1391 Loss 2.1124\n",
      "Epoch 1 Batch 1392 Loss 2.2307\n",
      "Epoch 1 Batch 1393 Loss 1.8756\n",
      "Epoch 1 Batch 1394 Loss 1.9208\n",
      "Epoch 1 Batch 1395 Loss 1.8515\n",
      "Epoch 1 Batch 1396 Loss 2.0379\n",
      "Epoch 1 Batch 1397 Loss 1.8751\n",
      "Epoch 1 Batch 1398 Loss 1.6410\n",
      "Epoch 1 Batch 1399 Loss 1.6404\n",
      "Epoch 1 Batch 1400 Loss 1.2898\n",
      "Epoch 1 Batch 1401 Loss 1.3483\n",
      "Epoch 1 Batch 1402 Loss 1.7164\n",
      "Epoch 1 Batch 1403 Loss 2.1458\n",
      "Epoch 1 Batch 1404 Loss 1.9083\n",
      "Epoch 1 Batch 1405 Loss 2.0069\n",
      "Epoch 1 Batch 1406 Loss 2.3250\n",
      "Epoch 1 Batch 1407 Loss 2.5876\n",
      "Epoch 1 Batch 1408 Loss 2.4507\n",
      "Epoch 1 Batch 1409 Loss 2.4247\n",
      "Epoch 1 Batch 1410 Loss 2.2237\n",
      "Epoch 1 Batch 1411 Loss 2.1957\n",
      "Epoch 1 Batch 1412 Loss 1.9084\n",
      "Epoch 1 Batch 1413 Loss 1.7970\n",
      "Epoch 1 Batch 1414 Loss 1.6962\n",
      "Epoch 1 Batch 1415 Loss 1.6466\n",
      "Epoch 1 Batch 1416 Loss 1.7179\n",
      "Epoch 1 Batch 1417 Loss 1.6108\n",
      "Epoch 1 Batch 1418 Loss 1.5944\n",
      "Epoch 1 Batch 1419 Loss 2.2573\n",
      "Epoch 1 Batch 1420 Loss 1.8889\n",
      "Epoch 1 Batch 1421 Loss 2.1954\n",
      "Epoch 1 Batch 1422 Loss 2.0343\n",
      "Epoch 1 Batch 1423 Loss 1.8270\n",
      "Epoch 1 Batch 1424 Loss 1.9288\n",
      "Epoch 1 Batch 1425 Loss 2.0543\n",
      "Epoch 1 Batch 1426 Loss 1.7768\n",
      "Epoch 1 Batch 1427 Loss 2.2194\n",
      "Epoch 1 Batch 1428 Loss 1.8320\n",
      "Epoch 1 Batch 1429 Loss 1.6440\n",
      "Epoch 1 Batch 1430 Loss 1.6478\n",
      "Epoch 1 Batch 1431 Loss 2.0787\n",
      "Epoch 1 Batch 1432 Loss 2.0667\n",
      "Epoch 1 Batch 1433 Loss 1.9224\n",
      "Epoch 1 Batch 1434 Loss 2.4380\n",
      "Epoch 1 Batch 1435 Loss 2.6222\n",
      "Epoch 1 Batch 1436 Loss 1.8706\n",
      "Epoch 1 Batch 1437 Loss 2.2339\n",
      "Epoch 1 Batch 1438 Loss 1.8744\n",
      "Epoch 1 Batch 1439 Loss 1.7321\n",
      "Epoch 1 Batch 1440 Loss 2.3660\n",
      "Epoch 1 Batch 1441 Loss 2.6235\n",
      "Epoch 1 Batch 1442 Loss 2.1341\n",
      "Epoch 1 Batch 1443 Loss 2.1444\n",
      "Epoch 1 Batch 1444 Loss 2.6931\n",
      "Epoch 1 Batch 1445 Loss 2.7541\n",
      "Epoch 1 Batch 1446 Loss 2.2870\n",
      "Epoch 1 Batch 1447 Loss 2.5695\n",
      "Epoch 1 Batch 1448 Loss 2.1731\n",
      "Epoch 1 Batch 1449 Loss 1.7051\n",
      "Epoch 1 Batch 1450 Loss 1.9713\n",
      "Epoch 1 Batch 1451 Loss 1.8118\n",
      "Epoch 1 Batch 1452 Loss 1.5588\n",
      "Epoch 1 Batch 1453 Loss 2.1624\n",
      "Epoch 1 Batch 1454 Loss 2.1129\n",
      "Epoch 1 Batch 1455 Loss 1.6965\n",
      "Epoch 1 Batch 1456 Loss 1.8055\n",
      "Epoch 1 Batch 1457 Loss 2.1655\n",
      "Epoch 1 Batch 1458 Loss 1.4739\n",
      "Epoch 1 Batch 1459 Loss 1.4853\n",
      "Epoch 1 Batch 1460 Loss 1.1080\n",
      "Epoch 1 Batch 1461 Loss 0.8393\n",
      "Epoch 1 Batch 1462 Loss 1.2075\n",
      "Epoch 1 Batch 1463 Loss 1.6744\n",
      "Epoch 1 Batch 1464 Loss 1.9068\n",
      "Epoch 1 Batch 1465 Loss 1.8039\n",
      "Epoch 1 Batch 1466 Loss 1.9167\n",
      "Epoch 1 Batch 1467 Loss 2.6301\n",
      "Epoch 1 Batch 1468 Loss 2.6844\n",
      "Epoch 1 Batch 1469 Loss 2.6369\n",
      "Epoch 1 Batch 1470 Loss 2.1501\n",
      "Epoch 1 Batch 1471 Loss 1.6837\n",
      "Epoch 1 Batch 1472 Loss 1.9010\n",
      "Epoch 1 Batch 1473 Loss 2.1437\n",
      "Epoch 1 Batch 1474 Loss 2.3138\n",
      "Epoch 1 Batch 1475 Loss 1.9783\n",
      "Epoch 1 Batch 1476 Loss 1.9172\n",
      "Epoch 1 Batch 1477 Loss 1.8583\n",
      "Epoch 1 Batch 1478 Loss 1.8401\n",
      "Epoch 1 Batch 1479 Loss 2.0378\n",
      "Epoch 1 Batch 1480 Loss 1.4219\n",
      "Epoch 1 Batch 1481 Loss 1.9716\n",
      "Epoch 1 Batch 1482 Loss 2.3651\n",
      "Epoch 1 Batch 1483 Loss 1.9585\n",
      "Epoch 1 Batch 1484 Loss 1.9290\n",
      "Epoch 1 Batch 1485 Loss 1.6562\n",
      "Epoch 1 Batch 1486 Loss 1.6198\n",
      "Epoch 1 Batch 1487 Loss 1.7509\n",
      "Epoch 1 Batch 1488 Loss 1.4915\n",
      "Epoch 1 Batch 1489 Loss 1.7607\n",
      "Epoch 1 Batch 1490 Loss 1.9492\n",
      "Epoch 1 Batch 1491 Loss 2.2023\n",
      "Epoch 1 Batch 1492 Loss 2.4952\n",
      "Epoch 1 Batch 1493 Loss 2.6703\n",
      "Epoch 1 Batch 1494 Loss 1.9827\n",
      "Epoch 1 Batch 1495 Loss 1.8636\n",
      "Epoch 1 Batch 1496 Loss 1.6675\n",
      "Epoch 1 Batch 1497 Loss 1.5401\n",
      "Epoch 1 Batch 1498 Loss 1.7697\n",
      "Epoch 1 Batch 1499 Loss 2.0381\n",
      "Epoch 1 Batch 1500 Loss 2.0473\n",
      "Epoch 1 Batch 1501 Loss 1.6853\n",
      "Epoch 1 Batch 1502 Loss 1.9117\n",
      "Epoch 1 Batch 1503 Loss 1.8928\n",
      "Epoch 1 Batch 1504 Loss 1.9853\n",
      "Epoch 1 Batch 1505 Loss 1.6660\n",
      "Epoch 1 Batch 1506 Loss 1.8941\n",
      "Epoch 1 Batch 1507 Loss 1.6114\n",
      "Epoch 1 Batch 1508 Loss 1.8876\n",
      "Epoch 1 Batch 1509 Loss 1.9534\n",
      "Epoch 1 Batch 1510 Loss 1.8825\n",
      "Epoch 1 Batch 1511 Loss 2.3584\n",
      "Epoch 1 Batch 1512 Loss 1.5604\n",
      "Epoch 1 Batch 1513 Loss 2.5142\n",
      "Epoch 1 Batch 1514 Loss 2.0736\n",
      "Epoch 1 Batch 1515 Loss 2.3993\n",
      "Epoch 1 Batch 1516 Loss 2.4055\n",
      "Epoch 1 Batch 1517 Loss 2.3884\n",
      "Epoch 1 Batch 1518 Loss 2.0970\n",
      "Epoch 1 Batch 1519 Loss 1.7227\n",
      "Epoch 1 Batch 1520 Loss 1.5924\n",
      "Epoch 1 Batch 1521 Loss 2.0380\n",
      "Epoch 1 Batch 1522 Loss 2.6528\n",
      "Epoch 1 Batch 1523 Loss 2.1100\n",
      "Epoch 1 Batch 1524 Loss 1.8992\n",
      "Epoch 1 Batch 1525 Loss 2.1223\n",
      "Epoch 1 Batch 1526 Loss 1.8859\n",
      "Epoch 1 Batch 1527 Loss 1.7700\n",
      "Epoch 1 Batch 1528 Loss 2.1024\n",
      "Epoch 1 Batch 1529 Loss 2.4353\n",
      "Epoch 1 Batch 1530 Loss 2.1419\n",
      "Epoch 1 Batch 1531 Loss 2.0385\n",
      "Epoch 1 Batch 1532 Loss 2.0880\n",
      "Epoch 1 Batch 1533 Loss 2.5460\n",
      "Epoch 1 Batch 1534 Loss 1.8389\n",
      "Epoch 1 Batch 1535 Loss 2.3120\n",
      "Epoch 1 Batch 1536 Loss 2.1926\n",
      "Epoch 1 Batch 1537 Loss 1.7701\n",
      "Epoch 1 Batch 1538 Loss 2.0823\n",
      "Epoch 1 Batch 1539 Loss 2.1170\n",
      "Epoch 1 Batch 1540 Loss 1.8399\n",
      "Epoch 1 Batch 1541 Loss 1.7473\n",
      "Epoch 1 Batch 1542 Loss 1.6899\n",
      "Epoch 1 Batch 1543 Loss 1.8539\n",
      "Epoch 1 Batch 1544 Loss 1.6212\n",
      "Epoch 1 Batch 1545 Loss 1.6624\n",
      "Epoch 1 Batch 1546 Loss 2.0536\n",
      "Epoch 1 Batch 1547 Loss 2.1579\n",
      "Epoch 1 Batch 1548 Loss 1.8129\n",
      "Epoch 1 Batch 1549 Loss 1.8266\n",
      "Epoch 1 Batch 1550 Loss 1.8853\n",
      "Epoch 1 Batch 1551 Loss 1.8994\n",
      "Epoch 1 Batch 1552 Loss 2.0681\n",
      "Epoch 1 Batch 1553 Loss 1.9177\n",
      "Epoch 1 Batch 1554 Loss 1.9445\n",
      "Epoch 1 Batch 1555 Loss 2.3792\n",
      "Epoch 1 Batch 1556 Loss 2.1800\n",
      "Epoch 1 Batch 1557 Loss 1.7130\n",
      "Epoch 1 Batch 1558 Loss 1.6676\n",
      "Epoch 1 Batch 1559 Loss 1.5716\n",
      "Epoch 1 Batch 1560 Loss 1.7624\n",
      "Epoch 1 Batch 1561 Loss 1.9298\n",
      "Epoch 1 Batch 1562 Loss 1.9121\n",
      "Epoch 1 Batch 1563 Loss 1.9274\n",
      "Epoch 1 Batch 1564 Loss 2.1880\n",
      "Epoch 1 Batch 1565 Loss 2.0096\n",
      "Epoch 1 Batch 1566 Loss 1.9680\n",
      "Epoch 1 Batch 1567 Loss 2.1397\n",
      "Epoch 1 Batch 1568 Loss 2.0411\n",
      "Epoch 1 Batch 1569 Loss 2.1333\n",
      "Epoch 1 Batch 1570 Loss 2.4201\n",
      "Epoch 1 Batch 1571 Loss 2.0121\n",
      "Epoch 1 Batch 1572 Loss 2.4295\n",
      "Epoch 1 Batch 1573 Loss 2.7001\n",
      "Epoch 1 Batch 1574 Loss 2.3030\n",
      "Epoch 1 Batch 1575 Loss 2.1946\n",
      "Epoch 1 Batch 1576 Loss 2.1870\n",
      "Epoch 1 Batch 1577 Loss 2.3833\n",
      "Epoch 1 Batch 1578 Loss 2.1200\n",
      "Epoch 1 Batch 1579 Loss 1.7816\n",
      "Epoch 1 Batch 1580 Loss 1.9558\n",
      "Epoch 1 Batch 1581 Loss 2.0255\n",
      "Epoch 1 Batch 1582 Loss 1.8723\n",
      "Epoch 1 Batch 1583 Loss 1.9038\n",
      "Epoch 1 Batch 1584 Loss 2.4222\n",
      "Epoch 1 Batch 1585 Loss 2.3953\n",
      "Epoch 1 Batch 1586 Loss 1.7477\n",
      "Epoch 1 Batch 1587 Loss 2.0629\n",
      "Epoch 1 Batch 1588 Loss 1.7477\n",
      "Epoch 1 Batch 1589 Loss 1.6677\n",
      "Epoch 1 Batch 1590 Loss 2.1817\n",
      "Epoch 1 Batch 1591 Loss 2.3465\n",
      "Epoch 1 Batch 1592 Loss 2.5459\n",
      "Epoch 1 Batch 1593 Loss 1.8206\n",
      "Epoch 1 Batch 1594 Loss 1.5870\n",
      "Epoch 1 Batch 1595 Loss 1.8954\n",
      "Epoch 1 Batch 1596 Loss 2.0789\n",
      "Epoch 1 Batch 1597 Loss 2.2791\n",
      "Epoch 1 Batch 1598 Loss 3.1362\n",
      "Epoch 1 Batch 1599 Loss 2.1127\n",
      "Epoch 1 Batch 1600 Loss 2.0161\n",
      "Epoch 1 Batch 1601 Loss 2.1129\n",
      "Epoch 1 Batch 1602 Loss 2.3693\n",
      "Epoch 1 Batch 1603 Loss 2.3099\n",
      "Epoch 1 Batch 1604 Loss 1.9161\n",
      "Epoch 1 Batch 1605 Loss 2.5077\n",
      "Epoch 1 Batch 1606 Loss 2.5796\n",
      "Epoch 1 Batch 1607 Loss 2.1268\n",
      "Epoch 1 Batch 1608 Loss 2.1992\n",
      "Epoch 1 Batch 1609 Loss 1.9040\n",
      "Epoch 1 Batch 1610 Loss 1.9697\n",
      "Epoch 1 Batch 1611 Loss 1.9144\n",
      "Epoch 1 Batch 1612 Loss 1.8105\n",
      "Epoch 1 Batch 1613 Loss 2.2959\n",
      "Epoch 1 Batch 1614 Loss 1.7724\n",
      "Epoch 1 Batch 1615 Loss 2.0309\n",
      "Epoch 1 Batch 1616 Loss 1.5370\n",
      "Epoch 1 Batch 1617 Loss 1.7324\n",
      "Epoch 1 Batch 1618 Loss 2.0770\n",
      "Epoch 1 Batch 1619 Loss 2.0557\n",
      "Epoch 1 Batch 1620 Loss 1.9432\n",
      "Epoch 1 Batch 1621 Loss 1.8825\n",
      "Epoch 1 Batch 1622 Loss 1.6952\n",
      "Epoch 1 Batch 1623 Loss 1.9417\n",
      "Epoch 1 Batch 1624 Loss 2.0954\n",
      "Epoch 1 Batch 1625 Loss 2.0976\n",
      "Epoch 1 Batch 1626 Loss 2.0892\n",
      "Epoch 1 Batch 1627 Loss 2.1512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 1628 Loss 2.2402\n",
      "Epoch 1 Batch 1629 Loss 2.4539\n",
      "Epoch 1 Batch 1630 Loss 2.2677\n",
      "Epoch 1 Batch 1631 Loss 2.5801\n",
      "Epoch 1 Batch 1632 Loss 2.4203\n",
      "Epoch 1 Batch 1633 Loss 2.3459\n",
      "Epoch 1 Batch 1634 Loss 2.1286\n",
      "Epoch 1 Batch 1635 Loss 2.3522\n",
      "Epoch 1 Batch 1636 Loss 2.5011\n",
      "Epoch 1 Batch 1637 Loss 2.0897\n",
      "Epoch 1 Batch 1638 Loss 1.8526\n",
      "Epoch 1 Batch 1639 Loss 1.9455\n",
      "Epoch 1 Batch 1640 Loss 2.1454\n",
      "Epoch 1 Batch 1641 Loss 2.0558\n",
      "Epoch 1 Batch 1642 Loss 1.8441\n",
      "Epoch 1 Batch 1643 Loss 1.7983\n",
      "Epoch 1 Batch 1644 Loss 2.0393\n",
      "Epoch 1 Batch 1645 Loss 1.4502\n",
      "Epoch 1 Batch 1646 Loss 1.2929\n",
      "Epoch 1 Batch 1647 Loss 1.7680\n",
      "Epoch 1 Batch 1648 Loss 1.2480\n",
      "Epoch 1 Batch 1649 Loss 1.9367\n",
      "Epoch 1 Batch 1650 Loss 2.4431\n",
      "Epoch 1 Batch 1651 Loss 1.9504\n",
      "Epoch 1 Batch 1652 Loss 1.8370\n",
      "Epoch 1 Batch 1653 Loss 2.1239\n",
      "Epoch 1 Batch 1654 Loss 1.7442\n",
      "Epoch 1 Batch 1655 Loss 2.2609\n",
      "Epoch 1 Batch 1656 Loss 1.7002\n",
      "Epoch 1 Batch 1657 Loss 2.3983\n",
      "Epoch 1 Batch 1658 Loss 2.4690\n",
      "Epoch 1 Batch 1659 Loss 2.1590\n",
      "Epoch 1 Batch 1660 Loss 1.9573\n",
      "Epoch 1 Batch 1661 Loss 2.0061\n",
      "Epoch 1 Batch 1662 Loss 1.9373\n",
      "Epoch 1 Batch 1663 Loss 1.7383\n",
      "Epoch 1 Batch 1664 Loss 2.6072\n",
      "Epoch 1 Batch 1665 Loss 2.2661\n",
      "Epoch 1 Batch 1666 Loss 2.3216\n",
      "Epoch 1 Batch 1667 Loss 2.2620\n",
      "Epoch 1 Batch 1668 Loss 2.1994\n",
      "Epoch 1 Batch 1669 Loss 2.1584\n",
      "Epoch 1 Batch 1670 Loss 2.3529\n",
      "Epoch 1 Batch 1671 Loss 2.2388\n",
      "Epoch 1 Batch 1672 Loss 1.8641\n",
      "Epoch 1 Batch 1673 Loss 2.3104\n",
      "Epoch 1 Batch 1674 Loss 1.8889\n",
      "Epoch 1 Batch 1675 Loss 2.2412\n",
      "Epoch 1 Batch 1676 Loss 1.8917\n",
      "Epoch 1 Batch 1677 Loss 1.8571\n",
      "Epoch 1 Batch 1678 Loss 2.9992\n",
      "Epoch 1 Batch 1679 Loss 1.8752\n",
      "Epoch 1 Batch 1680 Loss 2.4616\n",
      "Epoch 1 Batch 1681 Loss 1.8652\n",
      "Epoch 1 Batch 1682 Loss 1.9277\n",
      "Epoch 1 Batch 1683 Loss 1.3899\n",
      "Epoch 1 Batch 1684 Loss 1.7280\n",
      "Epoch 1 Batch 1685 Loss 2.0357\n",
      "Epoch 1 Batch 1686 Loss 1.8041\n",
      "Epoch 1 Batch 1687 Loss 1.7355\n",
      "Epoch 1 Batch 1688 Loss 1.7984\n",
      "Epoch 1 Batch 1689 Loss 1.5539\n",
      "Epoch 1 Batch 1690 Loss 1.9417\n",
      "Epoch 1 Batch 1691 Loss 1.9120\n",
      "Epoch 1 Batch 1692 Loss 2.5017\n",
      "Epoch 1 Batch 1693 Loss 2.4521\n",
      "Epoch 1 Batch 1694 Loss 2.4948\n",
      "Epoch 1 Batch 1695 Loss 2.2477\n",
      "Epoch 1 Batch 1696 Loss 2.2515\n",
      "Epoch 1 Batch 1697 Loss 1.9915\n",
      "Epoch 1 Batch 1698 Loss 2.2503\n",
      "Epoch 1 Batch 1699 Loss 2.2356\n",
      "Epoch 1 Batch 1700 Loss 2.5169\n",
      "Epoch 1 Batch 1701 Loss 2.0039\n",
      "Epoch 1 Batch 1702 Loss 2.1324\n",
      "Epoch 1 Batch 1703 Loss 2.3101\n",
      "Epoch 1 Batch 1704 Loss 1.8986\n",
      "Epoch 1 Batch 1705 Loss 1.9477\n",
      "Epoch 1 Batch 1706 Loss 2.1027\n",
      "Epoch 1 Batch 1707 Loss 2.0979\n",
      "Epoch 1 Batch 1708 Loss 2.3976\n",
      "Epoch 1 Batch 1709 Loss 1.9063\n",
      "Epoch 1 Batch 1710 Loss 2.6262\n",
      "Epoch 1 Batch 1711 Loss 2.1885\n",
      "Epoch 1 Batch 1712 Loss 2.1064\n",
      "Epoch 1 Batch 1713 Loss 2.2167\n",
      "Epoch 1 Batch 1714 Loss 2.4356\n",
      "Epoch 1 Batch 1715 Loss 2.1679\n",
      "Epoch 1 Batch 1716 Loss 2.0412\n",
      "Epoch 1 Batch 1717 Loss 1.8266\n",
      "Epoch 1 Batch 1718 Loss 2.0879\n",
      "Epoch 1 Batch 1719 Loss 2.8820\n",
      "Epoch 1 Batch 1720 Loss 2.1114\n",
      "Epoch 1 Batch 1721 Loss 2.1758\n",
      "Epoch 1 Batch 1722 Loss 1.9262\n",
      "Epoch 1 Batch 1723 Loss 2.0277\n",
      "Epoch 1 Batch 1724 Loss 1.9045\n",
      "Epoch 1 Batch 1725 Loss 1.6375\n",
      "Epoch 1 Batch 1726 Loss 2.3211\n",
      "Epoch 1 Batch 1727 Loss 2.0859\n",
      "Epoch 1 Batch 1728 Loss 1.8897\n",
      "Epoch 1 Batch 1729 Loss 1.7139\n",
      "Epoch 1 Batch 1730 Loss 2.3816\n",
      "Epoch 1 Batch 1731 Loss 2.5974\n",
      "Epoch 1 Batch 1732 Loss 1.7620\n",
      "Epoch 1 Batch 1733 Loss 1.9247\n",
      "Epoch 1 Batch 1734 Loss 1.4435\n",
      "Epoch 1 Batch 1735 Loss 1.6198\n",
      "Epoch 1 Batch 1736 Loss 1.7846\n",
      "Epoch 1 Batch 1737 Loss 2.4336\n",
      "Epoch 1 Batch 1738 Loss 2.8773\n",
      "Epoch 1 Batch 1739 Loss 2.2610\n",
      "Epoch 1 Batch 1740 Loss 1.7632\n",
      "Epoch 1 Batch 1741 Loss 1.9491\n",
      "Epoch 1 Batch 1742 Loss 1.8262\n",
      "Epoch 1 Batch 1743 Loss 1.5759\n",
      "Epoch 1 Batch 1744 Loss 1.6123\n",
      "Epoch 1 Batch 1745 Loss 1.7295\n",
      "Epoch 1 Batch 1746 Loss 2.1049\n",
      "Epoch 1 Batch 1747 Loss 1.9783\n",
      "Epoch 1 Batch 1748 Loss 1.5738\n",
      "Epoch 1 Batch 1749 Loss 1.8982\n",
      "Epoch 1 Batch 1750 Loss 1.8335\n",
      "Epoch 1 Batch 1751 Loss 1.9199\n",
      "Epoch 1 Batch 1752 Loss 2.1027\n",
      "Epoch 1 Batch 1753 Loss 2.6704\n",
      "Epoch 1 Batch 1754 Loss 2.3746\n",
      "Epoch 1 Batch 1755 Loss 2.4598\n",
      "Epoch 1 Batch 1756 Loss 1.9458\n",
      "Epoch 1 Batch 1757 Loss 2.2529\n",
      "Epoch 1 Batch 1758 Loss 2.3488\n",
      "Epoch 1 Batch 1759 Loss 1.6062\n",
      "Epoch 1 Batch 1760 Loss 2.4301\n",
      "Epoch 1 Batch 1761 Loss 1.8728\n",
      "Epoch 1 Batch 1762 Loss 2.1865\n",
      "Epoch 1 Batch 1763 Loss 2.0636\n",
      "Epoch 1 Batch 1764 Loss 2.2814\n",
      "Epoch 1 Batch 1765 Loss 2.0150\n",
      "Epoch 1 Batch 1766 Loss 1.4935\n",
      "Epoch 1 Batch 1767 Loss 1.8701\n",
      "Epoch 1 Batch 1768 Loss 1.8504\n",
      "Epoch 1 Batch 1769 Loss 2.4714\n",
      "Epoch 1 Batch 1770 Loss 2.9508\n",
      "Epoch 1 Batch 1771 Loss 2.1324\n",
      "Epoch 1 Batch 1772 Loss 2.2683\n",
      "Epoch 1 Batch 1773 Loss 2.3303\n",
      "Epoch 1 Batch 1774 Loss 2.2516\n",
      "Epoch 1 Batch 1775 Loss 2.1485\n",
      "Epoch 1 Batch 1776 Loss 2.0570\n",
      "Epoch 1 Batch 1777 Loss 1.8736\n",
      "Epoch 1 Batch 1778 Loss 2.2980\n",
      "Epoch 1 Batch 1779 Loss 2.0148\n",
      "Epoch 1 Batch 1780 Loss 2.5025\n",
      "Epoch 1 Batch 1781 Loss 2.2729\n",
      "Epoch 1 Batch 1782 Loss 2.1746\n",
      "Epoch 1 Batch 1783 Loss 2.1958\n",
      "Epoch 1 Batch 1784 Loss 2.6006\n",
      "Epoch 1 Batch 1785 Loss 2.3797\n",
      "Epoch 1 Batch 1786 Loss 2.4727\n",
      "Epoch 1 Batch 1787 Loss 2.1050\n",
      "Epoch 1 Batch 1788 Loss 2.2416\n",
      "Epoch 1 Batch 1789 Loss 2.0223\n",
      "Epoch 1 Batch 1790 Loss 2.1412\n",
      "Epoch 1 Batch 1791 Loss 1.8133\n",
      "Epoch 1 Batch 1792 Loss 1.9328\n",
      "Epoch 1 Batch 1793 Loss 1.8955\n",
      "Epoch 1 Batch 1794 Loss 2.3572\n",
      "Epoch 1 Batch 1795 Loss 1.9328\n",
      "Epoch 1 Batch 1796 Loss 2.2540\n",
      "Epoch 1 Batch 1797 Loss 1.6750\n",
      "Epoch 1 Batch 1798 Loss 1.5771\n",
      "Epoch 1 Batch 1799 Loss 2.0424\n",
      "Epoch 1 Batch 1800 Loss 1.6825\n",
      "Epoch 1 Batch 1801 Loss 2.2727\n",
      "Epoch 1 Batch 1802 Loss 2.3988\n",
      "Epoch 1 Batch 1803 Loss 2.6818\n",
      "Epoch 1 Batch 1804 Loss 1.9241\n",
      "Epoch 1 Batch 1805 Loss 1.7207\n",
      "Epoch 1 Batch 1806 Loss 1.9876\n",
      "Epoch 1 Batch 1807 Loss 1.7760\n",
      "Epoch 1 Batch 1808 Loss 2.3679\n",
      "Epoch 1 Batch 1809 Loss 2.1555\n",
      "Epoch 1 Batch 1810 Loss 2.2497\n",
      "Epoch 1 Batch 1811 Loss 2.3640\n",
      "Epoch 1 Batch 1812 Loss 2.9769\n",
      "Epoch 1 Batch 1813 Loss 2.2466\n",
      "Epoch 1 Batch 1814 Loss 1.9561\n",
      "Epoch 1 Batch 1815 Loss 2.3519\n",
      "Epoch 1 Batch 1816 Loss 1.9692\n",
      "Epoch 1 Batch 1817 Loss 1.8000\n",
      "Epoch 1 Batch 1818 Loss 2.2595\n",
      "Epoch 1 Batch 1819 Loss 2.1319\n",
      "Epoch 1 Batch 1820 Loss 1.9594\n",
      "Epoch 1 Batch 1821 Loss 2.1261\n",
      "Epoch 1 Batch 1822 Loss 2.0038\n",
      "Epoch 1 Batch 1823 Loss 2.0589\n",
      "Epoch 1 Batch 1824 Loss 2.2691\n",
      "Epoch 1 Batch 1825 Loss 2.1301\n",
      "Epoch 1 Batch 1826 Loss 2.5961\n",
      "Epoch 1 Batch 1827 Loss 2.7139\n",
      "Epoch 1 Batch 1828 Loss 2.2279\n",
      "Epoch 1 Batch 1829 Loss 1.9836\n",
      "Epoch 1 Batch 1830 Loss 1.9227\n",
      "Epoch 1 Batch 1831 Loss 2.2723\n",
      "Epoch 1 Batch 1832 Loss 2.7481\n",
      "Epoch 1 Batch 1833 Loss 2.5253\n",
      "Epoch 1 Batch 1834 Loss 2.6271\n",
      "Epoch 1 Batch 1835 Loss 2.0018\n",
      "Epoch 1 Batch 1836 Loss 2.5248\n",
      "Epoch 1 Batch 1837 Loss 2.2952\n",
      "Epoch 1 Batch 1838 Loss 2.0191\n",
      "Epoch 1 Batch 1839 Loss 2.1323\n",
      "Epoch 1 Batch 1840 Loss 2.2176\n",
      "Epoch 1 Batch 1841 Loss 2.1474\n",
      "Epoch 1 Batch 1842 Loss 1.6693\n",
      "Epoch 1 Batch 1843 Loss 1.6876\n",
      "Epoch 1 Batch 1844 Loss 1.8378\n",
      "Epoch 1 Batch 1845 Loss 1.9678\n",
      "Epoch 1 Batch 1846 Loss 1.8415\n",
      "Epoch 1 Batch 1847 Loss 1.9230\n",
      "Epoch 1 Batch 1848 Loss 2.4784\n",
      "Epoch 1 Batch 1849 Loss 2.0255\n",
      "Epoch 1 Batch 1850 Loss 1.7994\n",
      "Epoch 1 Batch 1851 Loss 1.7212\n",
      "Epoch 1 Batch 1852 Loss 2.2574\n",
      "Epoch 1 Batch 1853 Loss 2.0749\n",
      "Epoch 1 Batch 1854 Loss 1.5914\n",
      "Epoch 1 Batch 1855 Loss 1.6934\n",
      "Epoch 1 Batch 1856 Loss 1.9078\n",
      "Epoch 1 Batch 1857 Loss 1.4540\n",
      "Epoch 1 Batch 1858 Loss 2.0422\n",
      "Epoch 1 Batch 1859 Loss 1.9457\n",
      "Epoch 1 Batch 1860 Loss 1.7035\n",
      "Epoch 1 Batch 1861 Loss 1.5158\n",
      "Epoch 1 Batch 1862 Loss 1.4771\n",
      "Epoch 1 Batch 1863 Loss 2.1983\n",
      "Epoch 1 Batch 1864 Loss 1.7288\n",
      "Epoch 1 Batch 1865 Loss 2.1176\n",
      "Epoch 1 Batch 1866 Loss 2.0318\n",
      "Epoch 1 Batch 1867 Loss 2.1700\n",
      "Epoch 1 Batch 1868 Loss 1.8449\n",
      "Epoch 1 Batch 1869 Loss 2.2285\n",
      "Epoch 1 Batch 1870 Loss 2.2510\n",
      "Epoch 1 Batch 1871 Loss 2.4804\n",
      "Epoch 1 Batch 1872 Loss 2.3509\n",
      "Epoch 1 Batch 1873 Loss 2.1038\n",
      "Epoch 1 Batch 1874 Loss 2.4733\n",
      "Epoch 1 Batch 1875 Loss 3.0315\n",
      "Epoch 1 Batch 1876 Loss 2.5035\n",
      "Epoch 1 Batch 1877 Loss 2.0611\n",
      "Epoch 1 Batch 1878 Loss 2.2806\n",
      "Epoch 1 Batch 1879 Loss 2.0340\n",
      "Epoch 1 Batch 1880 Loss 1.9830\n",
      "Epoch 1 Batch 1881 Loss 1.8246\n",
      "Epoch 1 Batch 1882 Loss 1.6000\n",
      "Epoch 1 Batch 1883 Loss 2.2365\n",
      "Epoch 1 Batch 1884 Loss 1.7832\n",
      "Epoch 1 Batch 1885 Loss 2.1821\n",
      "Epoch 1 Batch 1886 Loss 2.1225\n",
      "Epoch 1 Batch 1887 Loss 2.1512\n",
      "Epoch 1 Batch 1888 Loss 1.9083\n",
      "Epoch 1 Batch 1889 Loss 1.7487\n",
      "Epoch 1 Batch 1890 Loss 1.5990\n",
      "Epoch 1 Batch 1891 Loss 2.2363\n",
      "Epoch 1 Batch 1892 Loss 1.4414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 1893 Loss 1.2295\n",
      "Epoch 1 Batch 1894 Loss 1.8127\n",
      "Epoch 1 Batch 1895 Loss 1.7397\n",
      "Epoch 1 Batch 1896 Loss 1.7049\n",
      "Epoch 1 Batch 1897 Loss 2.0696\n",
      "Epoch 1 Batch 1898 Loss 2.3105\n",
      "Epoch 1 Batch 1899 Loss 2.0449\n",
      "Epoch 1 Batch 1900 Loss 1.6538\n",
      "Epoch 1 Batch 1901 Loss 1.9251\n",
      "Epoch 1 Batch 1902 Loss 2.2535\n",
      "Epoch 1 Batch 1903 Loss 2.2247\n",
      "Epoch 1 Batch 1904 Loss 2.4726\n",
      "Epoch 1 Batch 1905 Loss 2.2299\n",
      "Epoch 1 Batch 1906 Loss 2.0551\n",
      "Epoch 1 Batch 1907 Loss 2.3703\n",
      "Epoch 1 Batch 1908 Loss 2.5753\n",
      "Epoch 1 Batch 1909 Loss 2.5692\n",
      "Epoch 1 Batch 1910 Loss 2.2339\n",
      "Epoch 1 Batch 1911 Loss 1.8068\n",
      "Epoch 1 Batch 1912 Loss 3.2892\n",
      "Epoch 1 Batch 1913 Loss 2.1539\n",
      "Epoch 1 Batch 1914 Loss 2.2837\n",
      "Epoch 1 Batch 1915 Loss 2.6217\n",
      "Epoch 1 Batch 1916 Loss 2.3346\n",
      "Epoch 1 Batch 1917 Loss 2.0129\n",
      "Epoch 1 Batch 1918 Loss 1.4536\n",
      "Epoch 1 Batch 1919 Loss 1.9354\n",
      "Epoch 1 Batch 1920 Loss 1.6419\n",
      "Epoch 1 Batch 1921 Loss 2.3058\n",
      "Epoch 1 Batch 1922 Loss 2.3742\n",
      "Epoch 1 Batch 1923 Loss 1.8754\n",
      "Epoch 1 Batch 1924 Loss 1.9236\n",
      "Epoch 1 Batch 1925 Loss 2.0238\n",
      "Epoch 1 Batch 1926 Loss 2.4497\n",
      "Epoch 1 Batch 1927 Loss 2.4544\n",
      "Epoch 1 Batch 1928 Loss 2.4490\n",
      "Epoch 1 Batch 1929 Loss 2.0027\n",
      "Epoch 1 Batch 1930 Loss 1.3328\n",
      "Epoch 1 Batch 1931 Loss 2.3379\n",
      "Epoch 1 Batch 1932 Loss 1.9827\n",
      "Epoch 1 Batch 1933 Loss 1.9488\n",
      "Epoch 1 Batch 1934 Loss 1.7538\n",
      "Epoch 1 Batch 1935 Loss 2.4905\n",
      "Epoch 1 Batch 1936 Loss 2.2384\n",
      "Epoch 1 Batch 1937 Loss 2.1224\n",
      "Epoch 1 Batch 1938 Loss 2.0968\n",
      "Epoch 1 Batch 1939 Loss 1.8746\n",
      "Epoch 1 Batch 1940 Loss 2.2584\n",
      "Epoch 1 Batch 1941 Loss 2.3355\n",
      "Epoch 1 Batch 1942 Loss 2.4681\n",
      "Epoch 1 Batch 1943 Loss 2.0827\n",
      "Epoch 1 Batch 1944 Loss 2.0337\n",
      "Epoch 1 Batch 1945 Loss 1.7750\n",
      "Epoch 1 Batch 1946 Loss 2.6369\n",
      "Epoch 1 Batch 1947 Loss 1.9866\n",
      "Epoch 1 Batch 1948 Loss 2.4198\n",
      "Epoch 1 Batch 1949 Loss 1.6707\n",
      "Epoch 1 Batch 1950 Loss 2.1460\n",
      "Epoch 1 Batch 1951 Loss 1.8380\n",
      "Epoch 1 Batch 1952 Loss 1.4255\n",
      "Epoch 1 Batch 1953 Loss 2.2851\n",
      "Epoch 1 Batch 1954 Loss 1.9193\n",
      "Epoch 1 Batch 1955 Loss 2.2199\n",
      "Epoch 1 Batch 1956 Loss 2.6545\n",
      "Epoch 1 Batch 1957 Loss 2.8453\n",
      "Epoch 1 Batch 1958 Loss 2.2016\n",
      "Epoch 1 Batch 1959 Loss 2.4477\n",
      "Epoch 1 Batch 1960 Loss 2.5548\n",
      "Epoch 1 Batch 1961 Loss 1.9973\n",
      "Epoch 1 Batch 1962 Loss 1.8704\n",
      "Epoch 1 Batch 1963 Loss 2.2174\n",
      "Epoch 1 Batch 1964 Loss 1.8483\n",
      "Epoch 1 Batch 1965 Loss 1.9946\n",
      "Epoch 1 Batch 1966 Loss 2.0389\n",
      "Epoch 1 Batch 1967 Loss 1.9089\n",
      "Epoch 1 Batch 1968 Loss 1.7941\n",
      "Epoch 1 Batch 1969 Loss 2.0612\n",
      "Epoch 1 Batch 1970 Loss 2.4396\n",
      "Epoch 1 Batch 1971 Loss 2.6547\n",
      "Epoch 1 Batch 1972 Loss 2.3386\n",
      "Epoch 1 Batch 1973 Loss 2.2712\n",
      "Epoch 1 Batch 1974 Loss 2.0489\n",
      "Epoch 1 Batch 1975 Loss 2.1315\n",
      "Epoch 1 Batch 1976 Loss 2.0992\n",
      "Epoch 1 Batch 1977 Loss 1.5004\n",
      "Epoch 1 Batch 1978 Loss 2.2238\n",
      "Epoch 1 Batch 1979 Loss 1.6614\n",
      "Epoch 1 Batch 1980 Loss 1.8456\n",
      "Epoch 1 Batch 1981 Loss 2.1323\n",
      "Epoch 1 Batch 1982 Loss 1.7026\n",
      "Epoch 1 Batch 1983 Loss 1.9280\n",
      "Epoch 1 Batch 1984 Loss 2.2086\n",
      "Epoch 1 Batch 1985 Loss 2.4660\n",
      "Epoch 1 Batch 1986 Loss 1.7983\n",
      "Epoch 1 Batch 1987 Loss 2.2495\n",
      "Epoch 1 Batch 1988 Loss 2.1426\n",
      "Epoch 1 Batch 1989 Loss 2.1485\n",
      "Epoch 1 Batch 1990 Loss 1.5611\n",
      "Epoch 1 Batch 1991 Loss 1.3363\n",
      "Epoch 1 Batch 1992 Loss 1.9001\n",
      "Epoch 1 Batch 1993 Loss 2.3154\n",
      "Epoch 1 Batch 1994 Loss 1.8209\n",
      "Epoch 1 Batch 1995 Loss 2.0813\n",
      "Epoch 1 Batch 1996 Loss 1.9461\n",
      "Epoch 1 Batch 1997 Loss 1.5379\n",
      "Epoch 1 Batch 1998 Loss 1.9041\n",
      "Epoch 1 Batch 1999 Loss 2.2201\n",
      "Epoch 1 Batch 2000 Loss 1.7520\n",
      "Epoch 1 Batch 2001 Loss 2.4402\n",
      "Epoch 1 Batch 2002 Loss 2.2838\n",
      "Epoch 1 Batch 2003 Loss 2.0605\n",
      "Epoch 1 Batch 2004 Loss 2.7859\n",
      "Epoch 1 Batch 2005 Loss 2.0887\n",
      "Epoch 1 Batch 2006 Loss 2.0780\n",
      "Epoch 1 Batch 2007 Loss 2.6293\n",
      "Epoch 1 Batch 2008 Loss 1.9102\n",
      "Epoch 1 Batch 2009 Loss 1.9365\n",
      "Epoch 1 Batch 2010 Loss 2.9783\n",
      "Epoch 1 Batch 2011 Loss 2.3346\n",
      "Epoch 1 Batch 2012 Loss 2.7066\n",
      "Epoch 1 Batch 2013 Loss 1.9847\n",
      "Epoch 1 Batch 2014 Loss 1.9359\n",
      "Epoch 1 Batch 2015 Loss 1.8843\n",
      "Epoch 1 Batch 2016 Loss 1.7888\n",
      "Epoch 1 Batch 2017 Loss 1.9180\n",
      "Epoch 1 Batch 2018 Loss 1.7053\n",
      "Epoch 1 Batch 2019 Loss 2.3256\n",
      "Epoch 1 Batch 2020 Loss 2.3740\n",
      "Epoch 1 Batch 2021 Loss 2.0244\n",
      "Epoch 1 Batch 2022 Loss 2.1208\n",
      "Epoch 1 Batch 2023 Loss 2.0027\n",
      "Epoch 1 Batch 2024 Loss 2.2210\n",
      "Epoch 1 Batch 2025 Loss 2.0426\n",
      "Epoch 1 Batch 2026 Loss 1.6734\n",
      "Epoch 1 Batch 2027 Loss 1.5153\n",
      "Epoch 1 Batch 2028 Loss 1.5645\n",
      "Epoch 1 Batch 2029 Loss 1.6347\n",
      "Epoch 1 Batch 2030 Loss 1.8524\n",
      "Epoch 1 Batch 2031 Loss 2.2008\n",
      "Epoch 1 Batch 2032 Loss 2.3385\n",
      "Epoch 1 Batch 2033 Loss 1.8172\n",
      "Epoch 1 Batch 2034 Loss 1.8254\n",
      "Epoch 1 Batch 2035 Loss 2.2275\n",
      "Epoch 1 Batch 2036 Loss 1.9301\n",
      "Epoch 1 Batch 2037 Loss 2.4321\n",
      "Epoch 1 Batch 2038 Loss 2.2870\n",
      "Epoch 1 Batch 2039 Loss 1.7982\n",
      "Epoch 1 Batch 2040 Loss 1.9606\n",
      "Epoch 1 Batch 2041 Loss 2.0104\n",
      "Epoch 1 Batch 2042 Loss 1.8473\n",
      "Epoch 1 Batch 2043 Loss 2.5322\n",
      "Epoch 1 Batch 2044 Loss 1.8705\n",
      "Epoch 1 Batch 2045 Loss 2.3846\n",
      "Epoch 1 Batch 2046 Loss 2.3991\n",
      "Epoch 1 Batch 2047 Loss 1.8457\n",
      "Epoch 1 Batch 2048 Loss 1.8776\n",
      "Epoch 1 Batch 2049 Loss 1.7234\n",
      "Epoch 1 Batch 2050 Loss 1.3874\n",
      "Epoch 1 Batch 2051 Loss 1.6657\n",
      "Epoch 1 Batch 2052 Loss 2.0562\n",
      "Epoch 1 Batch 2053 Loss 2.0533\n",
      "Epoch 1 Batch 2054 Loss 2.1357\n",
      "Epoch 1 Batch 2055 Loss 1.6837\n",
      "Epoch 1 Batch 2056 Loss 1.9883\n",
      "Epoch 1 Batch 2057 Loss 1.8084\n",
      "Epoch 1 Batch 2058 Loss 2.1881\n",
      "Epoch 1 Batch 2059 Loss 2.1888\n",
      "Epoch 1 Batch 2060 Loss 2.0234\n",
      "Epoch 1 Batch 2061 Loss 2.2708\n",
      "Epoch 1 Batch 2062 Loss 1.9288\n",
      "Epoch 1 Batch 2063 Loss 2.5777\n",
      "Epoch 1 Batch 2064 Loss 2.2421\n",
      "Epoch 1 Batch 2065 Loss 2.6735\n",
      "Epoch 1 Batch 2066 Loss 2.2137\n",
      "Epoch 1 Batch 2067 Loss 2.1055\n",
      "Epoch 1 Batch 2068 Loss 2.2528\n",
      "Epoch 1 Batch 2069 Loss 2.1417\n",
      "Epoch 1 Batch 2070 Loss 2.0356\n",
      "Epoch 1 Batch 2071 Loss 2.0854\n",
      "Epoch 1 Batch 2072 Loss 2.1842\n",
      "Epoch 1 Batch 2073 Loss 2.2255\n",
      "Epoch 1 Batch 2074 Loss 2.4092\n",
      "Epoch 1 Batch 2075 Loss 2.6465\n",
      "Epoch 1 Batch 2076 Loss 2.0640\n",
      "Epoch 1 Batch 2077 Loss 1.9681\n",
      "Epoch 1 Batch 2078 Loss 2.2308\n",
      "Epoch 1 Batch 2079 Loss 2.2722\n",
      "Epoch 1 Batch 2080 Loss 2.0690\n",
      "Epoch 1 Batch 2081 Loss 1.9883\n",
      "Epoch 1 Batch 2082 Loss 2.3503\n",
      "Epoch 1 Batch 2083 Loss 1.8042\n",
      "Epoch 1 Batch 2084 Loss 1.8453\n",
      "Epoch 1 Batch 2085 Loss 1.5017\n",
      "Epoch 1 Batch 2086 Loss 1.7363\n",
      "Epoch 1 Batch 2087 Loss 1.8841\n",
      "Epoch 1 Batch 2088 Loss 1.7331\n",
      "Epoch 1 Batch 2089 Loss 1.9010\n",
      "Epoch 1 Batch 2090 Loss 2.1996\n",
      "Epoch 1 Batch 2091 Loss 2.1022\n",
      "Epoch 1 Batch 2092 Loss 2.4743\n",
      "Epoch 1 Batch 2093 Loss 2.3183\n",
      "Epoch 1 Batch 2094 Loss 2.1016\n",
      "Epoch 1 Batch 2095 Loss 2.5262\n",
      "Epoch 1 Batch 2096 Loss 2.1225\n",
      "Epoch 1 Batch 2097 Loss 2.5986\n",
      "Epoch 1 Batch 2098 Loss 2.1587\n",
      "Epoch 1 Batch 2099 Loss 2.4376\n",
      "Epoch 1 Batch 2100 Loss 2.3408\n",
      "Epoch 1 Batch 2101 Loss 1.8860\n",
      "Epoch 1 Batch 2102 Loss 2.1653\n",
      "Epoch 1 Batch 2103 Loss 2.1835\n",
      "Epoch 1 Batch 2104 Loss 1.9477\n",
      "Epoch 1 Batch 2105 Loss 2.0827\n",
      "Epoch 1 Batch 2106 Loss 2.4968\n",
      "Epoch 1 Batch 2107 Loss 1.9633\n",
      "Epoch 1 Batch 2108 Loss 2.4207\n",
      "Epoch 1 Batch 2109 Loss 2.5979\n",
      "Epoch 1 Batch 2110 Loss 1.9526\n",
      "Epoch 1 Batch 2111 Loss 1.9228\n",
      "Epoch 1 Batch 2112 Loss 1.9889\n",
      "Epoch 1 Batch 2113 Loss 1.9939\n",
      "Epoch 1 Batch 2114 Loss 1.5049\n",
      "Epoch 1 Batch 2115 Loss 1.6972\n",
      "Epoch 1 Batch 2116 Loss 1.8871\n",
      "Epoch 1 Batch 2117 Loss 2.1594\n",
      "Epoch 1 Batch 2118 Loss 1.7279\n",
      "Epoch 1 Batch 2119 Loss 1.9487\n",
      "Epoch 1 Batch 2120 Loss 1.7569\n",
      "Epoch 1 Batch 2121 Loss 1.5835\n",
      "Epoch 1 Batch 2122 Loss 2.1677\n",
      "Epoch 1 Batch 2123 Loss 2.3507\n",
      "Epoch 1 Batch 2124 Loss 2.5618\n",
      "Epoch 1 Batch 2125 Loss 2.4621\n",
      "Epoch 1 Batch 2126 Loss 2.2803\n",
      "Epoch 1 Batch 2127 Loss 3.0956\n",
      "Epoch 1 Batch 2128 Loss 2.7358\n",
      "Epoch 1 Batch 2129 Loss 2.8385\n",
      "Epoch 1 Batch 2130 Loss 2.0099\n",
      "Epoch 1 Batch 2131 Loss 2.1061\n",
      "Epoch 1 Batch 2132 Loss 2.4649\n",
      "Epoch 1 Batch 2133 Loss 2.0005\n",
      "Epoch 1 Batch 2134 Loss 1.8127\n",
      "Epoch 1 Batch 2135 Loss 1.8721\n",
      "Epoch 1 Batch 2136 Loss 2.1268\n",
      "Epoch 1 Batch 2137 Loss 1.7919\n",
      "Epoch 1 Batch 2138 Loss 2.6343\n",
      "Epoch 1 Batch 2139 Loss 2.2695\n",
      "Epoch 1 Batch 2140 Loss 2.4562\n",
      "Epoch 1 Batch 2141 Loss 2.8389\n",
      "Epoch 1 Batch 2142 Loss 1.9782\n",
      "Epoch 1 Batch 2143 Loss 2.6092\n",
      "Epoch 1 Batch 2144 Loss 2.1148\n",
      "Epoch 1 Batch 2145 Loss 1.5942\n",
      "Epoch 1 Batch 2146 Loss 2.4603\n",
      "Epoch 1 Batch 2147 Loss 1.8021\n",
      "Epoch 1 Batch 2148 Loss 2.0277\n",
      "Epoch 1 Batch 2149 Loss 1.9654\n",
      "Epoch 1 Batch 2150 Loss 2.6119\n",
      "Epoch 1 Batch 2151 Loss 2.3232\n",
      "Epoch 1 Batch 2152 Loss 2.4751\n",
      "Epoch 1 Batch 2153 Loss 2.4900\n",
      "Epoch 1 Batch 2154 Loss 2.9998\n",
      "Epoch 1 Batch 2155 Loss 3.0984\n",
      "Epoch 1 Batch 2156 Loss 2.6367\n",
      "Epoch 1 Batch 2157 Loss 2.2366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 2158 Loss 2.3884\n",
      "Epoch 1 Batch 2159 Loss 2.5296\n",
      "Epoch 1 Batch 2160 Loss 2.2438\n",
      "Epoch 1 Batch 2161 Loss 1.7272\n",
      "Epoch 1 Batch 2162 Loss 1.5775\n",
      "Epoch 1 Batch 2163 Loss 1.7755\n",
      "Epoch 1 Batch 2164 Loss 3.1757\n",
      "Epoch 1 Batch 2165 Loss 1.9436\n",
      "Epoch 1 Batch 2166 Loss 2.2336\n",
      "Epoch 1 Batch 2167 Loss 2.2795\n",
      "Epoch 1 Batch 2168 Loss 1.6203\n",
      "Epoch 1 Batch 2169 Loss 1.6064\n",
      "Epoch 1 Batch 2170 Loss 1.9728\n",
      "Epoch 1 Batch 2171 Loss 1.9386\n",
      "Epoch 1 Batch 2172 Loss 2.1502\n",
      "Epoch 1 Batch 2173 Loss 1.5305\n",
      "Epoch 1 Batch 2174 Loss 2.1126\n",
      "Epoch 1 Batch 2175 Loss 1.6977\n",
      "Epoch 1 Batch 2176 Loss 2.2159\n",
      "Epoch 1 Batch 2177 Loss 1.8575\n",
      "Epoch 1 Batch 2178 Loss 2.2587\n",
      "Epoch 1 Batch 2179 Loss 2.5771\n",
      "Epoch 1 Batch 2180 Loss 1.9621\n",
      "Epoch 1 Batch 2181 Loss 2.2215\n",
      "Epoch 1 Batch 2182 Loss 2.2129\n",
      "Epoch 1 Batch 2183 Loss 1.9554\n",
      "Epoch 1 Batch 2184 Loss 2.0689\n",
      "Epoch 1 Batch 2185 Loss 2.1403\n",
      "Epoch 1 Batch 2186 Loss 1.9843\n",
      "Epoch 1 Batch 2187 Loss 2.0082\n",
      "Epoch 1 Batch 2188 Loss 1.6618\n",
      "Epoch 1 Batch 2189 Loss 1.6455\n",
      "Epoch 1 Batch 2190 Loss 2.3961\n",
      "Epoch 1 Batch 2191 Loss 2.3472\n",
      "Epoch 1 Batch 2192 Loss 2.4259\n",
      "Epoch 1 Batch 2193 Loss 1.8272\n",
      "Epoch 1 Batch 2194 Loss 1.9893\n",
      "Epoch 1 Batch 2195 Loss 1.6738\n",
      "Epoch 1 Batch 2196 Loss 1.4572\n",
      "Epoch 1 Batch 2197 Loss 1.7715\n",
      "Epoch 1 Batch 2198 Loss 1.6933\n",
      "Epoch 1 Batch 2199 Loss 1.7078\n",
      "Epoch 1 Batch 2200 Loss 1.7895\n",
      "Epoch 1 Batch 2201 Loss 1.8001\n",
      "Epoch 1 Batch 2202 Loss 1.6740\n",
      "Epoch 1 Batch 2203 Loss 1.8781\n",
      "Epoch 1 Batch 2204 Loss 2.0445\n",
      "Epoch 1 Batch 2205 Loss 1.9128\n",
      "Epoch 1 Batch 2206 Loss 2.2771\n",
      "Epoch 1 Batch 2207 Loss 2.0331\n",
      "Epoch 1 Batch 2208 Loss 1.7492\n",
      "Epoch 1 Batch 2209 Loss 2.0212\n",
      "Epoch 1 Batch 2210 Loss 2.2519\n",
      "Epoch 1 Batch 2211 Loss 2.5866\n",
      "Epoch 1 Batch 2212 Loss 2.2030\n",
      "Epoch 1 Batch 2213 Loss 2.4209\n",
      "Epoch 1 Batch 2214 Loss 2.3706\n",
      "Epoch 1 Batch 2215 Loss 2.0360\n",
      "Epoch 1 Batch 2216 Loss 2.5764\n",
      "Epoch 1 Batch 2217 Loss 2.0776\n",
      "Epoch 1 Batch 2218 Loss 1.8894\n",
      "Epoch 1 Batch 2219 Loss 1.9969\n",
      "Epoch 1 Batch 2220 Loss 2.1914\n",
      "Epoch 1 Batch 2221 Loss 1.6072\n",
      "Epoch 1 Batch 2222 Loss 1.4306\n",
      "Epoch 1 Batch 2223 Loss 1.7730\n",
      "Epoch 1 Batch 2224 Loss 1.7309\n",
      "Epoch 1 Batch 2225 Loss 1.4767\n",
      "Epoch 1 Batch 2226 Loss 2.0345\n",
      "Epoch 1 Batch 2227 Loss 2.4601\n",
      "Epoch 1 Batch 2228 Loss 2.3795\n",
      "Epoch 1 Batch 2229 Loss 2.0054\n",
      "Epoch 1 Batch 2230 Loss 1.4477\n",
      "Epoch 1 Batch 2231 Loss 2.1173\n",
      "Epoch 1 Batch 2232 Loss 1.3391\n",
      "Epoch 1 Batch 2233 Loss 1.8343\n",
      "Epoch 1 Batch 2234 Loss 2.0621\n",
      "Epoch 1 Batch 2235 Loss 2.3611\n",
      "Epoch 1 Batch 2236 Loss 2.2182\n",
      "Epoch 1 Batch 2237 Loss 2.0190\n",
      "Epoch 1 Batch 2238 Loss 2.4445\n",
      "Epoch 1 Batch 2239 Loss 2.3695\n",
      "Epoch 1 Batch 2240 Loss 2.5833\n",
      "Epoch 1 Batch 2241 Loss 1.6089\n",
      "Epoch 1 Batch 2242 Loss 1.9714\n",
      "Epoch 1 Batch 2243 Loss 1.6204\n",
      "Epoch 1 Batch 2244 Loss 1.8802\n",
      "Epoch 1 Batch 2245 Loss 1.6079\n",
      "Epoch 1 Batch 2246 Loss 1.3800\n",
      "Epoch 1 Batch 2247 Loss 1.6053\n",
      "Epoch 1 Batch 2248 Loss 1.9786\n",
      "Epoch 1 Batch 2249 Loss 1.6022\n",
      "Epoch 1 Batch 2250 Loss 1.6832\n",
      "Epoch 1 Batch 2251 Loss 1.8103\n",
      "Epoch 1 Batch 2252 Loss 1.8342\n",
      "Epoch 1 Batch 2253 Loss 2.0903\n",
      "Epoch 1 Batch 2254 Loss 1.7704\n",
      "Epoch 1 Batch 2255 Loss 1.4959\n",
      "Epoch 1 Batch 2256 Loss 2.1337\n",
      "Epoch 1 Batch 2257 Loss 2.2217\n",
      "Epoch 1 Batch 2258 Loss 1.6178\n",
      "Epoch 1 Batch 2259 Loss 1.7962\n",
      "Epoch 1 Batch 2260 Loss 1.9742\n",
      "Epoch 1 Batch 2261 Loss 1.8954\n",
      "Epoch 1 Batch 2262 Loss 1.8001\n",
      "Epoch 1 Batch 2263 Loss 1.9465\n",
      "Epoch 1 Batch 2264 Loss 1.6891\n",
      "Epoch 1 Batch 2265 Loss 1.7477\n",
      "Epoch 1 Batch 2266 Loss 1.6880\n",
      "Epoch 1 Batch 2267 Loss 2.0520\n",
      "Epoch 1 Batch 2268 Loss 1.7078\n",
      "Epoch 1 Batch 2269 Loss 1.9879\n",
      "Epoch 1 Batch 2270 Loss 1.7504\n",
      "Epoch 1 Batch 2271 Loss 1.7640\n",
      "Epoch 1 Batch 2272 Loss 2.1377\n",
      "Epoch 1 Batch 2273 Loss 1.6462\n",
      "Epoch 1 Batch 2274 Loss 1.9905\n",
      "Epoch 1 Batch 2275 Loss 2.1378\n",
      "Epoch 1 Batch 2276 Loss 1.8723\n",
      "Epoch 1 Batch 2277 Loss 1.9553\n",
      "Epoch 1 Batch 2278 Loss 1.6824\n",
      "Epoch 1 Batch 2279 Loss 2.2094\n",
      "Epoch 1 Batch 2280 Loss 1.5630\n",
      "Epoch 1 Batch 2281 Loss 2.1255\n",
      "Epoch 1 Batch 2282 Loss 1.8142\n",
      "Epoch 1 Batch 2283 Loss 1.6403\n",
      "Epoch 1 Batch 2284 Loss 1.7054\n",
      "Epoch 1 Batch 2285 Loss 1.6855\n",
      "Epoch 1 Batch 2286 Loss 1.7386\n",
      "Epoch 1 Batch 2287 Loss 1.6616\n",
      "Epoch 1 Batch 2288 Loss 1.6211\n",
      "Epoch 1 Batch 2289 Loss 1.6256\n",
      "Epoch 1 Batch 2290 Loss 2.4511\n",
      "Epoch 1 Batch 2291 Loss 1.9541\n",
      "Epoch 1 Batch 2292 Loss 1.5637\n",
      "Epoch 1 Batch 2293 Loss 1.7340\n",
      "Epoch 1 Batch 2294 Loss 1.8513\n",
      "Epoch 1 Batch 2295 Loss 2.4334\n",
      "Epoch 1 Batch 2296 Loss 2.5915\n",
      "Epoch 1 Batch 2297 Loss 1.9827\n",
      "Epoch 1 Batch 2298 Loss 1.3819\n",
      "Epoch 1 Batch 2299 Loss 1.9311\n",
      "Epoch 1 Batch 2300 Loss 1.8727\n",
      "Epoch 1 Batch 2301 Loss 1.5987\n",
      "Epoch 1 Batch 2302 Loss 2.1664\n",
      "Epoch 1 Batch 2303 Loss 1.8406\n",
      "Epoch 1 Batch 2304 Loss 2.2365\n",
      "Epoch 1 Batch 2305 Loss 2.1431\n",
      "Epoch 1 Batch 2306 Loss 2.1761\n",
      "Epoch 1 Batch 2307 Loss 1.9829\n",
      "Epoch 1 Batch 2308 Loss 1.4685\n",
      "Epoch 1 Batch 2309 Loss 1.6110\n",
      "Epoch 1 Batch 2310 Loss 1.7865\n",
      "Epoch 1 Batch 2311 Loss 1.6938\n",
      "Epoch 1 Batch 2312 Loss 2.2192\n",
      "Epoch 1 Batch 2313 Loss 2.3642\n",
      "Epoch 1 Batch 2314 Loss 2.2161\n",
      "Epoch 1 Batch 2315 Loss 1.7276\n",
      "Epoch 1 Batch 2316 Loss 1.7172\n",
      "Epoch 1 Batch 2317 Loss 2.2832\n",
      "Epoch 1 Batch 2318 Loss 2.3887\n",
      "Epoch 1 Batch 2319 Loss 2.1634\n",
      "Epoch 1 Batch 2320 Loss 2.0145\n",
      "Epoch 1 Batch 2321 Loss 1.7017\n",
      "Epoch 1 Batch 2322 Loss 2.0327\n",
      "Epoch 1 Batch 2323 Loss 1.8548\n",
      "Epoch 1 Batch 2324 Loss 2.0601\n",
      "Epoch 1 Batch 2325 Loss 1.9047\n",
      "Epoch 1 Batch 2326 Loss 1.9037\n",
      "Epoch 1 Batch 2327 Loss 1.6075\n",
      "Epoch 1 Batch 2328 Loss 1.4636\n",
      "Epoch 1 Batch 2329 Loss 1.4402\n",
      "Epoch 1 Batch 2330 Loss 1.6746\n",
      "Epoch 1 Batch 2331 Loss 1.0913\n",
      "Epoch 1 Batch 2332 Loss 1.4529\n",
      "Epoch 1 Batch 2333 Loss 1.3530\n",
      "Epoch 1 Batch 2334 Loss 1.8069\n",
      "Epoch 1 Batch 2335 Loss 1.7790\n",
      "Epoch 1 Batch 2336 Loss 1.6552\n",
      "Epoch 1 Batch 2337 Loss 1.6294\n",
      "Epoch 1 Batch 2338 Loss 1.3985\n",
      "Epoch 1 Batch 2339 Loss 2.0598\n",
      "Epoch 1 Batch 2340 Loss 1.9385\n",
      "Epoch 1 Batch 2341 Loss 1.6433\n",
      "Epoch 1 Batch 2342 Loss 1.4584\n",
      "Epoch 1 Batch 2343 Loss 1.9142\n",
      "Epoch 1 Batch 2344 Loss 1.8215\n",
      "Epoch 1 Batch 2345 Loss 2.1067\n",
      "Epoch 1 Batch 2346 Loss 2.2201\n",
      "Epoch 1 Batch 2347 Loss 1.6581\n",
      "Epoch 1 Batch 2348 Loss 1.7811\n",
      "Epoch 1 Batch 2349 Loss 1.7403\n",
      "Epoch 1 Batch 2350 Loss 1.7793\n",
      "Epoch 1 Batch 2351 Loss 1.7228\n",
      "Epoch 1 Batch 2352 Loss 1.9472\n",
      "Epoch 1 Batch 2353 Loss 1.6764\n",
      "Epoch 1 Batch 2354 Loss 1.8108\n",
      "Epoch 1 Batch 2355 Loss 1.6599\n",
      "Epoch 1 Batch 2356 Loss 1.7100\n",
      "Epoch 1 Batch 2357 Loss 1.6827\n",
      "Epoch 1 Batch 2358 Loss 1.3585\n",
      "Epoch 1 Batch 2359 Loss 1.6962\n",
      "Epoch 1 Batch 2360 Loss 1.8312\n",
      "Epoch 1 Batch 2361 Loss 1.3189\n",
      "Epoch 1 Batch 2362 Loss 1.5773\n",
      "Epoch 1 Batch 2363 Loss 1.8889\n",
      "Epoch 1 Batch 2364 Loss 2.0616\n",
      "Epoch 1 Batch 2365 Loss 1.6545\n",
      "Epoch 1 Batch 2366 Loss 1.6402\n",
      "Epoch 1 Batch 2367 Loss 1.8447\n",
      "Epoch 1 Batch 2368 Loss 1.6283\n",
      "Epoch 1 Batch 2369 Loss 1.6377\n",
      "Epoch 1 Batch 2370 Loss 1.5704\n",
      "Epoch 1 Batch 2371 Loss 1.3371\n",
      "Epoch 1 Batch 2372 Loss 1.3814\n",
      "Epoch 1 Batch 2373 Loss 1.7693\n",
      "Epoch 1 Batch 2374 Loss 1.5908\n",
      "Epoch 1 Batch 2375 Loss 1.5357\n",
      "Epoch 1 Batch 2376 Loss 1.4310\n",
      "Epoch 1 Batch 2377 Loss 1.4777\n",
      "Epoch 1 Batch 2378 Loss 1.5974\n",
      "Epoch 1 Batch 2379 Loss 2.0829\n",
      "Epoch 1 Batch 2380 Loss 1.9067\n",
      "Epoch 1 Batch 2381 Loss 1.6013\n",
      "Epoch 1 Batch 2382 Loss 1.8183\n",
      "Epoch 1 Batch 2383 Loss 2.3004\n",
      "Epoch 1 Batch 2384 Loss 2.1521\n",
      "Epoch 1 Batch 2385 Loss 1.6110\n",
      "Epoch 1 Batch 2386 Loss 2.0735\n",
      "Epoch 1 Batch 2387 Loss 1.9076\n",
      "Epoch 1 Batch 2388 Loss 1.7880\n",
      "Epoch 1 Batch 2389 Loss 2.0630\n",
      "Epoch 1 Batch 2390 Loss 2.1041\n",
      "Epoch 1 Batch 2391 Loss 1.7024\n",
      "Epoch 1 Batch 2392 Loss 2.0922\n",
      "Epoch 1 Batch 2393 Loss 1.7650\n",
      "Epoch 1 Batch 2394 Loss 1.4729\n",
      "Epoch 1 Batch 2395 Loss 1.3457\n",
      "Epoch 1 Batch 2396 Loss 2.3451\n",
      "Epoch 1 Batch 2397 Loss 1.6826\n",
      "Epoch 1 Batch 2398 Loss 1.3795\n",
      "Epoch 1 Batch 2399 Loss 1.3913\n",
      "Epoch 1 Batch 2400 Loss 1.4866\n",
      "Epoch 1 Batch 2401 Loss 1.4319\n",
      "Epoch 1 Batch 2402 Loss 1.6402\n",
      "Epoch 1 Batch 2403 Loss 2.0785\n",
      "Epoch 1 Batch 2404 Loss 1.8114\n",
      "Epoch 1 Batch 2405 Loss 1.8417\n",
      "Epoch 1 Batch 2406 Loss 2.0367\n",
      "Epoch 1 Batch 2407 Loss 2.1475\n",
      "Epoch 1 Batch 2408 Loss 2.1861\n",
      "Epoch 1 Batch 2409 Loss 1.7674\n",
      "Epoch 1 Batch 2410 Loss 2.2597\n",
      "Epoch 1 Batch 2411 Loss 1.3962\n",
      "Epoch 1 Batch 2412 Loss 1.7316\n",
      "Epoch 1 Batch 2413 Loss 1.5036\n",
      "Epoch 1 Batch 2414 Loss 1.5197\n",
      "Epoch 1 Batch 2415 Loss 1.6883\n",
      "Epoch 1 Batch 2416 Loss 2.1496\n",
      "Epoch 1 Batch 2417 Loss 1.5144\n",
      "Epoch 1 Batch 2418 Loss 1.6118\n",
      "Epoch 1 Batch 2419 Loss 1.5128\n",
      "Epoch 1 Batch 2420 Loss 1.3631\n",
      "Epoch 1 Batch 2421 Loss 1.3790\n",
      "Epoch 1 Batch 2422 Loss 1.3892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 2423 Loss 1.7813\n",
      "Epoch 1 Batch 2424 Loss 1.8407\n",
      "Epoch 1 Batch 2425 Loss 1.8700\n",
      "Epoch 1 Batch 2426 Loss 1.5934\n",
      "Epoch 1 Batch 2427 Loss 2.0567\n",
      "Epoch 1 Batch 2428 Loss 1.9167\n",
      "Epoch 1 Batch 2429 Loss 1.6275\n",
      "Epoch 1 Batch 2430 Loss 1.2497\n",
      "Epoch 1 Batch 2431 Loss 1.2487\n",
      "Epoch 1 Batch 2432 Loss 1.7100\n",
      "Epoch 1 Batch 2433 Loss 1.6126\n",
      "Epoch 1 Batch 2434 Loss 1.4995\n",
      "Epoch 1 Batch 2435 Loss 1.2786\n",
      "Epoch 1 Batch 2436 Loss 1.6552\n",
      "Epoch 1 Batch 2437 Loss 1.7767\n",
      "Epoch 1 Batch 2438 Loss 1.6656\n",
      "Epoch 1 Batch 2439 Loss 1.9592\n",
      "Epoch 1 Batch 2440 Loss 1.5365\n",
      "Epoch 1 Batch 2441 Loss 1.4588\n",
      "Epoch 1 Batch 2442 Loss 1.4257\n",
      "Epoch 1 Batch 2443 Loss 1.3978\n",
      "Epoch 1 Batch 2444 Loss 1.6049\n",
      "Epoch 1 Batch 2445 Loss 1.6749\n",
      "Epoch 1 Batch 2446 Loss 1.5781\n",
      "Epoch 1 Batch 2447 Loss 1.5970\n",
      "Epoch 1 Batch 2448 Loss 1.5103\n",
      "Epoch 1 Batch 2449 Loss 1.4395\n",
      "Epoch 1 Batch 2450 Loss 1.7926\n",
      "Epoch 1 Batch 2451 Loss 1.8084\n",
      "Epoch 1 Batch 2452 Loss 2.7646\n",
      "Epoch 1 Batch 2453 Loss 2.4109\n",
      "Epoch 1 Batch 2454 Loss 2.1096\n",
      "Epoch 1 Batch 2455 Loss 1.6832\n",
      "Epoch 1 Batch 2456 Loss 1.8556\n",
      "Epoch 1 Batch 2457 Loss 1.7542\n",
      "Epoch 1 Batch 2458 Loss 1.5684\n",
      "Epoch 1 Batch 2459 Loss 2.2774\n",
      "Epoch 1 Batch 2460 Loss 2.1839\n",
      "Epoch 1 Batch 2461 Loss 1.5986\n",
      "Epoch 1 Batch 2462 Loss 1.6619\n",
      "Epoch 1 Batch 2463 Loss 1.4187\n",
      "Epoch 1 Batch 2464 Loss 1.3799\n",
      "Epoch 1 Batch 2465 Loss 1.6651\n",
      "Epoch 1 Batch 2466 Loss 1.4598\n",
      "Epoch 1 Batch 2467 Loss 1.7279\n",
      "Epoch 1 Batch 2468 Loss 1.3401\n",
      "Epoch 1 Batch 2469 Loss 1.7366\n",
      "Epoch 1 Batch 2470 Loss 2.7422\n",
      "Epoch 1 Batch 2471 Loss 2.4212\n",
      "Epoch 1 Batch 2472 Loss 2.1898\n",
      "Epoch 1 Batch 2473 Loss 1.7174\n",
      "Epoch 1 Batch 2474 Loss 1.6170\n",
      "Epoch 1 Batch 2475 Loss 1.4779\n",
      "Epoch 1 Batch 2476 Loss 1.3381\n",
      "Epoch 1 Batch 2477 Loss 1.4405\n",
      "Epoch 1 Batch 2478 Loss 1.2176\n",
      "Epoch 1 Batch 2479 Loss 1.0858\n",
      "Epoch 1 Batch 2480 Loss 1.2937\n",
      "Epoch 1 Batch 2481 Loss 1.3705\n",
      "Epoch 1 Batch 2482 Loss 1.4479\n",
      "Epoch 1 Batch 2483 Loss 2.0542\n",
      "Epoch 1 Batch 2484 Loss 1.9096\n",
      "Epoch 1 Batch 2485 Loss 1.7974\n",
      "Epoch 1 Batch 2486 Loss 1.7446\n",
      "Epoch 1 Batch 2487 Loss 1.5777\n",
      "Epoch 1 Batch 2488 Loss 1.9612\n",
      "Epoch 1 Batch 2489 Loss 2.0301\n",
      "Epoch 1 Batch 2490 Loss 1.6542\n",
      "Epoch 1 Batch 2491 Loss 1.9372\n",
      "Epoch 1 Batch 2492 Loss 1.8468\n",
      "Epoch 1 Batch 2493 Loss 2.0895\n",
      "Epoch 1 Batch 2494 Loss 1.7579\n",
      "Epoch 1 Batch 2495 Loss 1.8401\n",
      "Epoch 1 Batch 2496 Loss 1.6963\n",
      "Epoch 1 Batch 2497 Loss 2.0066\n",
      "Epoch 1 Batch 2498 Loss 2.2377\n",
      "Epoch 1 Batch 2499 Loss 1.6849\n",
      "Epoch 1 Batch 2500 Loss 1.8489\n",
      "Epoch 1 Batch 2501 Loss 1.5467\n",
      "Epoch 1 Batch 2502 Loss 1.5220\n",
      "Epoch 1 Batch 2503 Loss 1.2927\n",
      "Epoch 1 Batch 2504 Loss 1.5967\n",
      "Epoch 1 Batch 2505 Loss 1.5049\n",
      "Epoch 1 Batch 2506 Loss 1.8843\n",
      "Epoch 1 Batch 2507 Loss 1.5229\n",
      "Epoch 1 Batch 2508 Loss 1.7701\n",
      "Epoch 1 Batch 2509 Loss 1.3190\n",
      "Epoch 1 Batch 2510 Loss 1.6605\n",
      "Epoch 1 Batch 2511 Loss 1.8225\n",
      "Epoch 1 Batch 2512 Loss 1.9533\n",
      "Epoch 1 Batch 2513 Loss 1.8338\n",
      "Epoch 1 Batch 2514 Loss 1.8307\n",
      "Epoch 1 Batch 2515 Loss 1.9354\n",
      "Epoch 1 Batch 2516 Loss 1.5827\n",
      "Epoch 1 Batch 2517 Loss 1.8963\n",
      "Epoch 1 Batch 2518 Loss 1.6474\n",
      "Epoch 1 Batch 2519 Loss 1.5611\n",
      "Epoch 1 Batch 2520 Loss 2.1991\n",
      "Epoch 1 Batch 2521 Loss 1.8197\n",
      "Epoch 1 Batch 2522 Loss 1.7147\n",
      "Epoch 1 Batch 2523 Loss 2.0159\n",
      "Epoch 1 Batch 2524 Loss 2.0622\n",
      "Epoch 1 Batch 2525 Loss 1.8249\n",
      "Epoch 1 Batch 2526 Loss 1.9283\n",
      "Epoch 1 Batch 2527 Loss 2.0214\n",
      "Epoch 1 Batch 2528 Loss 2.4058\n",
      "Epoch 1 Batch 2529 Loss 2.1177\n",
      "Epoch 1 Batch 2530 Loss 1.9566\n",
      "Epoch 1 Batch 2531 Loss 1.8764\n",
      "Epoch 1 Batch 2532 Loss 1.7233\n",
      "Epoch 1 Batch 2533 Loss 1.7266\n",
      "Epoch 1 Batch 2534 Loss 1.8076\n",
      "Epoch 1 Batch 2535 Loss 1.8232\n",
      "Epoch 1 Batch 2536 Loss 1.7286\n",
      "Epoch 1 Batch 2537 Loss 1.6456\n",
      "Epoch 1 Batch 2538 Loss 1.6795\n",
      "Epoch 1 Batch 2539 Loss 1.5008\n",
      "Epoch 1 Batch 2540 Loss 1.7434\n",
      "Epoch 1 Batch 2541 Loss 1.3257\n",
      "Epoch 1 Batch 2542 Loss 1.6498\n",
      "Epoch 1 Batch 2543 Loss 1.2153\n",
      "Epoch 1 Batch 2544 Loss 1.4468\n",
      "Epoch 1 Batch 2545 Loss 1.9454\n",
      "Epoch 1 Batch 2546 Loss 1.7165\n",
      "Epoch 1 Batch 2547 Loss 1.5760\n",
      "Epoch 1 Batch 2548 Loss 1.7043\n",
      "Epoch 1 Batch 2549 Loss 2.3495\n",
      "Epoch 1 Batch 2550 Loss 1.8127\n",
      "Epoch 1 Batch 2551 Loss 2.0000\n",
      "Epoch 1 Batch 2552 Loss 1.8920\n",
      "Epoch 1 Batch 2553 Loss 1.6928\n",
      "Epoch 1 Batch 2554 Loss 2.2502\n",
      "Epoch 1 Batch 2555 Loss 2.1518\n",
      "Epoch 1 Batch 2556 Loss 1.6535\n",
      "Epoch 1 Batch 2557 Loss 1.6954\n",
      "Epoch 1 Batch 2558 Loss 1.7340\n",
      "Epoch 1 Batch 2559 Loss 1.7508\n",
      "Epoch 1 Batch 2560 Loss 1.5673\n",
      "Epoch 1 Batch 2561 Loss 1.4564\n",
      "Epoch 1 Batch 2562 Loss 1.1828\n",
      "Epoch 1 Batch 2563 Loss 1.3487\n",
      "Epoch 1 Batch 2564 Loss 1.3654\n",
      "Epoch 1 Batch 2565 Loss 1.5058\n",
      "Epoch 1 Batch 2566 Loss 1.5184\n",
      "Epoch 1 Batch 2567 Loss 1.4177\n",
      "Epoch 1 Batch 2568 Loss 1.7554\n",
      "Epoch 1 Batch 2569 Loss 1.6444\n",
      "Epoch 1 Batch 2570 Loss 1.9937\n",
      "Epoch 1 Batch 2571 Loss 1.8468\n",
      "Epoch 1 Batch 2572 Loss 1.7156\n",
      "Epoch 1 Batch 2573 Loss 1.9623\n",
      "Epoch 1 Batch 2574 Loss 2.3292\n",
      "Epoch 1 Batch 2575 Loss 1.5661\n",
      "Epoch 1 Batch 2576 Loss 1.6318\n",
      "Epoch 1 Batch 2577 Loss 1.6747\n",
      "Epoch 1 Batch 2578 Loss 1.1965\n",
      "Epoch 1 Batch 2579 Loss 1.7715\n",
      "Epoch 1 Batch 2580 Loss 1.4917\n",
      "Epoch 1 Batch 2581 Loss 1.9488\n",
      "Epoch 1 Batch 2582 Loss 2.1086\n",
      "Epoch 1 Batch 2583 Loss 2.0245\n",
      "Epoch 1 Batch 2584 Loss 1.8443\n",
      "Epoch 1 Batch 2585 Loss 2.0485\n",
      "Epoch 1 Batch 2586 Loss 1.7213\n",
      "Epoch 1 Batch 2587 Loss 1.4462\n",
      "Epoch 1 Batch 2588 Loss 1.4780\n",
      "Epoch 1 Batch 2589 Loss 1.3113\n",
      "Epoch 1 Batch 2590 Loss 1.7489\n",
      "Epoch 1 Loss 2.1293\n",
      "Time taken for 1 epoch 483.87456583976746 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 2.6661\n",
      "Epoch 2 Batch 1 Loss 3.0656\n",
      "Epoch 2 Batch 2 Loss 2.5371\n",
      "Epoch 2 Batch 3 Loss 2.4774\n",
      "Epoch 2 Batch 4 Loss 2.1196\n",
      "Epoch 2 Batch 5 Loss 2.2130\n",
      "Epoch 2 Batch 6 Loss 2.1920\n",
      "Epoch 2 Batch 7 Loss 2.0513\n",
      "Epoch 2 Batch 8 Loss 2.4363\n",
      "Epoch 2 Batch 9 Loss 2.7092\n",
      "Epoch 2 Batch 10 Loss 1.8681\n",
      "Epoch 2 Batch 11 Loss 1.8786\n",
      "Epoch 2 Batch 12 Loss 1.7506\n",
      "Epoch 2 Batch 13 Loss 1.4825\n",
      "Epoch 2 Batch 14 Loss 1.3467\n",
      "Epoch 2 Batch 15 Loss 1.7330\n",
      "Epoch 2 Batch 16 Loss 1.5308\n",
      "Epoch 2 Batch 17 Loss 1.6481\n",
      "Epoch 2 Batch 18 Loss 1.7979\n",
      "Epoch 2 Batch 19 Loss 1.7787\n",
      "Epoch 2 Batch 20 Loss 2.2847\n",
      "Epoch 2 Batch 21 Loss 1.8222\n",
      "Epoch 2 Batch 22 Loss 1.5737\n",
      "Epoch 2 Batch 23 Loss 2.0835\n",
      "Epoch 2 Batch 24 Loss 1.7410\n",
      "Epoch 2 Batch 25 Loss 1.7060\n",
      "Epoch 2 Batch 26 Loss 1.6386\n",
      "Epoch 2 Batch 27 Loss 2.1248\n",
      "Epoch 2 Batch 28 Loss 2.3484\n",
      "Epoch 2 Batch 29 Loss 1.9906\n",
      "Epoch 2 Batch 30 Loss 2.2621\n",
      "Epoch 2 Batch 31 Loss 2.3360\n",
      "Epoch 2 Batch 32 Loss 1.6140\n",
      "Epoch 2 Batch 33 Loss 1.6093\n",
      "Epoch 2 Batch 34 Loss 1.1980\n",
      "Epoch 2 Batch 35 Loss 1.1467\n",
      "Epoch 2 Batch 36 Loss 1.0662\n",
      "Epoch 2 Batch 37 Loss 1.1128\n",
      "Epoch 2 Batch 38 Loss 0.9588\n",
      "Epoch 2 Batch 39 Loss 1.4672\n",
      "Epoch 2 Batch 40 Loss 1.2045\n",
      "Epoch 2 Batch 41 Loss 1.3702\n",
      "Epoch 2 Batch 42 Loss 1.5086\n",
      "Epoch 2 Batch 43 Loss 1.4425\n",
      "Epoch 2 Batch 44 Loss 1.5672\n",
      "Epoch 2 Batch 45 Loss 1.9320\n",
      "Epoch 2 Batch 46 Loss 1.7779\n",
      "Epoch 2 Batch 47 Loss 1.8579\n",
      "Epoch 2 Batch 48 Loss 1.5979\n",
      "Epoch 2 Batch 49 Loss 1.8101\n",
      "Epoch 2 Batch 50 Loss 1.5413\n",
      "Epoch 2 Batch 51 Loss 1.9192\n",
      "Epoch 2 Batch 52 Loss 1.8675\n",
      "Epoch 2 Batch 53 Loss 1.4373\n",
      "Epoch 2 Batch 54 Loss 2.0806\n",
      "Epoch 2 Batch 55 Loss 2.0680\n",
      "Epoch 2 Batch 56 Loss 2.6873\n",
      "Epoch 2 Batch 57 Loss 2.3686\n",
      "Epoch 2 Batch 58 Loss 1.9622\n",
      "Epoch 2 Batch 59 Loss 2.4593\n",
      "Epoch 2 Batch 60 Loss 2.0555\n",
      "Epoch 2 Batch 61 Loss 1.6832\n",
      "Epoch 2 Batch 62 Loss 1.8200\n",
      "Epoch 2 Batch 63 Loss 1.8490\n",
      "Epoch 2 Batch 64 Loss 1.9437\n",
      "Epoch 2 Batch 65 Loss 1.8291\n",
      "Epoch 2 Batch 66 Loss 1.3446\n",
      "Epoch 2 Batch 67 Loss 1.8119\n",
      "Epoch 2 Batch 68 Loss 1.4714\n",
      "Epoch 2 Batch 69 Loss 2.0654\n",
      "Epoch 2 Batch 70 Loss 2.2492\n",
      "Epoch 2 Batch 71 Loss 1.9244\n",
      "Epoch 2 Batch 72 Loss 1.2190\n",
      "Epoch 2 Batch 73 Loss 2.2804\n",
      "Epoch 2 Batch 74 Loss 2.1149\n",
      "Epoch 2 Batch 75 Loss 2.5468\n",
      "Epoch 2 Batch 76 Loss 1.8934\n",
      "Epoch 2 Batch 77 Loss 2.2646\n",
      "Epoch 2 Batch 78 Loss 1.6796\n",
      "Epoch 2 Batch 79 Loss 2.2029\n",
      "Epoch 2 Batch 80 Loss 2.1092\n",
      "Epoch 2 Batch 81 Loss 1.9311\n",
      "Epoch 2 Batch 82 Loss 1.7817\n",
      "Epoch 2 Batch 83 Loss 1.7659\n",
      "Epoch 2 Batch 84 Loss 2.0824\n",
      "Epoch 2 Batch 85 Loss 1.9312\n",
      "Epoch 2 Batch 86 Loss 1.9564\n",
      "Epoch 2 Batch 87 Loss 2.0339\n",
      "Epoch 2 Batch 88 Loss 1.7441\n",
      "Epoch 2 Batch 89 Loss 1.7318\n",
      "Epoch 2 Batch 90 Loss 1.8337\n",
      "Epoch 2 Batch 91 Loss 2.0970\n",
      "Epoch 2 Batch 92 Loss 1.8437\n",
      "Epoch 2 Batch 93 Loss 1.4159\n",
      "Epoch 2 Batch 94 Loss 1.2022\n",
      "Epoch 2 Batch 95 Loss 1.1648\n",
      "Epoch 2 Batch 96 Loss 1.8500\n",
      "Epoch 2 Batch 97 Loss 2.0284\n",
      "Epoch 2 Batch 98 Loss 2.3226\n",
      "Epoch 2 Batch 99 Loss 2.3417\n",
      "Epoch 2 Batch 100 Loss 2.4025\n",
      "Epoch 2 Batch 101 Loss 2.1234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Batch 102 Loss 2.2204\n",
      "Epoch 2 Batch 103 Loss 2.1744\n",
      "Epoch 2 Batch 104 Loss 2.0980\n",
      "Epoch 2 Batch 105 Loss 1.9441\n",
      "Epoch 2 Batch 106 Loss 2.1995\n",
      "Epoch 2 Batch 107 Loss 1.9849\n",
      "Epoch 2 Batch 108 Loss 2.2588\n",
      "Epoch 2 Batch 109 Loss 1.5151\n",
      "Epoch 2 Batch 110 Loss 1.4219\n",
      "Epoch 2 Batch 111 Loss 1.2025\n",
      "Epoch 2 Batch 112 Loss 1.8161\n",
      "Epoch 2 Batch 113 Loss 1.8331\n",
      "Epoch 2 Batch 114 Loss 1.8259\n",
      "Epoch 2 Batch 115 Loss 1.8014\n",
      "Epoch 2 Batch 116 Loss 2.0393\n",
      "Epoch 2 Batch 117 Loss 2.0928\n",
      "Epoch 2 Batch 118 Loss 1.7953\n",
      "Epoch 2 Batch 119 Loss 1.6497\n",
      "Epoch 2 Batch 120 Loss 1.7831\n",
      "Epoch 2 Batch 121 Loss 1.7864\n",
      "Epoch 2 Batch 122 Loss 1.9396\n",
      "Epoch 2 Batch 123 Loss 1.5450\n",
      "Epoch 2 Batch 124 Loss 1.6809\n",
      "Epoch 2 Batch 125 Loss 1.5374\n",
      "Epoch 2 Batch 126 Loss 1.2471\n",
      "Epoch 2 Batch 127 Loss 1.6255\n",
      "Epoch 2 Batch 128 Loss 1.3429\n",
      "Epoch 2 Batch 129 Loss 2.0075\n",
      "Epoch 2 Batch 130 Loss 1.8124\n",
      "Epoch 2 Batch 131 Loss 1.7176\n",
      "Epoch 2 Batch 132 Loss 1.6227\n",
      "Epoch 2 Batch 133 Loss 1.6996\n",
      "Epoch 2 Batch 134 Loss 2.6204\n",
      "Epoch 2 Batch 135 Loss 1.7540\n",
      "Epoch 2 Batch 136 Loss 1.6197\n",
      "Epoch 2 Batch 137 Loss 1.9512\n",
      "Epoch 2 Batch 138 Loss 1.8814\n",
      "Epoch 2 Batch 139 Loss 1.7668\n",
      "Epoch 2 Batch 140 Loss 1.6263\n",
      "Epoch 2 Batch 141 Loss 1.5212\n",
      "Epoch 2 Batch 142 Loss 1.9223\n",
      "Epoch 2 Batch 143 Loss 2.2461\n",
      "Epoch 2 Batch 144 Loss 1.8792\n",
      "Epoch 2 Batch 145 Loss 2.3329\n",
      "Epoch 2 Batch 146 Loss 1.3247\n",
      "Epoch 2 Batch 147 Loss 1.9896\n",
      "Epoch 2 Batch 148 Loss 1.9432\n",
      "Epoch 2 Batch 149 Loss 2.1265\n",
      "Epoch 2 Batch 150 Loss 2.4969\n",
      "Epoch 2 Batch 151 Loss 1.9238\n",
      "Epoch 2 Batch 152 Loss 1.8750\n",
      "Epoch 2 Batch 153 Loss 1.9058\n",
      "Epoch 2 Batch 154 Loss 1.5349\n",
      "Epoch 2 Batch 155 Loss 2.0306\n",
      "Epoch 2 Batch 156 Loss 1.7323\n",
      "Epoch 2 Batch 157 Loss 2.7184\n",
      "Epoch 2 Batch 158 Loss 2.1774\n",
      "Epoch 2 Batch 159 Loss 2.0538\n",
      "Epoch 2 Batch 160 Loss 2.2228\n",
      "Epoch 2 Batch 161 Loss 2.2042\n",
      "Epoch 2 Batch 162 Loss 1.4625\n",
      "Epoch 2 Batch 163 Loss 1.2633\n",
      "Epoch 2 Batch 164 Loss 1.0170\n",
      "Epoch 2 Batch 165 Loss 1.8768\n",
      "Epoch 2 Batch 166 Loss 1.9141\n",
      "Epoch 2 Batch 167 Loss 1.9606\n",
      "Epoch 2 Batch 168 Loss 1.7986\n",
      "Epoch 2 Batch 169 Loss 1.6564\n",
      "Epoch 2 Batch 170 Loss 1.8302\n",
      "Epoch 2 Batch 171 Loss 2.1869\n",
      "Epoch 2 Batch 172 Loss 1.7113\n",
      "Epoch 2 Batch 173 Loss 1.3712\n",
      "Epoch 2 Batch 174 Loss 1.7808\n",
      "Epoch 2 Batch 175 Loss 1.7570\n",
      "Epoch 2 Batch 176 Loss 1.8510\n",
      "Epoch 2 Batch 177 Loss 1.6403\n",
      "Epoch 2 Batch 178 Loss 1.8601\n",
      "Epoch 2 Batch 179 Loss 1.3362\n",
      "Epoch 2 Batch 180 Loss 1.6134\n",
      "Epoch 2 Batch 181 Loss 1.2982\n",
      "Epoch 2 Batch 182 Loss 1.4931\n",
      "Epoch 2 Batch 183 Loss 1.4981\n",
      "Epoch 2 Batch 184 Loss 1.0323\n",
      "Epoch 2 Batch 185 Loss 1.1901\n",
      "Epoch 2 Batch 186 Loss 1.3134\n",
      "Epoch 2 Batch 187 Loss 1.2967\n",
      "Epoch 2 Batch 188 Loss 1.5332\n",
      "Epoch 2 Batch 189 Loss 1.4240\n",
      "Epoch 2 Batch 190 Loss 2.0563\n",
      "Epoch 2 Batch 191 Loss 2.2689\n",
      "Epoch 2 Batch 192 Loss 1.7811\n",
      "Epoch 2 Batch 193 Loss 1.8975\n",
      "Epoch 2 Batch 194 Loss 2.1895\n",
      "Epoch 2 Batch 195 Loss 1.9420\n",
      "Epoch 2 Batch 196 Loss 1.8590\n",
      "Epoch 2 Batch 197 Loss 1.7878\n",
      "Epoch 2 Batch 198 Loss 2.3140\n",
      "Epoch 2 Batch 199 Loss 2.0447\n",
      "Epoch 2 Batch 200 Loss 1.8441\n",
      "Epoch 2 Batch 201 Loss 1.5008\n",
      "Epoch 2 Batch 202 Loss 1.4413\n",
      "Epoch 2 Batch 203 Loss 1.6573\n",
      "Epoch 2 Batch 204 Loss 1.5388\n",
      "Epoch 2 Batch 205 Loss 1.4832\n",
      "Epoch 2 Batch 206 Loss 1.9148\n",
      "Epoch 2 Batch 207 Loss 2.0454\n",
      "Epoch 2 Batch 208 Loss 1.7102\n",
      "Epoch 2 Batch 209 Loss 1.5434\n",
      "Epoch 2 Batch 210 Loss 1.6646\n",
      "Epoch 2 Batch 211 Loss 1.5183\n",
      "Epoch 2 Batch 212 Loss 1.7335\n",
      "Epoch 2 Batch 213 Loss 1.6551\n",
      "Epoch 2 Batch 214 Loss 1.5476\n",
      "Epoch 2 Batch 215 Loss 1.8396\n",
      "Epoch 2 Batch 216 Loss 2.2800\n",
      "Epoch 2 Batch 217 Loss 1.7767\n",
      "Epoch 2 Batch 218 Loss 1.4756\n",
      "Epoch 2 Batch 219 Loss 2.1566\n",
      "Epoch 2 Batch 220 Loss 1.7306\n",
      "Epoch 2 Batch 221 Loss 1.4517\n",
      "Epoch 2 Batch 222 Loss 1.6192\n",
      "Epoch 2 Batch 223 Loss 1.5931\n",
      "Epoch 2 Batch 224 Loss 1.6077\n",
      "Epoch 2 Batch 225 Loss 1.3270\n",
      "Epoch 2 Batch 226 Loss 1.9781\n",
      "Epoch 2 Batch 227 Loss 1.6791\n",
      "Epoch 2 Batch 228 Loss 2.0403\n",
      "Epoch 2 Batch 229 Loss 1.5527\n",
      "Epoch 2 Batch 230 Loss 1.6578\n",
      "Epoch 2 Batch 231 Loss 1.6271\n",
      "Epoch 2 Batch 232 Loss 1.3565\n",
      "Epoch 2 Batch 233 Loss 1.1193\n",
      "Epoch 2 Batch 234 Loss 1.3092\n",
      "Epoch 2 Batch 235 Loss 1.2346\n",
      "Epoch 2 Batch 236 Loss 1.8613\n",
      "Epoch 2 Batch 237 Loss 1.1854\n",
      "Epoch 2 Batch 238 Loss 1.9364\n",
      "Epoch 2 Batch 239 Loss 1.9140\n",
      "Epoch 2 Batch 240 Loss 1.5278\n",
      "Epoch 2 Batch 241 Loss 2.1000\n",
      "Epoch 2 Batch 242 Loss 1.9023\n",
      "Epoch 2 Batch 243 Loss 1.6792\n",
      "Epoch 2 Batch 244 Loss 1.8200\n",
      "Epoch 2 Batch 245 Loss 1.9343\n",
      "Epoch 2 Batch 246 Loss 1.6487\n",
      "Epoch 2 Batch 247 Loss 1.8488\n",
      "Epoch 2 Batch 248 Loss 1.9660\n",
      "Epoch 2 Batch 249 Loss 2.0110\n",
      "Epoch 2 Batch 250 Loss 1.6699\n",
      "Epoch 2 Batch 251 Loss 1.6878\n",
      "Epoch 2 Batch 252 Loss 1.9873\n",
      "Epoch 2 Batch 253 Loss 1.9140\n",
      "Epoch 2 Batch 254 Loss 1.9975\n",
      "Epoch 2 Batch 255 Loss 2.1143\n",
      "Epoch 2 Batch 256 Loss 2.2205\n",
      "Epoch 2 Batch 257 Loss 1.7622\n",
      "Epoch 2 Batch 258 Loss 1.4542\n",
      "Epoch 2 Batch 259 Loss 1.6310\n",
      "Epoch 2 Batch 260 Loss 1.4850\n",
      "Epoch 2 Batch 261 Loss 1.5787\n",
      "Epoch 2 Batch 262 Loss 1.5587\n",
      "Epoch 2 Batch 263 Loss 1.3845\n",
      "Epoch 2 Batch 264 Loss 1.3586\n",
      "Epoch 2 Batch 265 Loss 1.5700\n",
      "Epoch 2 Batch 266 Loss 1.4873\n",
      "Epoch 2 Batch 267 Loss 1.7858\n",
      "Epoch 2 Batch 268 Loss 1.9198\n",
      "Epoch 2 Batch 269 Loss 1.8025\n",
      "Epoch 2 Batch 270 Loss 2.3568\n",
      "Epoch 2 Batch 271 Loss 2.5190\n",
      "Epoch 2 Batch 272 Loss 2.1708\n",
      "Epoch 2 Batch 273 Loss 2.3333\n",
      "Epoch 2 Batch 274 Loss 2.1566\n",
      "Epoch 2 Batch 275 Loss 1.9383\n",
      "Epoch 2 Batch 276 Loss 1.8006\n",
      "Epoch 2 Batch 277 Loss 1.8243\n",
      "Epoch 2 Batch 278 Loss 1.8275\n",
      "Epoch 2 Batch 279 Loss 1.6279\n",
      "Epoch 2 Batch 280 Loss 1.5326\n",
      "Epoch 2 Batch 281 Loss 1.9672\n",
      "Epoch 2 Batch 282 Loss 2.2918\n",
      "Epoch 2 Batch 283 Loss 2.2244\n",
      "Epoch 2 Batch 284 Loss 1.7489\n",
      "Epoch 2 Batch 285 Loss 1.5143\n",
      "Epoch 2 Batch 286 Loss 1.3285\n",
      "Epoch 2 Batch 287 Loss 1.3491\n",
      "Epoch 2 Batch 288 Loss 1.7011\n",
      "Epoch 2 Batch 289 Loss 1.7942\n",
      "Epoch 2 Batch 290 Loss 2.2239\n",
      "Epoch 2 Batch 291 Loss 1.7243\n",
      "Epoch 2 Batch 292 Loss 1.3261\n",
      "Epoch 2 Batch 293 Loss 1.2511\n",
      "Epoch 2 Batch 294 Loss 1.5950\n",
      "Epoch 2 Batch 295 Loss 1.7514\n",
      "Epoch 2 Batch 296 Loss 1.4477\n",
      "Epoch 2 Batch 297 Loss 1.3366\n",
      "Epoch 2 Batch 298 Loss 1.7161\n",
      "Epoch 2 Batch 299 Loss 1.5879\n",
      "Epoch 2 Batch 300 Loss 1.8962\n",
      "Epoch 2 Batch 301 Loss 1.4534\n",
      "Epoch 2 Batch 302 Loss 1.6802\n",
      "Epoch 2 Batch 303 Loss 1.3346\n",
      "Epoch 2 Batch 304 Loss 1.8391\n",
      "Epoch 2 Batch 305 Loss 1.3179\n",
      "Epoch 2 Batch 306 Loss 1.8454\n",
      "Epoch 2 Batch 307 Loss 1.8830\n",
      "Epoch 2 Batch 308 Loss 1.5902\n",
      "Epoch 2 Batch 309 Loss 1.7276\n",
      "Epoch 2 Batch 310 Loss 1.4879\n",
      "Epoch 2 Batch 311 Loss 2.2535\n",
      "Epoch 2 Batch 312 Loss 1.9538\n",
      "Epoch 2 Batch 313 Loss 1.7359\n",
      "Epoch 2 Batch 314 Loss 1.6490\n",
      "Epoch 2 Batch 315 Loss 1.5090\n",
      "Epoch 2 Batch 316 Loss 1.6924\n",
      "Epoch 2 Batch 317 Loss 1.6955\n",
      "Epoch 2 Batch 318 Loss 1.5571\n",
      "Epoch 2 Batch 319 Loss 1.7833\n",
      "Epoch 2 Batch 320 Loss 1.4187\n",
      "Epoch 2 Batch 321 Loss 1.8309\n",
      "Epoch 2 Batch 322 Loss 1.8715\n",
      "Epoch 2 Batch 323 Loss 1.8626\n",
      "Epoch 2 Batch 324 Loss 1.7015\n",
      "Epoch 2 Batch 325 Loss 1.6646\n",
      "Epoch 2 Batch 326 Loss 1.6573\n",
      "Epoch 2 Batch 327 Loss 1.7637\n",
      "Epoch 2 Batch 328 Loss 1.7759\n",
      "Epoch 2 Batch 329 Loss 1.7681\n",
      "Epoch 2 Batch 330 Loss 1.7029\n",
      "Epoch 2 Batch 331 Loss 1.8467\n",
      "Epoch 2 Batch 332 Loss 1.5871\n",
      "Epoch 2 Batch 333 Loss 1.5536\n",
      "Epoch 2 Batch 334 Loss 1.6229\n",
      "Epoch 2 Batch 335 Loss 1.6860\n",
      "Epoch 2 Batch 336 Loss 1.5747\n",
      "Epoch 2 Batch 337 Loss 1.6946\n",
      "Epoch 2 Batch 338 Loss 1.9051\n",
      "Epoch 2 Batch 339 Loss 1.8351\n",
      "Epoch 2 Batch 340 Loss 1.8425\n",
      "Epoch 2 Batch 341 Loss 1.6269\n",
      "Epoch 2 Batch 342 Loss 1.6847\n",
      "Epoch 2 Batch 343 Loss 1.8800\n",
      "Epoch 2 Batch 344 Loss 1.6467\n",
      "Epoch 2 Batch 345 Loss 1.6332\n",
      "Epoch 2 Batch 346 Loss 2.2468\n",
      "Epoch 2 Batch 347 Loss 1.6647\n",
      "Epoch 2 Batch 348 Loss 1.9462\n",
      "Epoch 2 Batch 349 Loss 1.8231\n",
      "Epoch 2 Batch 350 Loss 1.5926\n",
      "Epoch 2 Batch 351 Loss 1.4440\n",
      "Epoch 2 Batch 352 Loss 1.3812\n",
      "Epoch 2 Batch 353 Loss 1.5337\n",
      "Epoch 2 Batch 354 Loss 1.3720\n",
      "Epoch 2 Batch 355 Loss 1.6592\n",
      "Epoch 2 Batch 356 Loss 1.9639\n",
      "Epoch 2 Batch 357 Loss 1.9728\n",
      "Epoch 2 Batch 358 Loss 1.8296\n",
      "Epoch 2 Batch 359 Loss 1.8092\n",
      "Epoch 2 Batch 360 Loss 1.7297\n",
      "Epoch 2 Batch 361 Loss 1.9435\n",
      "Epoch 2 Batch 362 Loss 1.9813\n",
      "Epoch 2 Batch 363 Loss 1.8698\n",
      "Epoch 2 Batch 364 Loss 1.8209\n",
      "Epoch 2 Batch 365 Loss 1.9299\n",
      "Epoch 2 Batch 366 Loss 2.2194\n",
      "Epoch 2 Batch 367 Loss 1.6898\n",
      "Epoch 2 Batch 368 Loss 1.5227\n",
      "Epoch 2 Batch 369 Loss 1.9369\n",
      "Epoch 2 Batch 370 Loss 2.1211\n",
      "Epoch 2 Batch 371 Loss 1.9110\n",
      "Epoch 2 Batch 372 Loss 1.5315\n",
      "Epoch 2 Batch 373 Loss 1.8002\n",
      "Epoch 2 Batch 374 Loss 1.4803\n",
      "Epoch 2 Batch 375 Loss 1.1547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Batch 376 Loss 1.3456\n",
      "Epoch 2 Batch 377 Loss 1.6116\n",
      "Epoch 2 Batch 378 Loss 1.3551\n",
      "Epoch 2 Batch 379 Loss 1.7152\n",
      "Epoch 2 Batch 380 Loss 1.4750\n",
      "Epoch 2 Batch 381 Loss 1.7426\n",
      "Epoch 2 Batch 382 Loss 1.5536\n",
      "Epoch 2 Batch 383 Loss 1.7262\n",
      "Epoch 2 Batch 384 Loss 1.8879\n",
      "Epoch 2 Batch 385 Loss 1.3849\n",
      "Epoch 2 Batch 386 Loss 1.5156\n",
      "Epoch 2 Batch 387 Loss 1.7760\n",
      "Epoch 2 Batch 388 Loss 1.6486\n",
      "Epoch 2 Batch 389 Loss 1.4967\n",
      "Epoch 2 Batch 390 Loss 1.5318\n",
      "Epoch 2 Batch 391 Loss 1.9993\n",
      "Epoch 2 Batch 392 Loss 2.0936\n",
      "Epoch 2 Batch 393 Loss 1.6048\n",
      "Epoch 2 Batch 394 Loss 1.4842\n",
      "Epoch 2 Batch 395 Loss 1.6104\n",
      "Epoch 2 Batch 396 Loss 1.4105\n",
      "Epoch 2 Batch 397 Loss 1.5293\n",
      "Epoch 2 Batch 398 Loss 1.4847\n",
      "Epoch 2 Batch 399 Loss 1.8645\n",
      "Epoch 2 Batch 400 Loss 2.2639\n",
      "Epoch 2 Batch 401 Loss 1.4083\n",
      "Epoch 2 Batch 402 Loss 0.9536\n",
      "Epoch 2 Batch 403 Loss 1.4660\n",
      "Epoch 2 Batch 404 Loss 1.9468\n",
      "Epoch 2 Batch 405 Loss 1.3354\n",
      "Epoch 2 Batch 406 Loss 1.6067\n",
      "Epoch 2 Batch 407 Loss 2.1680\n",
      "Epoch 2 Batch 408 Loss 2.0339\n",
      "Epoch 2 Batch 409 Loss 1.8652\n",
      "Epoch 2 Batch 410 Loss 1.6787\n",
      "Epoch 2 Batch 411 Loss 2.0121\n",
      "Epoch 2 Batch 412 Loss 2.1124\n",
      "Epoch 2 Batch 413 Loss 1.7576\n",
      "Epoch 2 Batch 414 Loss 1.7771\n",
      "Epoch 2 Batch 415 Loss 1.6996\n",
      "Epoch 2 Batch 416 Loss 2.1873\n",
      "Epoch 2 Batch 417 Loss 2.1870\n",
      "Epoch 2 Batch 418 Loss 1.8405\n",
      "Epoch 2 Batch 419 Loss 1.7040\n",
      "Epoch 2 Batch 420 Loss 1.7391\n",
      "Epoch 2 Batch 421 Loss 1.8524\n",
      "Epoch 2 Batch 422 Loss 1.4461\n",
      "Epoch 2 Batch 423 Loss 1.9453\n",
      "Epoch 2 Batch 424 Loss 2.0261\n",
      "Epoch 2 Batch 425 Loss 1.4643\n",
      "Epoch 2 Batch 426 Loss 2.1610\n",
      "Epoch 2 Batch 427 Loss 2.2693\n",
      "Epoch 2 Batch 428 Loss 1.7886\n",
      "Epoch 2 Batch 429 Loss 2.1210\n",
      "Epoch 2 Batch 430 Loss 2.0486\n",
      "Epoch 2 Batch 431 Loss 1.9724\n",
      "Epoch 2 Batch 432 Loss 1.7599\n",
      "Epoch 2 Batch 433 Loss 2.0448\n",
      "Epoch 2 Batch 434 Loss 1.4011\n",
      "Epoch 2 Batch 435 Loss 1.4062\n",
      "Epoch 2 Batch 436 Loss 1.7830\n",
      "Epoch 2 Batch 437 Loss 1.9988\n",
      "Epoch 2 Batch 438 Loss 1.7774\n",
      "Epoch 2 Batch 439 Loss 2.1072\n",
      "Epoch 2 Batch 440 Loss 2.0823\n",
      "Epoch 2 Batch 441 Loss 1.8639\n",
      "Epoch 2 Batch 442 Loss 1.8456\n",
      "Epoch 2 Batch 443 Loss 1.6136\n",
      "Epoch 2 Batch 444 Loss 1.4102\n",
      "Epoch 2 Batch 445 Loss 1.8084\n",
      "Epoch 2 Batch 446 Loss 1.7548\n",
      "Epoch 2 Batch 447 Loss 2.0371\n",
      "Epoch 2 Batch 448 Loss 2.6919\n",
      "Epoch 2 Batch 449 Loss 2.0268\n",
      "Epoch 2 Batch 450 Loss 1.6401\n",
      "Epoch 2 Batch 451 Loss 1.8915\n",
      "Epoch 2 Batch 452 Loss 2.1553\n",
      "Epoch 2 Batch 453 Loss 2.0437\n",
      "Epoch 2 Batch 454 Loss 1.8775\n",
      "Epoch 2 Batch 455 Loss 1.6501\n",
      "Epoch 2 Batch 456 Loss 1.9170\n",
      "Epoch 2 Batch 457 Loss 1.4422\n",
      "Epoch 2 Batch 458 Loss 1.2861\n",
      "Epoch 2 Batch 459 Loss 1.4370\n",
      "Epoch 2 Batch 460 Loss 1.9544\n",
      "Epoch 2 Batch 461 Loss 1.2830\n",
      "Epoch 2 Batch 462 Loss 1.7100\n",
      "Epoch 2 Batch 463 Loss 2.1777\n",
      "Epoch 2 Batch 464 Loss 2.1009\n",
      "Epoch 2 Batch 465 Loss 2.0238\n",
      "Epoch 2 Batch 466 Loss 1.7003\n",
      "Epoch 2 Batch 467 Loss 2.2201\n",
      "Epoch 2 Batch 468 Loss 1.8352\n",
      "Epoch 2 Batch 469 Loss 2.1401\n",
      "Epoch 2 Batch 470 Loss 1.9848\n",
      "Epoch 2 Batch 471 Loss 1.8320\n",
      "Epoch 2 Batch 472 Loss 1.7468\n",
      "Epoch 2 Batch 473 Loss 1.4736\n",
      "Epoch 2 Batch 474 Loss 1.7136\n",
      "Epoch 2 Batch 475 Loss 1.5571\n",
      "Epoch 2 Batch 476 Loss 1.4616\n",
      "Epoch 2 Batch 477 Loss 1.6182\n",
      "Epoch 2 Batch 478 Loss 1.3336\n",
      "Epoch 2 Batch 479 Loss 1.7242\n",
      "Epoch 2 Batch 480 Loss 1.9209\n",
      "Epoch 2 Batch 481 Loss 1.5690\n",
      "Epoch 2 Batch 482 Loss 1.8307\n",
      "Epoch 2 Batch 483 Loss 1.8147\n",
      "Epoch 2 Batch 484 Loss 1.5963\n",
      "Epoch 2 Batch 485 Loss 1.7409\n",
      "Epoch 2 Batch 486 Loss 1.9175\n",
      "Epoch 2 Batch 487 Loss 1.5104\n",
      "Epoch 2 Batch 488 Loss 1.4876\n",
      "Epoch 2 Batch 489 Loss 1.3705\n",
      "Epoch 2 Batch 490 Loss 1.6235\n",
      "Epoch 2 Batch 491 Loss 1.4878\n",
      "Epoch 2 Batch 492 Loss 1.8134\n",
      "Epoch 2 Batch 493 Loss 1.8428\n",
      "Epoch 2 Batch 494 Loss 1.7506\n",
      "Epoch 2 Batch 495 Loss 1.7477\n",
      "Epoch 2 Batch 496 Loss 1.3107\n",
      "Epoch 2 Batch 497 Loss 1.3537\n",
      "Epoch 2 Batch 498 Loss 1.6098\n",
      "Epoch 2 Batch 499 Loss 1.6091\n",
      "Epoch 2 Batch 500 Loss 1.6413\n",
      "Epoch 2 Batch 501 Loss 1.4050\n",
      "Epoch 2 Batch 502 Loss 1.1662\n",
      "Epoch 2 Batch 503 Loss 0.8931\n",
      "Epoch 2 Batch 504 Loss 1.5283\n",
      "Epoch 2 Batch 505 Loss 1.5998\n",
      "Epoch 2 Batch 506 Loss 1.4095\n",
      "Epoch 2 Batch 507 Loss 1.7349\n",
      "Epoch 2 Batch 508 Loss 1.9824\n",
      "Epoch 2 Batch 509 Loss 2.0513\n",
      "Epoch 2 Batch 510 Loss 1.9851\n",
      "Epoch 2 Batch 511 Loss 2.2111\n",
      "Epoch 2 Batch 512 Loss 1.4982\n",
      "Epoch 2 Batch 513 Loss 1.6828\n",
      "Epoch 2 Batch 514 Loss 1.4511\n",
      "Epoch 2 Batch 515 Loss 1.4462\n",
      "Epoch 2 Batch 516 Loss 1.6855\n",
      "Epoch 2 Batch 517 Loss 1.6994\n",
      "Epoch 2 Batch 518 Loss 2.0196\n",
      "Epoch 2 Batch 519 Loss 1.5677\n",
      "Epoch 2 Batch 520 Loss 1.3663\n",
      "Epoch 2 Batch 521 Loss 1.4861\n",
      "Epoch 2 Batch 522 Loss 1.6363\n",
      "Epoch 2 Batch 523 Loss 1.2304\n",
      "Epoch 2 Batch 524 Loss 1.7981\n",
      "Epoch 2 Batch 525 Loss 1.3381\n",
      "Epoch 2 Batch 526 Loss 1.3606\n",
      "Epoch 2 Batch 527 Loss 1.4697\n",
      "Epoch 2 Batch 528 Loss 1.9624\n",
      "Epoch 2 Batch 529 Loss 1.6586\n",
      "Epoch 2 Batch 530 Loss 1.6556\n",
      "Epoch 2 Batch 531 Loss 2.1019\n",
      "Epoch 2 Batch 532 Loss 1.8146\n",
      "Epoch 2 Batch 533 Loss 1.8631\n",
      "Epoch 2 Batch 534 Loss 2.1309\n",
      "Epoch 2 Batch 535 Loss 1.9647\n",
      "Epoch 2 Batch 536 Loss 1.9421\n",
      "Epoch 2 Batch 537 Loss 1.5610\n",
      "Epoch 2 Batch 538 Loss 1.8124\n",
      "Epoch 2 Batch 539 Loss 1.3871\n",
      "Epoch 2 Batch 540 Loss 1.3100\n",
      "Epoch 2 Batch 541 Loss 1.5005\n",
      "Epoch 2 Batch 542 Loss 1.6161\n",
      "Epoch 2 Batch 543 Loss 1.6507\n",
      "Epoch 2 Batch 544 Loss 1.7954\n",
      "Epoch 2 Batch 545 Loss 1.7134\n",
      "Epoch 2 Batch 546 Loss 1.9582\n",
      "Epoch 2 Batch 547 Loss 2.1752\n",
      "Epoch 2 Batch 548 Loss 2.3110\n",
      "Epoch 2 Batch 549 Loss 2.0175\n",
      "Epoch 2 Batch 550 Loss 1.8414\n",
      "Epoch 2 Batch 551 Loss 1.8881\n",
      "Epoch 2 Batch 552 Loss 1.8163\n",
      "Epoch 2 Batch 553 Loss 1.4632\n",
      "Epoch 2 Batch 554 Loss 1.6126\n",
      "Epoch 2 Batch 555 Loss 1.8385\n",
      "Epoch 2 Batch 556 Loss 2.2749\n",
      "Epoch 2 Batch 557 Loss 1.5356\n",
      "Epoch 2 Batch 558 Loss 1.6751\n",
      "Epoch 2 Batch 559 Loss 1.5424\n",
      "Epoch 2 Batch 560 Loss 1.6213\n",
      "Epoch 2 Batch 561 Loss 1.7673\n",
      "Epoch 2 Batch 562 Loss 1.7329\n",
      "Epoch 2 Batch 563 Loss 1.8734\n",
      "Epoch 2 Batch 564 Loss 1.8804\n",
      "Epoch 2 Batch 565 Loss 1.6410\n",
      "Epoch 2 Batch 566 Loss 1.4102\n",
      "Epoch 2 Batch 567 Loss 1.5673\n",
      "Epoch 2 Batch 568 Loss 1.6907\n",
      "Epoch 2 Batch 569 Loss 1.4837\n",
      "Epoch 2 Batch 570 Loss 1.4781\n",
      "Epoch 2 Batch 571 Loss 1.5780\n",
      "Epoch 2 Batch 572 Loss 1.4361\n",
      "Epoch 2 Batch 573 Loss 1.6277\n",
      "Epoch 2 Batch 574 Loss 1.4063\n",
      "Epoch 2 Batch 575 Loss 1.4554\n",
      "Epoch 2 Batch 576 Loss 2.1386\n",
      "Epoch 2 Batch 577 Loss 2.0767\n",
      "Epoch 2 Batch 578 Loss 1.8648\n",
      "Epoch 2 Batch 579 Loss 1.6830\n",
      "Epoch 2 Batch 580 Loss 1.9215\n",
      "Epoch 2 Batch 581 Loss 1.8892\n",
      "Epoch 2 Batch 582 Loss 1.9135\n",
      "Epoch 2 Batch 583 Loss 1.5535\n",
      "Epoch 2 Batch 584 Loss 1.6105\n",
      "Epoch 2 Batch 585 Loss 1.8213\n",
      "Epoch 2 Batch 586 Loss 1.5742\n",
      "Epoch 2 Batch 587 Loss 1.6320\n",
      "Epoch 2 Batch 588 Loss 1.5728\n",
      "Epoch 2 Batch 589 Loss 1.7724\n",
      "Epoch 2 Batch 590 Loss 1.8302\n",
      "Epoch 2 Batch 591 Loss 1.7597\n",
      "Epoch 2 Batch 592 Loss 1.9315\n",
      "Epoch 2 Batch 593 Loss 1.8185\n",
      "Epoch 2 Batch 594 Loss 1.9254\n",
      "Epoch 2 Batch 595 Loss 2.0267\n",
      "Epoch 2 Batch 596 Loss 1.8969\n",
      "Epoch 2 Batch 597 Loss 1.7882\n",
      "Epoch 2 Batch 598 Loss 1.7050\n",
      "Epoch 2 Batch 599 Loss 1.6785\n",
      "Epoch 2 Batch 600 Loss 1.8864\n",
      "Epoch 2 Batch 601 Loss 1.5757\n",
      "Epoch 2 Batch 602 Loss 1.9320\n",
      "Epoch 2 Batch 603 Loss 2.0785\n",
      "Epoch 2 Batch 604 Loss 2.0153\n",
      "Epoch 2 Batch 605 Loss 1.7894\n",
      "Epoch 2 Batch 606 Loss 1.9905\n",
      "Epoch 2 Batch 607 Loss 2.0354\n",
      "Epoch 2 Batch 608 Loss 1.6927\n",
      "Epoch 2 Batch 609 Loss 1.8805\n",
      "Epoch 2 Batch 610 Loss 1.6217\n",
      "Epoch 2 Batch 611 Loss 1.7391\n",
      "Epoch 2 Batch 612 Loss 1.8308\n",
      "Epoch 2 Batch 613 Loss 1.5775\n",
      "Epoch 2 Batch 614 Loss 1.7695\n",
      "Epoch 2 Batch 615 Loss 1.9545\n",
      "Epoch 2 Batch 616 Loss 1.7912\n",
      "Epoch 2 Batch 617 Loss 1.8416\n",
      "Epoch 2 Batch 618 Loss 1.6833\n",
      "Epoch 2 Batch 619 Loss 1.8909\n",
      "Epoch 2 Batch 620 Loss 1.8886\n",
      "Epoch 2 Batch 621 Loss 1.3276\n",
      "Epoch 2 Batch 622 Loss 1.4735\n",
      "Epoch 2 Batch 623 Loss 1.3464\n",
      "Epoch 2 Batch 624 Loss 1.2061\n",
      "Epoch 2 Batch 625 Loss 1.8682\n",
      "Epoch 2 Batch 626 Loss 1.5593\n",
      "Epoch 2 Batch 627 Loss 2.2450\n",
      "Epoch 2 Batch 628 Loss 1.5909\n",
      "Epoch 2 Batch 629 Loss 1.8112\n",
      "Epoch 2 Batch 630 Loss 2.3432\n",
      "Epoch 2 Batch 631 Loss 1.9561\n",
      "Epoch 2 Batch 632 Loss 1.7516\n",
      "Epoch 2 Batch 633 Loss 1.9724\n",
      "Epoch 2 Batch 634 Loss 1.4119\n",
      "Epoch 2 Batch 635 Loss 1.9085\n",
      "Epoch 2 Batch 636 Loss 2.1308\n",
      "Epoch 2 Batch 637 Loss 1.9088\n",
      "Epoch 2 Batch 638 Loss 1.7319\n",
      "Epoch 2 Batch 639 Loss 1.7060\n",
      "Epoch 2 Batch 640 Loss 1.7059\n",
      "Epoch 2 Batch 641 Loss 1.5514\n",
      "Epoch 2 Batch 642 Loss 1.3403\n",
      "Epoch 2 Batch 643 Loss 1.4757\n",
      "Epoch 2 Batch 644 Loss 1.4798\n",
      "Epoch 2 Batch 645 Loss 1.6724\n",
      "Epoch 2 Batch 646 Loss 1.5273\n",
      "Epoch 2 Batch 647 Loss 2.0142\n",
      "Epoch 2 Batch 648 Loss 1.9131\n",
      "Epoch 2 Batch 649 Loss 1.8084\n",
      "Epoch 2 Batch 650 Loss 1.7914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Batch 651 Loss 1.9376\n",
      "Epoch 2 Batch 652 Loss 1.6397\n",
      "Epoch 2 Batch 653 Loss 1.9460\n",
      "Epoch 2 Batch 654 Loss 1.9667\n",
      "Epoch 2 Batch 655 Loss 2.0017\n",
      "Epoch 2 Batch 656 Loss 2.2575\n",
      "Epoch 2 Batch 657 Loss 2.0220\n",
      "Epoch 2 Batch 658 Loss 1.7966\n",
      "Epoch 2 Batch 659 Loss 1.6859\n",
      "Epoch 2 Batch 660 Loss 1.9585\n",
      "Epoch 2 Batch 661 Loss 2.2060\n",
      "Epoch 2 Batch 662 Loss 2.0312\n",
      "Epoch 2 Batch 663 Loss 2.2047\n",
      "Epoch 2 Batch 664 Loss 1.6347\n",
      "Epoch 2 Batch 665 Loss 1.8405\n",
      "Epoch 2 Batch 666 Loss 1.8049\n",
      "Epoch 2 Batch 667 Loss 1.8763\n",
      "Epoch 2 Batch 668 Loss 1.8379\n",
      "Epoch 2 Batch 669 Loss 1.6899\n",
      "Epoch 2 Batch 670 Loss 1.3526\n",
      "Epoch 2 Batch 671 Loss 1.8028\n",
      "Epoch 2 Batch 672 Loss 1.8892\n",
      "Epoch 2 Batch 673 Loss 1.8820\n",
      "Epoch 2 Batch 674 Loss 1.4918\n",
      "Epoch 2 Batch 675 Loss 1.7031\n",
      "Epoch 2 Batch 676 Loss 1.5578\n",
      "Epoch 2 Batch 677 Loss 1.5191\n",
      "Epoch 2 Batch 678 Loss 2.0056\n",
      "Epoch 2 Batch 679 Loss 2.0204\n",
      "Epoch 2 Batch 680 Loss 1.5174\n",
      "Epoch 2 Batch 681 Loss 1.8429\n",
      "Epoch 2 Batch 682 Loss 1.9911\n",
      "Epoch 2 Batch 683 Loss 1.6954\n",
      "Epoch 2 Batch 684 Loss 2.0227\n",
      "Epoch 2 Batch 685 Loss 1.7618\n",
      "Epoch 2 Batch 686 Loss 1.7867\n",
      "Epoch 2 Batch 687 Loss 1.4168\n",
      "Epoch 2 Batch 688 Loss 1.5274\n",
      "Epoch 2 Batch 689 Loss 1.9218\n",
      "Epoch 2 Batch 690 Loss 2.1088\n",
      "Epoch 2 Batch 691 Loss 2.1862\n",
      "Epoch 2 Batch 692 Loss 2.0060\n",
      "Epoch 2 Batch 693 Loss 1.8653\n",
      "Epoch 2 Batch 694 Loss 1.7935\n",
      "Epoch 2 Batch 695 Loss 1.7725\n",
      "Epoch 2 Batch 696 Loss 1.9221\n",
      "Epoch 2 Batch 697 Loss 1.8627\n",
      "Epoch 2 Batch 698 Loss 1.8759\n",
      "Epoch 2 Batch 699 Loss 1.7107\n",
      "Epoch 2 Batch 700 Loss 1.4343\n",
      "Epoch 2 Batch 701 Loss 1.7297\n",
      "Epoch 2 Batch 702 Loss 1.4280\n",
      "Epoch 2 Batch 703 Loss 1.3521\n",
      "Epoch 2 Batch 704 Loss 1.7154\n",
      "Epoch 2 Batch 705 Loss 1.7314\n",
      "Epoch 2 Batch 706 Loss 1.6396\n",
      "Epoch 2 Batch 707 Loss 2.1501\n",
      "Epoch 2 Batch 708 Loss 2.2912\n",
      "Epoch 2 Batch 709 Loss 2.0377\n",
      "Epoch 2 Batch 710 Loss 1.9708\n",
      "Epoch 2 Batch 711 Loss 1.6535\n",
      "Epoch 2 Batch 712 Loss 1.3272\n",
      "Epoch 2 Batch 713 Loss 1.4950\n",
      "Epoch 2 Batch 714 Loss 1.6877\n",
      "Epoch 2 Batch 715 Loss 1.8275\n",
      "Epoch 2 Batch 716 Loss 1.7699\n",
      "Epoch 2 Batch 717 Loss 1.6969\n",
      "Epoch 2 Batch 718 Loss 1.9566\n",
      "Epoch 2 Batch 719 Loss 1.8549\n",
      "Epoch 2 Batch 720 Loss 1.5445\n",
      "Epoch 2 Batch 721 Loss 1.9253\n",
      "Epoch 2 Batch 722 Loss 2.3388\n",
      "Epoch 2 Batch 723 Loss 2.0809\n",
      "Epoch 2 Batch 724 Loss 2.1477\n",
      "Epoch 2 Batch 725 Loss 2.1303\n",
      "Epoch 2 Batch 726 Loss 1.8341\n",
      "Epoch 2 Batch 727 Loss 1.6615\n",
      "Epoch 2 Batch 728 Loss 1.7499\n",
      "Epoch 2 Batch 729 Loss 0.9246\n",
      "Epoch 2 Batch 730 Loss 1.5374\n",
      "Epoch 2 Batch 731 Loss 1.5756\n",
      "Epoch 2 Batch 732 Loss 1.2731\n",
      "Epoch 2 Batch 733 Loss 1.2859\n",
      "Epoch 2 Batch 734 Loss 1.3633\n",
      "Epoch 2 Batch 735 Loss 1.2413\n",
      "Epoch 2 Batch 736 Loss 1.5348\n",
      "Epoch 2 Batch 737 Loss 1.1254\n",
      "Epoch 2 Batch 738 Loss 1.1100\n",
      "Epoch 2 Batch 739 Loss 1.4471\n",
      "Epoch 2 Batch 740 Loss 1.5380\n",
      "Epoch 2 Batch 741 Loss 1.8020\n",
      "Epoch 2 Batch 742 Loss 1.5383\n",
      "Epoch 2 Batch 743 Loss 1.6813\n",
      "Epoch 2 Batch 744 Loss 1.7594\n",
      "Epoch 2 Batch 745 Loss 1.6578\n",
      "Epoch 2 Batch 746 Loss 2.3115\n",
      "Epoch 2 Batch 747 Loss 1.3797\n",
      "Epoch 2 Batch 748 Loss 1.5544\n",
      "Epoch 2 Batch 749 Loss 1.4649\n",
      "Epoch 2 Batch 750 Loss 1.8140\n",
      "Epoch 2 Batch 751 Loss 1.7201\n",
      "Epoch 2 Batch 752 Loss 1.8767\n",
      "Epoch 2 Batch 753 Loss 1.7491\n",
      "Epoch 2 Batch 754 Loss 1.2124\n",
      "Epoch 2 Batch 755 Loss 1.5107\n",
      "Epoch 2 Batch 756 Loss 1.5613\n",
      "Epoch 2 Batch 757 Loss 1.6595\n",
      "Epoch 2 Batch 758 Loss 1.7303\n",
      "Epoch 2 Batch 759 Loss 1.5903\n",
      "Epoch 2 Batch 760 Loss 2.0210\n",
      "Epoch 2 Batch 761 Loss 2.0054\n",
      "Epoch 2 Batch 762 Loss 2.4313\n",
      "Epoch 2 Batch 763 Loss 1.7396\n",
      "Epoch 2 Batch 764 Loss 1.7853\n",
      "Epoch 2 Batch 765 Loss 1.8399\n",
      "Epoch 2 Batch 766 Loss 1.9725\n",
      "Epoch 2 Batch 767 Loss 1.8509\n",
      "Epoch 2 Batch 768 Loss 1.9736\n",
      "Epoch 2 Batch 769 Loss 2.9634\n",
      "Epoch 2 Batch 770 Loss 1.8787\n",
      "Epoch 2 Batch 771 Loss 2.1912\n",
      "Epoch 2 Batch 772 Loss 2.0467\n",
      "Epoch 2 Batch 773 Loss 1.8780\n",
      "Epoch 2 Batch 774 Loss 1.8573\n",
      "Epoch 2 Batch 775 Loss 2.3366\n",
      "Epoch 2 Batch 776 Loss 2.1100\n",
      "Epoch 2 Batch 777 Loss 1.8669\n",
      "Epoch 2 Batch 778 Loss 1.6332\n",
      "Epoch 2 Batch 779 Loss 1.8177\n",
      "Epoch 2 Batch 780 Loss 1.5056\n",
      "Epoch 2 Batch 781 Loss 1.8187\n",
      "Epoch 2 Batch 782 Loss 1.7629\n",
      "Epoch 2 Batch 783 Loss 1.9096\n",
      "Epoch 2 Batch 784 Loss 1.2983\n",
      "Epoch 2 Batch 785 Loss 1.7603\n",
      "Epoch 2 Batch 786 Loss 2.0110\n",
      "Epoch 2 Batch 787 Loss 1.3909\n",
      "Epoch 2 Batch 788 Loss 1.8677\n",
      "Epoch 2 Batch 789 Loss 1.8896\n",
      "Epoch 2 Batch 790 Loss 1.5664\n",
      "Epoch 2 Batch 791 Loss 1.7284\n",
      "Epoch 2 Batch 792 Loss 1.6665\n",
      "Epoch 2 Batch 793 Loss 1.4967\n",
      "Epoch 2 Batch 794 Loss 1.2107\n",
      "Epoch 2 Batch 795 Loss 1.4535\n",
      "Epoch 2 Batch 796 Loss 1.6914\n",
      "Epoch 2 Batch 797 Loss 1.3720\n",
      "Epoch 2 Batch 798 Loss 1.3500\n",
      "Epoch 2 Batch 799 Loss 1.3492\n",
      "Epoch 2 Batch 800 Loss 1.4156\n",
      "Epoch 2 Batch 801 Loss 1.6941\n",
      "Epoch 2 Batch 802 Loss 1.5872\n",
      "Epoch 2 Batch 803 Loss 1.4427\n",
      "Epoch 2 Batch 804 Loss 1.4675\n",
      "Epoch 2 Batch 805 Loss 1.5131\n",
      "Epoch 2 Batch 806 Loss 1.6381\n",
      "Epoch 2 Batch 807 Loss 1.8460\n",
      "Epoch 2 Batch 808 Loss 1.7767\n",
      "Epoch 2 Batch 809 Loss 2.2053\n",
      "Epoch 2 Batch 810 Loss 1.8897\n",
      "Epoch 2 Batch 811 Loss 1.8408\n",
      "Epoch 2 Batch 812 Loss 2.1054\n",
      "Epoch 2 Batch 813 Loss 1.9260\n",
      "Epoch 2 Batch 814 Loss 1.6014\n",
      "Epoch 2 Batch 815 Loss 1.9971\n",
      "Epoch 2 Batch 816 Loss 1.6900\n",
      "Epoch 2 Batch 817 Loss 1.8376\n",
      "Epoch 2 Batch 818 Loss 2.0045\n",
      "Epoch 2 Batch 819 Loss 2.7548\n",
      "Epoch 2 Batch 820 Loss 1.9299\n",
      "Epoch 2 Batch 821 Loss 2.1191\n",
      "Epoch 2 Batch 822 Loss 1.9060\n",
      "Epoch 2 Batch 823 Loss 1.5096\n",
      "Epoch 2 Batch 824 Loss 1.3481\n",
      "Epoch 2 Batch 825 Loss 1.6495\n",
      "Epoch 2 Batch 826 Loss 1.7874\n",
      "Epoch 2 Batch 827 Loss 2.0040\n",
      "Epoch 2 Batch 828 Loss 1.9750\n",
      "Epoch 2 Batch 829 Loss 2.0342\n",
      "Epoch 2 Batch 830 Loss 2.3444\n",
      "Epoch 2 Batch 831 Loss 1.6893\n",
      "Epoch 2 Batch 832 Loss 1.7560\n",
      "Epoch 2 Batch 833 Loss 1.4879\n",
      "Epoch 2 Batch 834 Loss 1.7574\n",
      "Epoch 2 Batch 835 Loss 1.5932\n",
      "Epoch 2 Batch 836 Loss 1.7830\n",
      "Epoch 2 Batch 837 Loss 1.6955\n",
      "Epoch 2 Batch 838 Loss 1.5603\n",
      "Epoch 2 Batch 839 Loss 1.6618\n",
      "Epoch 2 Batch 840 Loss 1.8598\n",
      "Epoch 2 Batch 841 Loss 1.8935\n",
      "Epoch 2 Batch 842 Loss 1.5950\n",
      "Epoch 2 Batch 843 Loss 1.6669\n",
      "Epoch 2 Batch 844 Loss 1.8025\n",
      "Epoch 2 Batch 845 Loss 1.4553\n",
      "Epoch 2 Batch 846 Loss 1.7657\n",
      "Epoch 2 Batch 847 Loss 1.7821\n",
      "Epoch 2 Batch 848 Loss 1.7688\n",
      "Epoch 2 Batch 849 Loss 1.9555\n",
      "Epoch 2 Batch 850 Loss 1.4122\n",
      "Epoch 2 Batch 851 Loss 1.5335\n",
      "Epoch 2 Batch 852 Loss 2.0730\n",
      "Epoch 2 Batch 853 Loss 2.0125\n",
      "Epoch 2 Batch 854 Loss 1.5548\n",
      "Epoch 2 Batch 855 Loss 1.8261\n",
      "Epoch 2 Batch 856 Loss 1.6165\n",
      "Epoch 2 Batch 857 Loss 1.2373\n",
      "Epoch 2 Batch 858 Loss 1.7004\n",
      "Epoch 2 Batch 859 Loss 1.7239\n",
      "Epoch 2 Batch 860 Loss 1.8076\n",
      "Epoch 2 Batch 861 Loss 1.4774\n",
      "Epoch 2 Batch 862 Loss 1.7967\n",
      "Epoch 2 Batch 863 Loss 2.2773\n",
      "Epoch 2 Batch 864 Loss 2.1448\n",
      "Epoch 2 Batch 865 Loss 2.1303\n",
      "Epoch 2 Batch 866 Loss 1.7463\n",
      "Epoch 2 Batch 867 Loss 1.7344\n",
      "Epoch 2 Batch 868 Loss 1.7498\n",
      "Epoch 2 Batch 869 Loss 1.9398\n",
      "Epoch 2 Batch 870 Loss 1.3743\n",
      "Epoch 2 Batch 871 Loss 1.3324\n",
      "Epoch 2 Batch 872 Loss 1.3164\n",
      "Epoch 2 Batch 873 Loss 1.2887\n",
      "Epoch 2 Batch 874 Loss 1.4818\n",
      "Epoch 2 Batch 875 Loss 1.4300\n",
      "Epoch 2 Batch 876 Loss 1.4352\n",
      "Epoch 2 Batch 877 Loss 1.8369\n",
      "Epoch 2 Batch 878 Loss 1.7741\n",
      "Epoch 2 Batch 879 Loss 1.5874\n",
      "Epoch 2 Batch 880 Loss 1.6902\n",
      "Epoch 2 Batch 881 Loss 1.3060\n",
      "Epoch 2 Batch 882 Loss 1.5721\n",
      "Epoch 2 Batch 883 Loss 1.5322\n",
      "Epoch 2 Batch 884 Loss 1.9166\n",
      "Epoch 2 Batch 885 Loss 2.2482\n",
      "Epoch 2 Batch 886 Loss 1.5509\n",
      "Epoch 2 Batch 887 Loss 1.5640\n",
      "Epoch 2 Batch 888 Loss 1.5054\n",
      "Epoch 2 Batch 889 Loss 1.5339\n",
      "Epoch 2 Batch 890 Loss 1.4740\n",
      "Epoch 2 Batch 891 Loss 1.7969\n",
      "Epoch 2 Batch 892 Loss 1.3690\n",
      "Epoch 2 Batch 893 Loss 1.4566\n",
      "Epoch 2 Batch 894 Loss 1.5709\n",
      "Epoch 2 Batch 895 Loss 1.5180\n",
      "Epoch 2 Batch 896 Loss 1.7866\n",
      "Epoch 2 Batch 897 Loss 1.8537\n",
      "Epoch 2 Batch 898 Loss 1.5131\n",
      "Epoch 2 Batch 899 Loss 1.6069\n",
      "Epoch 2 Batch 900 Loss 1.5140\n",
      "Epoch 2 Batch 901 Loss 1.8375\n",
      "Epoch 2 Batch 902 Loss 1.6731\n",
      "Epoch 2 Batch 903 Loss 1.8117\n",
      "Epoch 2 Batch 904 Loss 1.7551\n",
      "Epoch 2 Batch 905 Loss 1.5982\n",
      "Epoch 2 Batch 906 Loss 1.9504\n",
      "Epoch 2 Batch 907 Loss 2.2127\n",
      "Epoch 2 Batch 908 Loss 2.1510\n",
      "Epoch 2 Batch 909 Loss 2.1350\n",
      "Epoch 2 Batch 910 Loss 1.7966\n",
      "Epoch 2 Batch 911 Loss 1.5136\n",
      "Epoch 2 Batch 912 Loss 2.1039\n",
      "Epoch 2 Batch 913 Loss 1.7368\n",
      "Epoch 2 Batch 914 Loss 2.0240\n",
      "Epoch 2 Batch 915 Loss 1.9629\n",
      "Epoch 2 Batch 916 Loss 2.0211\n",
      "Epoch 2 Batch 917 Loss 1.9190\n",
      "Epoch 2 Batch 918 Loss 1.9374\n",
      "Epoch 2 Batch 919 Loss 1.7829\n",
      "Epoch 2 Batch 920 Loss 1.9354\n",
      "Epoch 2 Batch 921 Loss 1.8858\n",
      "Epoch 2 Batch 922 Loss 1.6790\n",
      "Epoch 2 Batch 923 Loss 1.4929\n",
      "Epoch 2 Batch 924 Loss 2.0152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Batch 925 Loss 1.9046\n",
      "Epoch 2 Batch 926 Loss 1.8664\n",
      "Epoch 2 Batch 927 Loss 2.2941\n",
      "Epoch 2 Batch 928 Loss 1.8429\n",
      "Epoch 2 Batch 929 Loss 2.2032\n",
      "Epoch 2 Batch 930 Loss 2.3576\n",
      "Epoch 2 Batch 931 Loss 2.0422\n",
      "Epoch 2 Batch 932 Loss 2.0817\n",
      "Epoch 2 Batch 933 Loss 2.0922\n",
      "Epoch 2 Batch 934 Loss 2.3039\n",
      "Epoch 2 Batch 935 Loss 2.2971\n",
      "Epoch 2 Batch 936 Loss 1.8157\n",
      "Epoch 2 Batch 937 Loss 2.1820\n",
      "Epoch 2 Batch 938 Loss 2.3465\n",
      "Epoch 2 Batch 939 Loss 1.8046\n",
      "Epoch 2 Batch 940 Loss 1.7184\n",
      "Epoch 2 Batch 941 Loss 1.4933\n",
      "Epoch 2 Batch 942 Loss 1.4213\n",
      "Epoch 2 Batch 943 Loss 1.5421\n",
      "Epoch 2 Batch 944 Loss 1.4555\n",
      "Epoch 2 Batch 945 Loss 1.7181\n",
      "Epoch 2 Batch 946 Loss 1.7952\n",
      "Epoch 2 Batch 947 Loss 1.6747\n",
      "Epoch 2 Batch 948 Loss 2.3030\n",
      "Epoch 2 Batch 949 Loss 1.8549\n",
      "Epoch 2 Batch 950 Loss 1.8452\n",
      "Epoch 2 Batch 951 Loss 1.6072\n",
      "Epoch 2 Batch 952 Loss 1.9605\n",
      "Epoch 2 Batch 953 Loss 1.9512\n",
      "Epoch 2 Batch 954 Loss 1.4356\n",
      "Epoch 2 Batch 955 Loss 1.6545\n",
      "Epoch 2 Batch 956 Loss 2.3048\n",
      "Epoch 2 Batch 957 Loss 2.0689\n",
      "Epoch 2 Batch 958 Loss 1.5224\n",
      "Epoch 2 Batch 959 Loss 1.4426\n",
      "Epoch 2 Batch 960 Loss 1.8016\n",
      "Epoch 2 Batch 961 Loss 1.8896\n",
      "Epoch 2 Batch 962 Loss 2.0623\n",
      "Epoch 2 Batch 963 Loss 2.1581\n",
      "Epoch 2 Batch 964 Loss 1.6878\n",
      "Epoch 2 Batch 965 Loss 1.6677\n",
      "Epoch 2 Batch 966 Loss 2.0348\n",
      "Epoch 2 Batch 967 Loss 1.9073\n",
      "Epoch 2 Batch 968 Loss 1.9196\n",
      "Epoch 2 Batch 969 Loss 1.3150\n",
      "Epoch 2 Batch 970 Loss 1.7232\n",
      "Epoch 2 Batch 971 Loss 1.3327\n",
      "Epoch 2 Batch 972 Loss 1.8349\n",
      "Epoch 2 Batch 973 Loss 1.8051\n",
      "Epoch 2 Batch 974 Loss 1.9262\n",
      "Epoch 2 Batch 975 Loss 1.8580\n",
      "Epoch 2 Batch 976 Loss 1.4501\n",
      "Epoch 2 Batch 977 Loss 1.1239\n",
      "Epoch 2 Batch 978 Loss 1.5323\n",
      "Epoch 2 Batch 979 Loss 1.5290\n",
      "Epoch 2 Batch 980 Loss 1.9483\n",
      "Epoch 2 Batch 981 Loss 1.5467\n",
      "Epoch 2 Batch 982 Loss 1.6003\n",
      "Epoch 2 Batch 983 Loss 1.6208\n",
      "Epoch 2 Batch 984 Loss 1.5746\n",
      "Epoch 2 Batch 985 Loss 1.7459\n",
      "Epoch 2 Batch 986 Loss 1.6736\n",
      "Epoch 2 Batch 987 Loss 1.8461\n",
      "Epoch 2 Batch 988 Loss 1.7999\n",
      "Epoch 2 Batch 989 Loss 1.6281\n",
      "Epoch 2 Batch 990 Loss 1.7849\n",
      "Epoch 2 Batch 991 Loss 1.7593\n",
      "Epoch 2 Batch 992 Loss 1.6513\n",
      "Epoch 2 Batch 993 Loss 1.4890\n",
      "Epoch 2 Batch 994 Loss 1.3636\n",
      "Epoch 2 Batch 995 Loss 1.2957\n",
      "Epoch 2 Batch 996 Loss 1.9183\n",
      "Epoch 2 Batch 997 Loss 1.7724\n",
      "Epoch 2 Batch 998 Loss 1.6781\n",
      "Epoch 2 Batch 999 Loss 1.7393\n",
      "Epoch 2 Batch 1000 Loss 1.8793\n",
      "Epoch 2 Batch 1001 Loss 1.6667\n",
      "Epoch 2 Batch 1002 Loss 1.8897\n",
      "Epoch 2 Batch 1003 Loss 1.6510\n",
      "Epoch 2 Batch 1004 Loss 1.7235\n",
      "Epoch 2 Batch 1005 Loss 1.9649\n",
      "Epoch 2 Batch 1006 Loss 1.8177\n",
      "Epoch 2 Batch 1007 Loss 1.6066\n",
      "Epoch 2 Batch 1008 Loss 1.7490\n",
      "Epoch 2 Batch 1009 Loss 2.0178\n",
      "Epoch 2 Batch 1010 Loss 1.2849\n",
      "Epoch 2 Batch 1011 Loss 1.7997\n",
      "Epoch 2 Batch 1012 Loss 1.7526\n",
      "Epoch 2 Batch 1013 Loss 1.9268\n",
      "Epoch 2 Batch 1014 Loss 1.8222\n",
      "Epoch 2 Batch 1015 Loss 1.9295\n",
      "Epoch 2 Batch 1016 Loss 1.4529\n",
      "Epoch 2 Batch 1017 Loss 1.8414\n",
      "Epoch 2 Batch 1018 Loss 2.2335\n",
      "Epoch 2 Batch 1019 Loss 1.6270\n",
      "Epoch 2 Batch 1020 Loss 1.1790\n",
      "Epoch 2 Batch 1021 Loss 2.0214\n",
      "Epoch 2 Batch 1022 Loss 1.8406\n",
      "Epoch 2 Batch 1023 Loss 1.6490\n",
      "Epoch 2 Batch 1024 Loss 2.0602\n",
      "Epoch 2 Batch 1025 Loss 1.7801\n",
      "Epoch 2 Batch 1026 Loss 1.6603\n",
      "Epoch 2 Batch 1027 Loss 1.5395\n",
      "Epoch 2 Batch 1028 Loss 1.5808\n",
      "Epoch 2 Batch 1029 Loss 1.3468\n",
      "Epoch 2 Batch 1030 Loss 1.4082\n",
      "Epoch 2 Batch 1031 Loss 1.6078\n",
      "Epoch 2 Batch 1032 Loss 1.6284\n",
      "Epoch 2 Batch 1033 Loss 1.9212\n",
      "Epoch 2 Batch 1034 Loss 1.3550\n",
      "Epoch 2 Batch 1035 Loss 1.8931\n",
      "Epoch 2 Batch 1036 Loss 1.6987\n",
      "Epoch 2 Batch 1037 Loss 1.7222\n",
      "Epoch 2 Batch 1038 Loss 1.5075\n",
      "Epoch 2 Batch 1039 Loss 1.7407\n",
      "Epoch 2 Batch 1040 Loss 1.1945\n",
      "Epoch 2 Batch 1041 Loss 1.6186\n",
      "Epoch 2 Batch 1042 Loss 1.4746\n",
      "Epoch 2 Batch 1043 Loss 1.5921\n",
      "Epoch 2 Batch 1044 Loss 1.3854\n",
      "Epoch 2 Batch 1045 Loss 1.0807\n",
      "Epoch 2 Batch 1046 Loss 1.4516\n",
      "Epoch 2 Batch 1047 Loss 1.3928\n",
      "Epoch 2 Batch 1048 Loss 1.7389\n",
      "Epoch 2 Batch 1049 Loss 2.0182\n",
      "Epoch 2 Batch 1050 Loss 2.0526\n",
      "Epoch 2 Batch 1051 Loss 2.0614\n",
      "Epoch 2 Batch 1052 Loss 2.3394\n",
      "Epoch 2 Batch 1053 Loss 1.6241\n",
      "Epoch 2 Batch 1054 Loss 2.3404\n",
      "Epoch 2 Batch 1055 Loss 1.7664\n",
      "Epoch 2 Batch 1056 Loss 1.4077\n",
      "Epoch 2 Batch 1057 Loss 1.3107\n",
      "Epoch 2 Batch 1058 Loss 1.8634\n",
      "Epoch 2 Batch 1059 Loss 1.9165\n",
      "Epoch 2 Batch 1060 Loss 1.9377\n",
      "Epoch 2 Batch 1061 Loss 1.4284\n",
      "Epoch 2 Batch 1062 Loss 1.5348\n",
      "Epoch 2 Batch 1063 Loss 1.5053\n",
      "Epoch 2 Batch 1064 Loss 1.1720\n",
      "Epoch 2 Batch 1065 Loss 1.3515\n",
      "Epoch 2 Batch 1066 Loss 1.8115\n",
      "Epoch 2 Batch 1067 Loss 1.4395\n",
      "Epoch 2 Batch 1068 Loss 1.6170\n",
      "Epoch 2 Batch 1069 Loss 1.5138\n",
      "Epoch 2 Batch 1070 Loss 1.2571\n",
      "Epoch 2 Batch 1071 Loss 1.4476\n",
      "Epoch 2 Batch 1072 Loss 1.4447\n",
      "Epoch 2 Batch 1073 Loss 1.8552\n",
      "Epoch 2 Batch 1074 Loss 2.1734\n",
      "Epoch 2 Batch 1075 Loss 1.6073\n",
      "Epoch 2 Batch 1076 Loss 1.8246\n",
      "Epoch 2 Batch 1077 Loss 1.5781\n",
      "Epoch 2 Batch 1078 Loss 1.5508\n",
      "Epoch 2 Batch 1079 Loss 1.6596\n",
      "Epoch 2 Batch 1080 Loss 1.4902\n",
      "Epoch 2 Batch 1081 Loss 2.0878\n",
      "Epoch 2 Batch 1082 Loss 1.9030\n",
      "Epoch 2 Batch 1083 Loss 1.6769\n",
      "Epoch 2 Batch 1084 Loss 1.8165\n",
      "Epoch 2 Batch 1085 Loss 1.4397\n",
      "Epoch 2 Batch 1086 Loss 1.9444\n",
      "Epoch 2 Batch 1087 Loss 1.7767\n",
      "Epoch 2 Batch 1088 Loss 1.9390\n",
      "Epoch 2 Batch 1089 Loss 2.0189\n",
      "Epoch 2 Batch 1090 Loss 1.8642\n",
      "Epoch 2 Batch 1091 Loss 1.7113\n",
      "Epoch 2 Batch 1092 Loss 1.7224\n",
      "Epoch 2 Batch 1093 Loss 1.9938\n",
      "Epoch 2 Batch 1094 Loss 2.2138\n",
      "Epoch 2 Batch 1095 Loss 1.9729\n",
      "Epoch 2 Batch 1096 Loss 2.1663\n",
      "Epoch 2 Batch 1097 Loss 2.1202\n",
      "Epoch 2 Batch 1098 Loss 1.8690\n",
      "Epoch 2 Batch 1099 Loss 1.6866\n",
      "Epoch 2 Batch 1100 Loss 1.6820\n",
      "Epoch 2 Batch 1101 Loss 1.4388\n",
      "Epoch 2 Batch 1102 Loss 1.9558\n",
      "Epoch 2 Batch 1103 Loss 1.5996\n",
      "Epoch 2 Batch 1104 Loss 1.7943\n",
      "Epoch 2 Batch 1105 Loss 1.7755\n",
      "Epoch 2 Batch 1106 Loss 2.0874\n",
      "Epoch 2 Batch 1107 Loss 2.2820\n",
      "Epoch 2 Batch 1108 Loss 1.9375\n",
      "Epoch 2 Batch 1109 Loss 2.0719\n",
      "Epoch 2 Batch 1110 Loss 1.8580\n",
      "Epoch 2 Batch 1111 Loss 1.7845\n",
      "Epoch 2 Batch 1112 Loss 1.6454\n",
      "Epoch 2 Batch 1113 Loss 1.6323\n",
      "Epoch 2 Batch 1114 Loss 1.8603\n",
      "Epoch 2 Batch 1115 Loss 1.5119\n",
      "Epoch 2 Batch 1116 Loss 1.6091\n",
      "Epoch 2 Batch 1117 Loss 1.7191\n",
      "Epoch 2 Batch 1118 Loss 1.8973\n",
      "Epoch 2 Batch 1119 Loss 1.8974\n",
      "Epoch 2 Batch 1120 Loss 1.5075\n",
      "Epoch 2 Batch 1121 Loss 2.2576\n",
      "Epoch 2 Batch 1122 Loss 2.1323\n",
      "Epoch 2 Batch 1123 Loss 1.7833\n",
      "Epoch 2 Batch 1124 Loss 1.7465\n",
      "Epoch 2 Batch 1125 Loss 1.5037\n",
      "Epoch 2 Batch 1126 Loss 2.0550\n",
      "Epoch 2 Batch 1127 Loss 2.0388\n",
      "Epoch 2 Batch 1128 Loss 2.0234\n",
      "Epoch 2 Batch 1129 Loss 1.9575\n",
      "Epoch 2 Batch 1130 Loss 2.2569\n",
      "Epoch 2 Batch 1131 Loss 2.1067\n",
      "Epoch 2 Batch 1132 Loss 2.0145\n",
      "Epoch 2 Batch 1133 Loss 2.1269\n",
      "Epoch 2 Batch 1134 Loss 1.9329\n",
      "Epoch 2 Batch 1135 Loss 1.9033\n",
      "Epoch 2 Batch 1136 Loss 2.2281\n",
      "Epoch 2 Batch 1137 Loss 1.8345\n",
      "Epoch 2 Batch 1138 Loss 1.9706\n",
      "Epoch 2 Batch 1139 Loss 1.9422\n",
      "Epoch 2 Batch 1140 Loss 1.9454\n",
      "Epoch 2 Batch 1141 Loss 1.8222\n",
      "Epoch 2 Batch 1142 Loss 2.0104\n",
      "Epoch 2 Batch 1143 Loss 1.3935\n",
      "Epoch 2 Batch 1144 Loss 1.5014\n",
      "Epoch 2 Batch 1145 Loss 1.5203\n",
      "Epoch 2 Batch 1146 Loss 1.5276\n",
      "Epoch 2 Batch 1147 Loss 1.3542\n",
      "Epoch 2 Batch 1148 Loss 1.7267\n",
      "Epoch 2 Batch 1149 Loss 1.6343\n",
      "Epoch 2 Batch 1150 Loss 1.7030\n",
      "Epoch 2 Batch 1151 Loss 1.9668\n",
      "Epoch 2 Batch 1152 Loss 1.5236\n",
      "Epoch 2 Batch 1153 Loss 1.7735\n",
      "Epoch 2 Batch 1154 Loss 1.9686\n",
      "Epoch 2 Batch 1155 Loss 2.1504\n",
      "Epoch 2 Batch 1156 Loss 2.0355\n",
      "Epoch 2 Batch 1157 Loss 1.9871\n",
      "Epoch 2 Batch 1158 Loss 1.7770\n",
      "Epoch 2 Batch 1159 Loss 1.9299\n",
      "Epoch 2 Batch 1160 Loss 1.7119\n",
      "Epoch 2 Batch 1161 Loss 1.7976\n",
      "Epoch 2 Batch 1162 Loss 1.3088\n",
      "Epoch 2 Batch 1163 Loss 1.7500\n",
      "Epoch 2 Batch 1164 Loss 1.3779\n",
      "Epoch 2 Batch 1165 Loss 1.8163\n",
      "Epoch 2 Batch 1166 Loss 1.9947\n",
      "Epoch 2 Batch 1167 Loss 2.4576\n",
      "Epoch 2 Batch 1168 Loss 1.6707\n",
      "Epoch 2 Batch 1169 Loss 1.4455\n",
      "Epoch 2 Batch 1170 Loss 1.3640\n",
      "Epoch 2 Batch 1171 Loss 1.2357\n",
      "Epoch 2 Batch 1172 Loss 1.4985\n",
      "Epoch 2 Batch 1173 Loss 1.5649\n",
      "Epoch 2 Batch 1174 Loss 1.3702\n",
      "Epoch 2 Batch 1175 Loss 1.2273\n",
      "Epoch 2 Batch 1176 Loss 1.3247\n",
      "Epoch 2 Batch 1177 Loss 1.4100\n",
      "Epoch 2 Batch 1178 Loss 1.3637\n",
      "Epoch 2 Batch 1179 Loss 1.4407\n",
      "Epoch 2 Batch 1180 Loss 1.7783\n",
      "Epoch 2 Batch 1181 Loss 1.5645\n",
      "Epoch 2 Batch 1182 Loss 1.7796\n",
      "Epoch 2 Batch 1183 Loss 2.1997\n",
      "Epoch 2 Batch 1184 Loss 1.6107\n",
      "Epoch 2 Batch 1185 Loss 1.9668\n",
      "Epoch 2 Batch 1186 Loss 1.3817\n",
      "Epoch 2 Batch 1187 Loss 1.7217\n",
      "Epoch 2 Batch 1188 Loss 1.9972\n",
      "Epoch 2 Batch 1189 Loss 2.0132\n",
      "Epoch 2 Batch 1190 Loss 2.4382\n",
      "Epoch 2 Batch 1191 Loss 1.8666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Batch 1192 Loss 1.8594\n",
      "Epoch 2 Batch 1193 Loss 2.2117\n",
      "Epoch 2 Batch 1194 Loss 1.6596\n",
      "Epoch 2 Batch 1195 Loss 2.1522\n",
      "Epoch 2 Batch 1196 Loss 1.6355\n",
      "Epoch 2 Batch 1197 Loss 1.4964\n",
      "Epoch 2 Batch 1198 Loss 1.2892\n",
      "Epoch 2 Batch 1199 Loss 1.2181\n",
      "Epoch 2 Batch 1200 Loss 1.6333\n",
      "Epoch 2 Batch 1201 Loss 1.7266\n",
      "Epoch 2 Batch 1202 Loss 1.7640\n",
      "Epoch 2 Batch 1203 Loss 2.0295\n",
      "Epoch 2 Batch 1204 Loss 1.4829\n",
      "Epoch 2 Batch 1205 Loss 1.5517\n",
      "Epoch 2 Batch 1206 Loss 1.5709\n",
      "Epoch 2 Batch 1207 Loss 1.7172\n",
      "Epoch 2 Batch 1208 Loss 1.6169\n",
      "Epoch 2 Batch 1209 Loss 1.5054\n",
      "Epoch 2 Batch 1210 Loss 1.8536\n",
      "Epoch 2 Batch 1211 Loss 2.0059\n",
      "Epoch 2 Batch 1212 Loss 1.7219\n",
      "Epoch 2 Batch 1213 Loss 1.8512\n",
      "Epoch 2 Batch 1214 Loss 1.8955\n",
      "Epoch 2 Batch 1215 Loss 1.7029\n",
      "Epoch 2 Batch 1216 Loss 1.7937\n",
      "Epoch 2 Batch 1217 Loss 1.5606\n",
      "Epoch 2 Batch 1218 Loss 1.8648\n",
      "Epoch 2 Batch 1219 Loss 1.8446\n",
      "Epoch 2 Batch 1220 Loss 1.1272\n",
      "Epoch 2 Batch 1221 Loss 1.8334\n",
      "Epoch 2 Batch 1222 Loss 1.6484\n",
      "Epoch 2 Batch 1223 Loss 1.5135\n",
      "Epoch 2 Batch 1224 Loss 1.9883\n",
      "Epoch 2 Batch 1225 Loss 1.6200\n",
      "Epoch 2 Batch 1226 Loss 2.2839\n",
      "Epoch 2 Batch 1227 Loss 1.6860\n",
      "Epoch 2 Batch 1228 Loss 1.4670\n",
      "Epoch 2 Batch 1229 Loss 1.5142\n",
      "Epoch 2 Batch 1230 Loss 1.6729\n",
      "Epoch 2 Batch 1231 Loss 2.5193\n",
      "Epoch 2 Batch 1232 Loss 2.2802\n",
      "Epoch 2 Batch 1233 Loss 1.7339\n",
      "Epoch 2 Batch 1234 Loss 2.0220\n",
      "Epoch 2 Batch 1235 Loss 1.8796\n",
      "Epoch 2 Batch 1236 Loss 1.6948\n",
      "Epoch 2 Batch 1237 Loss 1.8380\n",
      "Epoch 2 Batch 1238 Loss 1.5131\n",
      "Epoch 2 Batch 1239 Loss 1.9409\n",
      "Epoch 2 Batch 1240 Loss 1.5605\n",
      "Epoch 2 Batch 1241 Loss 1.3628\n",
      "Epoch 2 Batch 1242 Loss 1.6104\n",
      "Epoch 2 Batch 1243 Loss 1.5938\n",
      "Epoch 2 Batch 1244 Loss 2.1519\n",
      "Epoch 2 Batch 1245 Loss 1.7903\n",
      "Epoch 2 Batch 1246 Loss 1.7985\n",
      "Epoch 2 Batch 1247 Loss 1.8194\n",
      "Epoch 2 Batch 1248 Loss 1.6857\n",
      "Epoch 2 Batch 1249 Loss 1.9561\n",
      "Epoch 2 Batch 1250 Loss 1.7690\n",
      "Epoch 2 Batch 1251 Loss 1.6542\n",
      "Epoch 2 Batch 1252 Loss 1.8690\n",
      "Epoch 2 Batch 1253 Loss 1.8758\n",
      "Epoch 2 Batch 1254 Loss 2.0846\n",
      "Epoch 2 Batch 1255 Loss 1.8748\n",
      "Epoch 2 Batch 1256 Loss 1.5585\n",
      "Epoch 2 Batch 1257 Loss 1.7069\n",
      "Epoch 2 Batch 1258 Loss 1.6468\n",
      "Epoch 2 Batch 1259 Loss 1.9731\n",
      "Epoch 2 Batch 1260 Loss 1.6364\n",
      "Epoch 2 Batch 1261 Loss 2.1867\n",
      "Epoch 2 Batch 1262 Loss 1.6993\n",
      "Epoch 2 Batch 1263 Loss 1.9517\n",
      "Epoch 2 Batch 1264 Loss 1.8718\n",
      "Epoch 2 Batch 1265 Loss 1.7797\n",
      "Epoch 2 Batch 1266 Loss 1.8667\n",
      "Epoch 2 Batch 1267 Loss 1.6551\n",
      "Epoch 2 Batch 1268 Loss 1.5509\n",
      "Epoch 2 Batch 1269 Loss 2.2663\n",
      "Epoch 2 Batch 1270 Loss 1.7354\n",
      "Epoch 2 Batch 1271 Loss 2.0875\n",
      "Epoch 2 Batch 1272 Loss 2.2542\n",
      "Epoch 2 Batch 1273 Loss 2.2248\n",
      "Epoch 2 Batch 1274 Loss 2.3361\n",
      "Epoch 2 Batch 1275 Loss 2.2383\n",
      "Epoch 2 Batch 1276 Loss 2.4114\n",
      "Epoch 2 Batch 1277 Loss 1.9564\n",
      "Epoch 2 Batch 1278 Loss 1.6024\n",
      "Epoch 2 Batch 1279 Loss 1.8967\n",
      "Epoch 2 Batch 1280 Loss 2.0044\n",
      "Epoch 2 Batch 1281 Loss 1.9289\n",
      "Epoch 2 Batch 1282 Loss 1.5699\n",
      "Epoch 2 Batch 1283 Loss 1.4638\n",
      "Epoch 2 Batch 1284 Loss 1.2354\n",
      "Epoch 2 Batch 1285 Loss 1.7883\n",
      "Epoch 2 Batch 1286 Loss 1.4238\n",
      "Epoch 2 Batch 1287 Loss 1.6320\n",
      "Epoch 2 Batch 1288 Loss 1.4928\n",
      "Epoch 2 Batch 1289 Loss 1.6683\n",
      "Epoch 2 Batch 1290 Loss 1.5278\n",
      "Epoch 2 Batch 1291 Loss 1.4675\n",
      "Epoch 2 Batch 1292 Loss 1.4166\n",
      "Epoch 2 Batch 1293 Loss 1.3587\n",
      "Epoch 2 Batch 1294 Loss 1.2682\n",
      "Epoch 2 Batch 1295 Loss 1.1093\n",
      "Epoch 2 Batch 1296 Loss 1.0064\n",
      "Epoch 2 Batch 1297 Loss 1.1916\n",
      "Epoch 2 Batch 1298 Loss 1.2317\n",
      "Epoch 2 Batch 1299 Loss 1.3056\n",
      "Epoch 2 Batch 1300 Loss 1.4389\n",
      "Epoch 2 Batch 1301 Loss 1.1233\n",
      "Epoch 2 Batch 1302 Loss 1.3442\n",
      "Epoch 2 Batch 1303 Loss 1.2891\n",
      "Epoch 2 Batch 1304 Loss 1.5017\n",
      "Epoch 2 Batch 1305 Loss 1.9496\n",
      "Epoch 2 Batch 1306 Loss 1.3737\n",
      "Epoch 2 Batch 1307 Loss 1.5267\n",
      "Epoch 2 Batch 1308 Loss 1.6701\n",
      "Epoch 2 Batch 1309 Loss 2.0463\n",
      "Epoch 2 Batch 1310 Loss 1.8808\n",
      "Epoch 2 Batch 1311 Loss 1.4649\n",
      "Epoch 2 Batch 1312 Loss 1.5018\n",
      "Epoch 2 Batch 1313 Loss 1.5395\n",
      "Epoch 2 Batch 1314 Loss 1.2413\n",
      "Epoch 2 Batch 1315 Loss 1.4149\n",
      "Epoch 2 Batch 1316 Loss 1.7406\n",
      "Epoch 2 Batch 1317 Loss 1.3877\n",
      "Epoch 2 Batch 1318 Loss 2.0123\n",
      "Epoch 2 Batch 1319 Loss 1.5526\n",
      "Epoch 2 Batch 1320 Loss 1.7112\n",
      "Epoch 2 Batch 1321 Loss 1.2776\n",
      "Epoch 2 Batch 1322 Loss 2.0088\n",
      "Epoch 2 Batch 1323 Loss 1.7741\n",
      "Epoch 2 Batch 1324 Loss 1.9463\n",
      "Epoch 2 Batch 1325 Loss 1.8606\n",
      "Epoch 2 Batch 1326 Loss 1.9291\n",
      "Epoch 2 Batch 1327 Loss 1.5608\n",
      "Epoch 2 Batch 1328 Loss 1.7958\n",
      "Epoch 2 Batch 1329 Loss 1.8649\n",
      "Epoch 2 Batch 1330 Loss 1.3605\n",
      "Epoch 2 Batch 1331 Loss 1.1481\n",
      "Epoch 2 Batch 1332 Loss 1.5950\n",
      "Epoch 2 Batch 1333 Loss 1.4627\n",
      "Epoch 2 Batch 1334 Loss 1.5730\n",
      "Epoch 2 Batch 1335 Loss 1.6596\n",
      "Epoch 2 Batch 1336 Loss 1.9827\n",
      "Epoch 2 Batch 1337 Loss 2.0180\n",
      "Epoch 2 Batch 1338 Loss 2.0654\n",
      "Epoch 2 Batch 1339 Loss 2.3720\n",
      "Epoch 2 Batch 1340 Loss 2.0477\n",
      "Epoch 2 Batch 1341 Loss 1.8146\n",
      "Epoch 2 Batch 1342 Loss 1.5801\n",
      "Epoch 2 Batch 1343 Loss 1.6767\n",
      "Epoch 2 Batch 1344 Loss 1.6205\n",
      "Epoch 2 Batch 1345 Loss 1.8133\n",
      "Epoch 2 Batch 1346 Loss 1.5325\n",
      "Epoch 2 Batch 1347 Loss 1.5497\n",
      "Epoch 2 Batch 1348 Loss 1.7263\n",
      "Epoch 2 Batch 1349 Loss 1.7700\n",
      "Epoch 2 Batch 1350 Loss 1.5481\n",
      "Epoch 2 Batch 1351 Loss 1.6615\n",
      "Epoch 2 Batch 1352 Loss 1.6459\n",
      "Epoch 2 Batch 1353 Loss 1.7542\n",
      "Epoch 2 Batch 1354 Loss 1.6917\n",
      "Epoch 2 Batch 1355 Loss 1.4809\n",
      "Epoch 2 Batch 1356 Loss 1.6164\n",
      "Epoch 2 Batch 1357 Loss 2.1046\n",
      "Epoch 2 Batch 1358 Loss 1.6512\n",
      "Epoch 2 Batch 1359 Loss 1.5764\n",
      "Epoch 2 Batch 1360 Loss 1.4627\n",
      "Epoch 2 Batch 1361 Loss 2.0845\n",
      "Epoch 2 Batch 1362 Loss 1.7874\n",
      "Epoch 2 Batch 1363 Loss 1.2380\n",
      "Epoch 2 Batch 1364 Loss 1.8117\n",
      "Epoch 2 Batch 1365 Loss 1.5693\n",
      "Epoch 2 Batch 1366 Loss 1.3688\n",
      "Epoch 2 Batch 1367 Loss 1.6340\n",
      "Epoch 2 Batch 1368 Loss 1.7488\n",
      "Epoch 2 Batch 1369 Loss 1.8727\n",
      "Epoch 2 Batch 1370 Loss 1.6145\n",
      "Epoch 2 Batch 1371 Loss 1.8551\n",
      "Epoch 2 Batch 1372 Loss 1.6548\n",
      "Epoch 2 Batch 1373 Loss 1.6369\n",
      "Epoch 2 Batch 1374 Loss 1.8914\n",
      "Epoch 2 Batch 1375 Loss 1.4384\n",
      "Epoch 2 Batch 1376 Loss 1.1699\n",
      "Epoch 2 Batch 1377 Loss 1.1731\n",
      "Epoch 2 Batch 1378 Loss 1.4641\n",
      "Epoch 2 Batch 1379 Loss 1.6358\n",
      "Epoch 2 Batch 1380 Loss 2.1205\n",
      "Epoch 2 Batch 1381 Loss 1.9664\n",
      "Epoch 2 Batch 1382 Loss 1.9266\n",
      "Epoch 2 Batch 1383 Loss 1.4990\n",
      "Epoch 2 Batch 1384 Loss 1.8463\n",
      "Epoch 2 Batch 1385 Loss 1.9721\n",
      "Epoch 2 Batch 1386 Loss 1.8210\n",
      "Epoch 2 Batch 1387 Loss 1.1960\n",
      "Epoch 2 Batch 1388 Loss 1.3449\n",
      "Epoch 2 Batch 1389 Loss 1.8043\n",
      "Epoch 2 Batch 1390 Loss 1.5138\n",
      "Epoch 2 Batch 1391 Loss 2.0527\n",
      "Epoch 2 Batch 1392 Loss 1.7826\n",
      "Epoch 2 Batch 1393 Loss 1.7738\n",
      "Epoch 2 Batch 1394 Loss 2.0126\n",
      "Epoch 2 Batch 1395 Loss 1.7070\n",
      "Epoch 2 Batch 1396 Loss 1.4162\n",
      "Epoch 2 Batch 1397 Loss 1.5556\n",
      "Epoch 2 Batch 1398 Loss 1.5193\n",
      "Epoch 2 Batch 1399 Loss 1.5174\n",
      "Epoch 2 Batch 1400 Loss 1.3697\n",
      "Epoch 2 Batch 1401 Loss 1.4058\n",
      "Epoch 2 Batch 1402 Loss 1.3730\n",
      "Epoch 2 Batch 1403 Loss 1.6037\n",
      "Epoch 2 Batch 1404 Loss 1.5293\n",
      "Epoch 2 Batch 1405 Loss 1.6232\n",
      "Epoch 2 Batch 1406 Loss 2.0042\n",
      "Epoch 2 Batch 1407 Loss 2.2831\n",
      "Epoch 2 Batch 1408 Loss 2.2158\n",
      "Epoch 2 Batch 1409 Loss 2.1755\n",
      "Epoch 2 Batch 1410 Loss 1.9090\n",
      "Epoch 2 Batch 1411 Loss 1.8197\n",
      "Epoch 2 Batch 1412 Loss 1.6874\n",
      "Epoch 2 Batch 1413 Loss 1.3354\n",
      "Epoch 2 Batch 1414 Loss 1.3926\n",
      "Epoch 2 Batch 1415 Loss 1.0861\n",
      "Epoch 2 Batch 1416 Loss 1.3823\n",
      "Epoch 2 Batch 1417 Loss 1.3334\n",
      "Epoch 2 Batch 1418 Loss 1.3743\n",
      "Epoch 2 Batch 1419 Loss 1.8852\n",
      "Epoch 2 Batch 1420 Loss 1.9096\n",
      "Epoch 2 Batch 1421 Loss 1.6552\n",
      "Epoch 2 Batch 1422 Loss 1.8105\n",
      "Epoch 2 Batch 1423 Loss 1.8490\n",
      "Epoch 2 Batch 1424 Loss 1.5439\n",
      "Epoch 2 Batch 1425 Loss 1.3441\n",
      "Epoch 2 Batch 1426 Loss 1.8187\n",
      "Epoch 2 Batch 1427 Loss 1.7446\n",
      "Epoch 2 Batch 1428 Loss 1.7920\n",
      "Epoch 2 Batch 1429 Loss 1.7472\n",
      "Epoch 2 Batch 1430 Loss 1.4999\n",
      "Epoch 2 Batch 1431 Loss 1.5969\n",
      "Epoch 2 Batch 1432 Loss 1.7717\n",
      "Epoch 2 Batch 1433 Loss 1.5340\n",
      "Epoch 2 Batch 1434 Loss 1.8354\n",
      "Epoch 2 Batch 1435 Loss 2.2365\n",
      "Epoch 2 Batch 1436 Loss 2.2867\n",
      "Epoch 2 Batch 1437 Loss 1.9688\n",
      "Epoch 2 Batch 1438 Loss 1.5437\n",
      "Epoch 2 Batch 1439 Loss 1.8089\n",
      "Epoch 2 Batch 1440 Loss 1.4654\n",
      "Epoch 2 Batch 1441 Loss 1.9137\n",
      "Epoch 2 Batch 1442 Loss 2.0561\n",
      "Epoch 2 Batch 1443 Loss 2.1928\n",
      "Epoch 2 Batch 1444 Loss 2.4078\n",
      "Epoch 2 Batch 1445 Loss 2.0559\n",
      "Epoch 2 Batch 1446 Loss 1.9701\n",
      "Epoch 2 Batch 1447 Loss 1.7651\n",
      "Epoch 2 Batch 1448 Loss 2.1120\n",
      "Epoch 2 Batch 1449 Loss 1.3104\n",
      "Epoch 2 Batch 1450 Loss 1.4539\n",
      "Epoch 2 Batch 1451 Loss 1.7190\n",
      "Epoch 2 Batch 1452 Loss 1.1874\n",
      "Epoch 2 Batch 1453 Loss 2.1505\n",
      "Epoch 2 Batch 1454 Loss 1.5673\n",
      "Epoch 2 Batch 1455 Loss 1.4914\n",
      "Epoch 2 Batch 1456 Loss 1.8404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Batch 1457 Loss 1.4511\n",
      "Epoch 2 Batch 1458 Loss 1.3410\n",
      "Epoch 2 Batch 1459 Loss 1.2117\n",
      "Epoch 2 Batch 1460 Loss 1.1700\n",
      "Epoch 2 Batch 1461 Loss 1.0425\n",
      "Epoch 2 Batch 1462 Loss 1.0422\n",
      "Epoch 2 Batch 1463 Loss 1.5183\n",
      "Epoch 2 Batch 1464 Loss 1.8602\n",
      "Epoch 2 Batch 1465 Loss 1.2234\n",
      "Epoch 2 Batch 1466 Loss 1.8263\n",
      "Epoch 2 Batch 1467 Loss 1.9694\n",
      "Epoch 2 Batch 1468 Loss 2.2182\n",
      "Epoch 2 Batch 1469 Loss 1.6182\n",
      "Epoch 2 Batch 1470 Loss 1.5797\n",
      "Epoch 2 Batch 1471 Loss 1.7319\n",
      "Epoch 2 Batch 1472 Loss 1.8827\n",
      "Epoch 2 Batch 1473 Loss 1.8429\n",
      "Epoch 2 Batch 1474 Loss 1.8932\n",
      "Epoch 2 Batch 1475 Loss 1.8751\n",
      "Epoch 2 Batch 1476 Loss 1.7006\n",
      "Epoch 2 Batch 1477 Loss 1.6550\n",
      "Epoch 2 Batch 1478 Loss 1.8305\n",
      "Epoch 2 Batch 1479 Loss 1.3209\n",
      "Epoch 2 Batch 1480 Loss 1.4558\n",
      "Epoch 2 Batch 1481 Loss 1.6966\n",
      "Epoch 2 Batch 1482 Loss 1.8829\n",
      "Epoch 2 Batch 1483 Loss 1.9668\n",
      "Epoch 2 Batch 1484 Loss 1.5463\n",
      "Epoch 2 Batch 1485 Loss 1.3049\n",
      "Epoch 2 Batch 1486 Loss 1.3465\n",
      "Epoch 2 Batch 1487 Loss 1.1690\n",
      "Epoch 2 Batch 1488 Loss 1.2716\n",
      "Epoch 2 Batch 1489 Loss 1.7470\n",
      "Epoch 2 Batch 1490 Loss 1.5952\n",
      "Epoch 2 Batch 1491 Loss 2.2239\n",
      "Epoch 2 Batch 1492 Loss 2.0800\n",
      "Epoch 2 Batch 1493 Loss 2.0503\n",
      "Epoch 2 Batch 1494 Loss 1.9186\n",
      "Epoch 2 Batch 1495 Loss 1.7346\n",
      "Epoch 2 Batch 1496 Loss 1.3714\n",
      "Epoch 2 Batch 1497 Loss 1.2381\n",
      "Epoch 2 Batch 1498 Loss 1.3576\n",
      "Epoch 2 Batch 1499 Loss 1.6743\n",
      "Epoch 2 Batch 1500 Loss 1.5940\n",
      "Epoch 2 Batch 1501 Loss 1.7213\n",
      "Epoch 2 Batch 1502 Loss 1.9084\n",
      "Epoch 2 Batch 1503 Loss 1.6029\n",
      "Epoch 2 Batch 1504 Loss 1.3138\n",
      "Epoch 2 Batch 1505 Loss 1.4303\n",
      "Epoch 2 Batch 1506 Loss 1.5196\n",
      "Epoch 2 Batch 1507 Loss 1.4216\n",
      "Epoch 2 Batch 1508 Loss 1.7144\n",
      "Epoch 2 Batch 1509 Loss 1.5766\n",
      "Epoch 2 Batch 1510 Loss 1.6693\n",
      "Epoch 2 Batch 1511 Loss 2.0745\n",
      "Epoch 2 Batch 1512 Loss 1.9614\n",
      "Epoch 2 Batch 1513 Loss 1.7961\n",
      "Epoch 2 Batch 1514 Loss 2.1980\n",
      "Epoch 2 Batch 1515 Loss 1.6111\n",
      "Epoch 2 Batch 1516 Loss 2.3505\n",
      "Epoch 2 Batch 1517 Loss 1.9713\n",
      "Epoch 2 Batch 1518 Loss 1.5173\n",
      "Epoch 2 Batch 1519 Loss 1.4790\n",
      "Epoch 2 Batch 1520 Loss 1.3123\n",
      "Epoch 2 Batch 1521 Loss 2.0619\n",
      "Epoch 2 Batch 1522 Loss 2.0493\n",
      "Epoch 2 Batch 1523 Loss 1.9010\n",
      "Epoch 2 Batch 1524 Loss 1.7765\n",
      "Epoch 2 Batch 1525 Loss 1.6821\n",
      "Epoch 2 Batch 1526 Loss 1.5896\n",
      "Epoch 2 Batch 1527 Loss 1.9784\n",
      "Epoch 2 Batch 1528 Loss 1.3364\n",
      "Epoch 2 Batch 1529 Loss 2.3384\n",
      "Epoch 2 Batch 1530 Loss 1.9400\n",
      "Epoch 2 Batch 1531 Loss 1.5473\n",
      "Epoch 2 Batch 1532 Loss 1.6566\n",
      "Epoch 2 Batch 1533 Loss 1.7658\n",
      "Epoch 2 Batch 1534 Loss 2.1766\n",
      "Epoch 2 Batch 1535 Loss 2.0068\n",
      "Epoch 2 Batch 1536 Loss 1.6483\n",
      "Epoch 2 Batch 1537 Loss 1.6343\n",
      "Epoch 2 Batch 1538 Loss 1.6725\n",
      "Epoch 2 Batch 1539 Loss 1.8988\n",
      "Epoch 2 Batch 1540 Loss 2.0293\n",
      "Epoch 2 Batch 1541 Loss 1.3627\n",
      "Epoch 2 Batch 1542 Loss 1.1871\n",
      "Epoch 2 Batch 1543 Loss 1.3355\n",
      "Epoch 2 Batch 1544 Loss 1.6371\n",
      "Epoch 2 Batch 1545 Loss 1.7831\n",
      "Epoch 2 Batch 1546 Loss 1.7834\n",
      "Epoch 2 Batch 1547 Loss 2.0223\n",
      "Epoch 2 Batch 1548 Loss 1.9973\n",
      "Epoch 2 Batch 1549 Loss 1.5451\n",
      "Epoch 2 Batch 1550 Loss 1.7170\n",
      "Epoch 2 Batch 1551 Loss 1.7736\n",
      "Epoch 2 Batch 1552 Loss 1.5720\n",
      "Epoch 2 Batch 1553 Loss 1.4702\n",
      "Epoch 2 Batch 1554 Loss 1.6829\n",
      "Epoch 2 Batch 1555 Loss 2.0235\n",
      "Epoch 2 Batch 1556 Loss 1.8463\n",
      "Epoch 2 Batch 1557 Loss 1.4433\n",
      "Epoch 2 Batch 1558 Loss 1.7059\n",
      "Epoch 2 Batch 1559 Loss 1.6626\n",
      "Epoch 2 Batch 1560 Loss 1.7068\n",
      "Epoch 2 Batch 1561 Loss 1.6090\n",
      "Epoch 2 Batch 1562 Loss 1.4638\n",
      "Epoch 2 Batch 1563 Loss 1.6624\n",
      "Epoch 2 Batch 1564 Loss 1.6731\n",
      "Epoch 2 Batch 1565 Loss 1.7475\n",
      "Epoch 2 Batch 1566 Loss 2.0469\n",
      "Epoch 2 Batch 1567 Loss 1.8560\n",
      "Epoch 2 Batch 1568 Loss 1.9457\n",
      "Epoch 2 Batch 1569 Loss 1.8386\n",
      "Epoch 2 Batch 1570 Loss 1.7975\n",
      "Epoch 2 Batch 1571 Loss 2.2471\n",
      "Epoch 2 Batch 1572 Loss 1.9624\n",
      "Epoch 2 Batch 1573 Loss 2.0754\n",
      "Epoch 2 Batch 1574 Loss 1.8348\n",
      "Epoch 2 Batch 1575 Loss 2.0830\n",
      "Epoch 2 Batch 1576 Loss 1.6896\n",
      "Epoch 2 Batch 1577 Loss 2.0449\n",
      "Epoch 2 Batch 1578 Loss 1.8148\n",
      "Epoch 2 Batch 1579 Loss 1.6797\n",
      "Epoch 2 Batch 1580 Loss 1.7694\n",
      "Epoch 2 Batch 1581 Loss 1.8551\n",
      "Epoch 2 Batch 1582 Loss 1.5604\n",
      "Epoch 2 Batch 1583 Loss 1.5360\n",
      "Epoch 2 Batch 1584 Loss 1.8231\n",
      "Epoch 2 Batch 1585 Loss 2.1780\n",
      "Epoch 2 Batch 1586 Loss 1.9609\n",
      "Epoch 2 Batch 1587 Loss 1.6157\n",
      "Epoch 2 Batch 1588 Loss 1.7227\n",
      "Epoch 2 Batch 1589 Loss 1.3542\n",
      "Epoch 2 Batch 1590 Loss 1.7923\n",
      "Epoch 2 Batch 1591 Loss 2.0026\n",
      "Epoch 2 Batch 1592 Loss 1.7931\n",
      "Epoch 2 Batch 1593 Loss 1.6333\n",
      "Epoch 2 Batch 1594 Loss 1.7899\n",
      "Epoch 2 Batch 1595 Loss 1.4460\n",
      "Epoch 2 Batch 1596 Loss 2.2468\n",
      "Epoch 2 Batch 1597 Loss 1.8066\n",
      "Epoch 2 Batch 1598 Loss 1.8005\n",
      "Epoch 2 Batch 1599 Loss 2.1639\n",
      "Epoch 2 Batch 1600 Loss 1.9311\n",
      "Epoch 2 Batch 1601 Loss 2.2667\n",
      "Epoch 2 Batch 1602 Loss 1.7025\n",
      "Epoch 2 Batch 1603 Loss 1.9929\n",
      "Epoch 2 Batch 1604 Loss 2.1572\n",
      "Epoch 2 Batch 1605 Loss 1.9373\n",
      "Epoch 2 Batch 1606 Loss 1.8266\n",
      "Epoch 2 Batch 1607 Loss 1.7123\n",
      "Epoch 2 Batch 1608 Loss 2.2478\n",
      "Epoch 2 Batch 1609 Loss 2.0583\n",
      "Epoch 2 Batch 1610 Loss 1.6642\n",
      "Epoch 2 Batch 1611 Loss 1.6143\n",
      "Epoch 2 Batch 1612 Loss 1.8509\n",
      "Epoch 2 Batch 1613 Loss 1.7961\n",
      "Epoch 2 Batch 1614 Loss 1.5495\n",
      "Epoch 2 Batch 1615 Loss 1.8092\n",
      "Epoch 2 Batch 1616 Loss 1.5241\n",
      "Epoch 2 Batch 1617 Loss 1.4884\n",
      "Epoch 2 Batch 1618 Loss 1.6603\n",
      "Epoch 2 Batch 1619 Loss 1.8311\n",
      "Epoch 2 Batch 1620 Loss 1.8054\n",
      "Epoch 2 Batch 1621 Loss 1.5232\n",
      "Epoch 2 Batch 1622 Loss 1.4828\n",
      "Epoch 2 Batch 1623 Loss 1.6671\n",
      "Epoch 2 Batch 1624 Loss 1.6413\n",
      "Epoch 2 Batch 1625 Loss 2.0302\n",
      "Epoch 2 Batch 1626 Loss 1.5712\n",
      "Epoch 2 Batch 1627 Loss 2.5157\n",
      "Epoch 2 Batch 1628 Loss 1.8675\n",
      "Epoch 2 Batch 1629 Loss 2.2011\n",
      "Epoch 2 Batch 1630 Loss 2.0961\n",
      "Epoch 2 Batch 1631 Loss 1.8195\n",
      "Epoch 2 Batch 1632 Loss 1.9016\n",
      "Epoch 2 Batch 1633 Loss 1.6816\n",
      "Epoch 2 Batch 1634 Loss 2.0685\n",
      "Epoch 2 Batch 1635 Loss 2.3937\n",
      "Epoch 2 Batch 1636 Loss 1.7606\n",
      "Epoch 2 Batch 1637 Loss 1.3043\n",
      "Epoch 2 Batch 1638 Loss 1.9558\n",
      "Epoch 2 Batch 1639 Loss 1.7633\n",
      "Epoch 2 Batch 1640 Loss 2.3248\n",
      "Epoch 2 Batch 1641 Loss 1.6180\n",
      "Epoch 2 Batch 1642 Loss 1.5126\n",
      "Epoch 2 Batch 1643 Loss 1.8458\n",
      "Epoch 2 Batch 1644 Loss 1.3715\n",
      "Epoch 2 Batch 1645 Loss 1.1046\n",
      "Epoch 2 Batch 1646 Loss 1.2937\n",
      "Epoch 2 Batch 1647 Loss 1.3183\n",
      "Epoch 2 Batch 1648 Loss 1.3441\n",
      "Epoch 2 Batch 1649 Loss 1.7154\n",
      "Epoch 2 Batch 1650 Loss 1.8516\n",
      "Epoch 2 Batch 1651 Loss 1.6894\n",
      "Epoch 2 Batch 1652 Loss 1.4945\n",
      "Epoch 2 Batch 1653 Loss 1.8814\n",
      "Epoch 2 Batch 1654 Loss 1.4438\n",
      "Epoch 2 Batch 1655 Loss 1.6947\n",
      "Epoch 2 Batch 1656 Loss 2.2582\n",
      "Epoch 2 Batch 1657 Loss 1.7219\n",
      "Epoch 2 Batch 1658 Loss 1.8188\n",
      "Epoch 2 Batch 1659 Loss 2.3544\n",
      "Epoch 2 Batch 1660 Loss 1.3545\n",
      "Epoch 2 Batch 1661 Loss 1.7489\n",
      "Epoch 2 Batch 1662 Loss 1.7620\n",
      "Epoch 2 Batch 1663 Loss 2.2495\n",
      "Epoch 2 Batch 1664 Loss 1.9269\n",
      "Epoch 2 Batch 1665 Loss 2.1002\n",
      "Epoch 2 Batch 1666 Loss 2.1125\n",
      "Epoch 2 Batch 1667 Loss 2.0036\n",
      "Epoch 2 Batch 1668 Loss 1.8198\n",
      "Epoch 2 Batch 1669 Loss 2.3620\n",
      "Epoch 2 Batch 1670 Loss 1.7005\n",
      "Epoch 2 Batch 1671 Loss 1.9144\n",
      "Epoch 2 Batch 1672 Loss 1.5297\n",
      "Epoch 2 Batch 1673 Loss 1.6429\n",
      "Epoch 2 Batch 1674 Loss 2.0285\n",
      "Epoch 2 Batch 1675 Loss 1.9610\n",
      "Epoch 2 Batch 1676 Loss 1.5394\n",
      "Epoch 2 Batch 1677 Loss 2.4699\n",
      "Epoch 2 Batch 1678 Loss 2.0153\n",
      "Epoch 2 Batch 1679 Loss 1.8804\n",
      "Epoch 2 Batch 1680 Loss 1.6485\n",
      "Epoch 2 Batch 1681 Loss 1.5690\n",
      "Epoch 2 Batch 1682 Loss 1.8162\n",
      "Epoch 2 Batch 1683 Loss 1.3685\n",
      "Epoch 2 Batch 1684 Loss 1.5434\n",
      "Epoch 2 Batch 1685 Loss 1.5203\n",
      "Epoch 2 Batch 1686 Loss 1.5004\n",
      "Epoch 2 Batch 1687 Loss 1.5229\n",
      "Epoch 2 Batch 1688 Loss 1.5056\n",
      "Epoch 2 Batch 1689 Loss 1.5782\n",
      "Epoch 2 Batch 1690 Loss 1.7642\n",
      "Epoch 2 Batch 1691 Loss 1.6123\n",
      "Epoch 2 Batch 1692 Loss 2.0056\n",
      "Epoch 2 Batch 1693 Loss 2.2377\n",
      "Epoch 2 Batch 1694 Loss 2.0427\n",
      "Epoch 2 Batch 1695 Loss 1.6855\n",
      "Epoch 2 Batch 1696 Loss 1.9513\n",
      "Epoch 2 Batch 1697 Loss 1.7489\n",
      "Epoch 2 Batch 1698 Loss 1.9104\n",
      "Epoch 2 Batch 1699 Loss 2.0608\n",
      "Epoch 2 Batch 1700 Loss 2.2060\n",
      "Epoch 2 Batch 1701 Loss 1.6001\n",
      "Epoch 2 Batch 1702 Loss 1.9167\n",
      "Epoch 2 Batch 1703 Loss 1.9643\n",
      "Epoch 2 Batch 1704 Loss 1.7771\n",
      "Epoch 2 Batch 1705 Loss 1.4702\n",
      "Epoch 2 Batch 1706 Loss 1.7849\n",
      "Epoch 2 Batch 1707 Loss 2.2215\n",
      "Epoch 2 Batch 1708 Loss 1.6322\n",
      "Epoch 2 Batch 1709 Loss 2.1969\n",
      "Epoch 2 Batch 1710 Loss 2.2470\n",
      "Epoch 2 Batch 1711 Loss 1.7371\n",
      "Epoch 2 Batch 1712 Loss 1.9384\n",
      "Epoch 2 Batch 1713 Loss 2.0516\n",
      "Epoch 2 Batch 1714 Loss 2.0061\n",
      "Epoch 2 Batch 1715 Loss 1.8284\n",
      "Epoch 2 Batch 1716 Loss 1.4432\n",
      "Epoch 2 Batch 1717 Loss 1.7737\n",
      "Epoch 2 Batch 1718 Loss 1.8753\n",
      "Epoch 2 Batch 1719 Loss 2.0802\n",
      "Epoch 2 Batch 1720 Loss 2.1914\n",
      "Epoch 2 Batch 1721 Loss 1.7957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Batch 1722 Loss 1.5812\n",
      "Epoch 2 Batch 1723 Loss 1.5656\n",
      "Epoch 2 Batch 1724 Loss 1.7838\n",
      "Epoch 2 Batch 1725 Loss 1.9295\n",
      "Epoch 2 Batch 1726 Loss 1.6679\n",
      "Epoch 2 Batch 1727 Loss 1.5300\n",
      "Epoch 2 Batch 1728 Loss 1.3320\n",
      "Epoch 2 Batch 1729 Loss 2.0356\n",
      "Epoch 2 Batch 1730 Loss 2.2335\n",
      "Epoch 2 Batch 1731 Loss 2.0170\n",
      "Epoch 2 Batch 1732 Loss 1.8651\n",
      "Epoch 2 Batch 1733 Loss 1.4591\n",
      "Epoch 2 Batch 1734 Loss 1.3171\n",
      "Epoch 2 Batch 1735 Loss 1.2969\n",
      "Epoch 2 Batch 1736 Loss 1.4808\n",
      "Epoch 2 Batch 1737 Loss 2.6074\n",
      "Epoch 2 Batch 1738 Loss 2.0553\n",
      "Epoch 2 Batch 1739 Loss 1.9081\n",
      "Epoch 2 Batch 1740 Loss 1.7225\n",
      "Epoch 2 Batch 1741 Loss 1.4212\n",
      "Epoch 2 Batch 1742 Loss 1.4733\n",
      "Epoch 2 Batch 1743 Loss 1.3731\n",
      "Epoch 2 Batch 1744 Loss 1.4849\n",
      "Epoch 2 Batch 1745 Loss 1.7276\n",
      "Epoch 2 Batch 1746 Loss 1.7612\n",
      "Epoch 2 Batch 1747 Loss 1.4508\n",
      "Epoch 2 Batch 1748 Loss 1.5653\n",
      "Epoch 2 Batch 1749 Loss 1.8352\n",
      "Epoch 2 Batch 1750 Loss 1.7904\n",
      "Epoch 2 Batch 1751 Loss 1.9852\n",
      "Epoch 2 Batch 1752 Loss 1.6336\n",
      "Epoch 2 Batch 1753 Loss 2.3096\n",
      "Epoch 2 Batch 1754 Loss 2.2080\n",
      "Epoch 2 Batch 1755 Loss 2.2176\n",
      "Epoch 2 Batch 1756 Loss 1.4933\n",
      "Epoch 2 Batch 1757 Loss 2.0798\n",
      "Epoch 2 Batch 1758 Loss 1.6932\n",
      "Epoch 2 Batch 1759 Loss 1.8167\n",
      "Epoch 2 Batch 1760 Loss 1.9447\n",
      "Epoch 2 Batch 1761 Loss 1.8496\n",
      "Epoch 2 Batch 1762 Loss 1.6507\n",
      "Epoch 2 Batch 1763 Loss 1.8577\n",
      "Epoch 2 Batch 1764 Loss 1.4959\n",
      "Epoch 2 Batch 1765 Loss 1.4806\n",
      "Epoch 2 Batch 1766 Loss 1.7823\n",
      "Epoch 2 Batch 1767 Loss 1.9152\n",
      "Epoch 2 Batch 1768 Loss 2.0408\n",
      "Epoch 2 Batch 1769 Loss 2.0971\n",
      "Epoch 2 Batch 1770 Loss 2.4028\n",
      "Epoch 2 Batch 1771 Loss 1.8828\n",
      "Epoch 2 Batch 1772 Loss 1.9961\n",
      "Epoch 2 Batch 1773 Loss 2.3460\n",
      "Epoch 2 Batch 1774 Loss 1.6794\n",
      "Epoch 2 Batch 1775 Loss 1.9568\n",
      "Epoch 2 Batch 1776 Loss 1.4201\n",
      "Epoch 2 Batch 1777 Loss 1.9166\n",
      "Epoch 2 Batch 1778 Loss 1.6746\n",
      "Epoch 2 Batch 1779 Loss 1.8033\n",
      "Epoch 2 Batch 1780 Loss 2.1121\n",
      "Epoch 2 Batch 1781 Loss 2.0307\n",
      "Epoch 2 Batch 1782 Loss 1.7781\n",
      "Epoch 2 Batch 1783 Loss 2.1770\n",
      "Epoch 2 Batch 1784 Loss 2.1841\n",
      "Epoch 2 Batch 1785 Loss 2.1690\n",
      "Epoch 2 Batch 1786 Loss 1.7425\n",
      "Epoch 2 Batch 1787 Loss 2.0334\n",
      "Epoch 2 Batch 1788 Loss 2.0150\n",
      "Epoch 2 Batch 1789 Loss 1.8061\n",
      "Epoch 2 Batch 1790 Loss 1.8547\n",
      "Epoch 2 Batch 1791 Loss 1.6373\n",
      "Epoch 2 Batch 1792 Loss 1.5504\n",
      "Epoch 2 Batch 1793 Loss 2.2339\n",
      "Epoch 2 Batch 1794 Loss 1.6862\n",
      "Epoch 2 Batch 1795 Loss 1.6700\n",
      "Epoch 2 Batch 1796 Loss 2.0228\n",
      "Epoch 2 Batch 1797 Loss 1.5377\n",
      "Epoch 2 Batch 1798 Loss 1.4595\n",
      "Epoch 2 Batch 1799 Loss 1.9210\n",
      "Epoch 2 Batch 1800 Loss 1.7802\n",
      "Epoch 2 Batch 1801 Loss 1.8599\n",
      "Epoch 2 Batch 1802 Loss 1.8142\n",
      "Epoch 2 Batch 1803 Loss 2.1117\n",
      "Epoch 2 Batch 1804 Loss 2.0149\n",
      "Epoch 2 Batch 1805 Loss 1.6638\n",
      "Epoch 2 Batch 1806 Loss 1.7422\n",
      "Epoch 2 Batch 1807 Loss 1.9113\n",
      "Epoch 2 Batch 1808 Loss 1.7812\n",
      "Epoch 2 Batch 1809 Loss 1.9716\n",
      "Epoch 2 Batch 1810 Loss 1.8269\n",
      "Epoch 2 Batch 1811 Loss 2.0961\n",
      "Epoch 2 Batch 1812 Loss 2.2218\n",
      "Epoch 2 Batch 1813 Loss 2.2777\n",
      "Epoch 2 Batch 1814 Loss 1.9506\n",
      "Epoch 2 Batch 1815 Loss 1.8504\n",
      "Epoch 2 Batch 1816 Loss 1.8365\n",
      "Epoch 2 Batch 1817 Loss 1.9213\n",
      "Epoch 2 Batch 1818 Loss 1.7922\n",
      "Epoch 2 Batch 1819 Loss 1.8728\n",
      "Epoch 2 Batch 1820 Loss 1.7064\n",
      "Epoch 2 Batch 1821 Loss 2.0071\n",
      "Epoch 2 Batch 1822 Loss 1.6523\n",
      "Epoch 2 Batch 1823 Loss 1.6703\n",
      "Epoch 2 Batch 1824 Loss 1.6131\n",
      "Epoch 2 Batch 1825 Loss 1.8717\n",
      "Epoch 2 Batch 1826 Loss 1.5111\n",
      "Epoch 2 Batch 1827 Loss 2.3506\n",
      "Epoch 2 Batch 1828 Loss 2.0294\n",
      "Epoch 2 Batch 1829 Loss 1.9985\n",
      "Epoch 2 Batch 1830 Loss 1.6974\n",
      "Epoch 2 Batch 1831 Loss 2.1303\n",
      "Epoch 2 Batch 1832 Loss 2.0945\n",
      "Epoch 2 Batch 1833 Loss 2.3775\n",
      "Epoch 2 Batch 1834 Loss 1.9553\n",
      "Epoch 2 Batch 1835 Loss 2.2692\n",
      "Epoch 2 Batch 1836 Loss 1.8497\n",
      "Epoch 2 Batch 1837 Loss 2.1765\n",
      "Epoch 2 Batch 1838 Loss 1.9885\n",
      "Epoch 2 Batch 1839 Loss 1.9502\n",
      "Epoch 2 Batch 1840 Loss 1.9409\n",
      "Epoch 2 Batch 1841 Loss 1.7186\n",
      "Epoch 2 Batch 1842 Loss 1.6748\n",
      "Epoch 2 Batch 1843 Loss 1.6295\n",
      "Epoch 2 Batch 1844 Loss 1.6196\n",
      "Epoch 2 Batch 1845 Loss 1.4583\n",
      "Epoch 2 Batch 1846 Loss 1.5721\n",
      "Epoch 2 Batch 1847 Loss 1.8360\n",
      "Epoch 2 Batch 1848 Loss 1.8266\n",
      "Epoch 2 Batch 1849 Loss 1.9744\n",
      "Epoch 2 Batch 1850 Loss 1.7749\n",
      "Epoch 2 Batch 1851 Loss 1.4723\n",
      "Epoch 2 Batch 1852 Loss 1.3927\n",
      "Epoch 2 Batch 1853 Loss 1.7680\n",
      "Epoch 2 Batch 1854 Loss 1.5404\n",
      "Epoch 2 Batch 1855 Loss 1.6919\n",
      "Epoch 2 Batch 1856 Loss 1.4514\n",
      "Epoch 2 Batch 1857 Loss 1.3730\n",
      "Epoch 2 Batch 1858 Loss 1.5987\n",
      "Epoch 2 Batch 1859 Loss 1.4011\n",
      "Epoch 2 Batch 1860 Loss 1.3744\n",
      "Epoch 2 Batch 1861 Loss 1.2363\n",
      "Epoch 2 Batch 1862 Loss 1.9698\n",
      "Epoch 2 Batch 1863 Loss 1.6715\n",
      "Epoch 2 Batch 1864 Loss 1.9965\n",
      "Epoch 2 Batch 1865 Loss 1.4939\n",
      "Epoch 2 Batch 1866 Loss 2.0116\n",
      "Epoch 2 Batch 1867 Loss 1.4209\n",
      "Epoch 2 Batch 1868 Loss 1.8435\n",
      "Epoch 2 Batch 1869 Loss 1.8476\n",
      "Epoch 2 Batch 1870 Loss 1.9414\n",
      "Epoch 2 Batch 1871 Loss 1.9452\n",
      "Epoch 2 Batch 1872 Loss 2.1343\n",
      "Epoch 2 Batch 1873 Loss 2.1908\n",
      "Epoch 2 Batch 1874 Loss 2.5341\n",
      "Epoch 2 Batch 1875 Loss 2.4428\n",
      "Epoch 2 Batch 1876 Loss 2.2633\n",
      "Epoch 2 Batch 1877 Loss 2.2591\n",
      "Epoch 2 Batch 1878 Loss 1.4983\n",
      "Epoch 2 Batch 1879 Loss 1.5393\n",
      "Epoch 2 Batch 1880 Loss 1.7693\n",
      "Epoch 2 Batch 1881 Loss 1.5019\n",
      "Epoch 2 Batch 1882 Loss 1.7743\n",
      "Epoch 2 Batch 1883 Loss 1.8266\n",
      "Epoch 2 Batch 1884 Loss 1.7561\n",
      "Epoch 2 Batch 1885 Loss 1.7989\n",
      "Epoch 2 Batch 1886 Loss 1.7441\n",
      "Epoch 2 Batch 1887 Loss 1.7714\n",
      "Epoch 2 Batch 1888 Loss 1.8460\n",
      "Epoch 2 Batch 1889 Loss 1.5596\n",
      "Epoch 2 Batch 1890 Loss 1.3937\n",
      "Epoch 2 Batch 1891 Loss 1.6379\n",
      "Epoch 2 Batch 1892 Loss 1.5464\n",
      "Epoch 2 Batch 1893 Loss 1.0734\n",
      "Epoch 2 Batch 1894 Loss 1.4148\n",
      "Epoch 2 Batch 1895 Loss 1.4071\n",
      "Epoch 2 Batch 1896 Loss 1.6102\n",
      "Epoch 2 Batch 1897 Loss 2.2434\n",
      "Epoch 2 Batch 1898 Loss 1.9236\n",
      "Epoch 2 Batch 1899 Loss 1.6141\n",
      "Epoch 2 Batch 1900 Loss 1.5361\n",
      "Epoch 2 Batch 1901 Loss 1.8674\n",
      "Epoch 2 Batch 1902 Loss 2.0491\n",
      "Epoch 2 Batch 1903 Loss 1.8380\n",
      "Epoch 2 Batch 1904 Loss 2.0552\n",
      "Epoch 2 Batch 1905 Loss 1.7272\n",
      "Epoch 2 Batch 1906 Loss 1.8054\n",
      "Epoch 2 Batch 1907 Loss 2.1268\n",
      "Epoch 2 Batch 1908 Loss 2.4982\n",
      "Epoch 2 Batch 1909 Loss 2.1683\n",
      "Epoch 2 Batch 1910 Loss 1.9358\n",
      "Epoch 2 Batch 1911 Loss 2.2310\n",
      "Epoch 2 Batch 1912 Loss 1.9034\n",
      "Epoch 2 Batch 1913 Loss 2.3304\n",
      "Epoch 2 Batch 1914 Loss 1.9141\n",
      "Epoch 2 Batch 1915 Loss 2.3817\n",
      "Epoch 2 Batch 1916 Loss 2.0140\n",
      "Epoch 2 Batch 1917 Loss 1.5127\n",
      "Epoch 2 Batch 1918 Loss 1.6920\n",
      "Epoch 2 Batch 1919 Loss 1.5978\n",
      "Epoch 2 Batch 1920 Loss 1.6526\n",
      "Epoch 2 Batch 1921 Loss 1.8054\n",
      "Epoch 2 Batch 1922 Loss 1.7575\n",
      "Epoch 2 Batch 1923 Loss 2.0492\n",
      "Epoch 2 Batch 1924 Loss 1.9820\n",
      "Epoch 2 Batch 1925 Loss 1.7443\n",
      "Epoch 2 Batch 1926 Loss 1.6739\n",
      "Epoch 2 Batch 1927 Loss 2.1007\n",
      "Epoch 2 Batch 1928 Loss 1.7958\n",
      "Epoch 2 Batch 1929 Loss 1.5163\n",
      "Epoch 2 Batch 1930 Loss 1.9131\n",
      "Epoch 2 Batch 1931 Loss 1.9366\n",
      "Epoch 2 Batch 1932 Loss 1.6356\n",
      "Epoch 2 Batch 1933 Loss 2.0476\n",
      "Epoch 2 Batch 1934 Loss 1.5308\n",
      "Epoch 2 Batch 1935 Loss 2.1322\n",
      "Epoch 2 Batch 1936 Loss 1.8982\n",
      "Epoch 2 Batch 1937 Loss 2.0277\n",
      "Epoch 2 Batch 1938 Loss 1.7504\n",
      "Epoch 2 Batch 1939 Loss 1.4610\n",
      "Epoch 2 Batch 1940 Loss 2.4137\n",
      "Epoch 2 Batch 1941 Loss 1.7284\n",
      "Epoch 2 Batch 1942 Loss 1.9285\n",
      "Epoch 2 Batch 1943 Loss 2.1160\n",
      "Epoch 2 Batch 1944 Loss 1.5184\n",
      "Epoch 2 Batch 1945 Loss 1.7982\n",
      "Epoch 2 Batch 1946 Loss 2.3865\n",
      "Epoch 2 Batch 1947 Loss 2.0742\n",
      "Epoch 2 Batch 1948 Loss 1.7387\n",
      "Epoch 2 Batch 1949 Loss 1.5661\n",
      "Epoch 2 Batch 1950 Loss 2.0739\n",
      "Epoch 2 Batch 1951 Loss 1.5599\n",
      "Epoch 2 Batch 1952 Loss 1.7341\n",
      "Epoch 2 Batch 1953 Loss 1.5872\n",
      "Epoch 2 Batch 1954 Loss 1.7066\n",
      "Epoch 2 Batch 1955 Loss 1.9645\n",
      "Epoch 2 Batch 1956 Loss 2.1275\n",
      "Epoch 2 Batch 1957 Loss 2.4936\n",
      "Epoch 2 Batch 1958 Loss 2.0842\n",
      "Epoch 2 Batch 1959 Loss 2.2201\n",
      "Epoch 2 Batch 1960 Loss 2.1832\n",
      "Epoch 2 Batch 1961 Loss 1.9673\n",
      "Epoch 2 Batch 1962 Loss 1.6766\n",
      "Epoch 2 Batch 1963 Loss 1.7009\n",
      "Epoch 2 Batch 1964 Loss 1.9744\n",
      "Epoch 2 Batch 1965 Loss 1.6319\n",
      "Epoch 2 Batch 1966 Loss 1.7359\n",
      "Epoch 2 Batch 1967 Loss 1.4089\n",
      "Epoch 2 Batch 1968 Loss 1.7687\n",
      "Epoch 2 Batch 1969 Loss 1.6043\n",
      "Epoch 2 Batch 1970 Loss 2.0342\n",
      "Epoch 2 Batch 1971 Loss 1.9236\n",
      "Epoch 2 Batch 1972 Loss 2.1851\n",
      "Epoch 2 Batch 1973 Loss 2.1377\n",
      "Epoch 2 Batch 1974 Loss 1.5989\n",
      "Epoch 2 Batch 1975 Loss 1.8619\n",
      "Epoch 2 Batch 1976 Loss 1.9691\n",
      "Epoch 2 Batch 1977 Loss 1.9660\n",
      "Epoch 2 Batch 1978 Loss 2.2757\n",
      "Epoch 2 Batch 1979 Loss 1.7147\n",
      "Epoch 2 Batch 1980 Loss 1.7138\n",
      "Epoch 2 Batch 1981 Loss 1.5832\n",
      "Epoch 2 Batch 1982 Loss 1.6036\n",
      "Epoch 2 Batch 1983 Loss 1.3537\n",
      "Epoch 2 Batch 1984 Loss 1.8319\n",
      "Epoch 2 Batch 1985 Loss 1.9847\n",
      "Epoch 2 Batch 1986 Loss 1.8632\n",
      "Epoch 2 Batch 1987 Loss 1.9832\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Batch 1988 Loss 1.8416\n",
      "Epoch 2 Batch 1989 Loss 1.5954\n",
      "Epoch 2 Batch 1990 Loss 1.6995\n",
      "Epoch 2 Batch 1991 Loss 1.3771\n",
      "Epoch 2 Batch 1992 Loss 1.4248\n",
      "Epoch 2 Batch 1993 Loss 1.9456\n",
      "Epoch 2 Batch 1994 Loss 1.5793\n",
      "Epoch 2 Batch 1995 Loss 1.9000\n",
      "Epoch 2 Batch 1996 Loss 1.8518\n",
      "Epoch 2 Batch 1997 Loss 1.6370\n",
      "Epoch 2 Batch 1998 Loss 1.3670\n",
      "Epoch 2 Batch 1999 Loss 1.6531\n",
      "Epoch 2 Batch 2000 Loss 2.2406\n",
      "Epoch 2 Batch 2001 Loss 1.6798\n",
      "Epoch 2 Batch 2002 Loss 2.0211\n",
      "Epoch 2 Batch 2003 Loss 2.3570\n",
      "Epoch 2 Batch 2004 Loss 1.6874\n",
      "Epoch 2 Batch 2005 Loss 2.0864\n",
      "Epoch 2 Batch 2006 Loss 2.2876\n",
      "Epoch 2 Batch 2007 Loss 1.9529\n",
      "Epoch 2 Batch 2008 Loss 2.2285\n",
      "Epoch 2 Batch 2009 Loss 1.8163\n",
      "Epoch 2 Batch 2010 Loss 2.0719\n",
      "Epoch 2 Batch 2011 Loss 1.7850\n",
      "Epoch 2 Batch 2012 Loss 1.9371\n",
      "Epoch 2 Batch 2013 Loss 1.7491\n",
      "Epoch 2 Batch 2014 Loss 1.9416\n",
      "Epoch 2 Batch 2015 Loss 1.6574\n",
      "Epoch 2 Batch 2016 Loss 1.8164\n",
      "Epoch 2 Batch 2017 Loss 1.9025\n",
      "Epoch 2 Batch 2018 Loss 1.8338\n",
      "Epoch 2 Batch 2019 Loss 2.0044\n",
      "Epoch 2 Batch 2020 Loss 1.9240\n",
      "Epoch 2 Batch 2021 Loss 1.7813\n",
      "Epoch 2 Batch 2022 Loss 1.8113\n",
      "Epoch 2 Batch 2023 Loss 1.7665\n",
      "Epoch 2 Batch 2024 Loss 2.2164\n",
      "Epoch 2 Batch 2025 Loss 1.5511\n",
      "Epoch 2 Batch 2026 Loss 1.2666\n",
      "Epoch 2 Batch 2027 Loss 1.2293\n",
      "Epoch 2 Batch 2028 Loss 1.6466\n",
      "Epoch 2 Batch 2029 Loss 1.3256\n",
      "Epoch 2 Batch 2030 Loss 1.7534\n",
      "Epoch 2 Batch 2031 Loss 2.0210\n",
      "Epoch 2 Batch 2032 Loss 1.9012\n",
      "Epoch 2 Batch 2033 Loss 1.7636\n",
      "Epoch 2 Batch 2034 Loss 1.7658\n",
      "Epoch 2 Batch 2035 Loss 1.5542\n",
      "Epoch 2 Batch 2036 Loss 1.8816\n",
      "Epoch 2 Batch 2037 Loss 2.1329\n",
      "Epoch 2 Batch 2038 Loss 1.6847\n",
      "Epoch 2 Batch 2039 Loss 1.7875\n",
      "Epoch 2 Batch 2040 Loss 1.7291\n",
      "Epoch 2 Batch 2041 Loss 1.6571\n",
      "Epoch 2 Batch 2042 Loss 1.4379\n",
      "Epoch 2 Batch 2043 Loss 1.8797\n",
      "Epoch 2 Batch 2044 Loss 1.9298\n",
      "Epoch 2 Batch 2045 Loss 2.1439\n",
      "Epoch 2 Batch 2046 Loss 2.0965\n",
      "Epoch 2 Batch 2047 Loss 1.9316\n",
      "Epoch 2 Batch 2048 Loss 1.5324\n",
      "Epoch 2 Batch 2049 Loss 1.2332\n",
      "Epoch 2 Batch 2050 Loss 1.6833\n",
      "Epoch 2 Batch 2051 Loss 1.8226\n",
      "Epoch 2 Batch 2052 Loss 1.8224\n",
      "Epoch 2 Batch 2053 Loss 1.3029\n",
      "Epoch 2 Batch 2054 Loss 1.6997\n",
      "Epoch 2 Batch 2055 Loss 1.5748\n",
      "Epoch 2 Batch 2056 Loss 1.7212\n",
      "Epoch 2 Batch 2057 Loss 2.2232\n",
      "Epoch 2 Batch 2058 Loss 2.0214\n",
      "Epoch 2 Batch 2059 Loss 1.6466\n",
      "Epoch 2 Batch 2060 Loss 1.7866\n",
      "Epoch 2 Batch 2061 Loss 2.0412\n",
      "Epoch 2 Batch 2062 Loss 2.1052\n",
      "Epoch 2 Batch 2063 Loss 2.1148\n",
      "Epoch 2 Batch 2064 Loss 2.0827\n",
      "Epoch 2 Batch 2065 Loss 2.1812\n",
      "Epoch 2 Batch 2066 Loss 1.7716\n",
      "Epoch 2 Batch 2067 Loss 1.8127\n",
      "Epoch 2 Batch 2068 Loss 2.0778\n",
      "Epoch 2 Batch 2069 Loss 1.9884\n",
      "Epoch 2 Batch 2070 Loss 1.8657\n",
      "Epoch 2 Batch 2071 Loss 1.9326\n",
      "Epoch 2 Batch 2072 Loss 1.6533\n",
      "Epoch 2 Batch 2073 Loss 1.9996\n",
      "Epoch 2 Batch 2074 Loss 2.2175\n",
      "Epoch 2 Batch 2075 Loss 1.9359\n",
      "Epoch 2 Batch 2076 Loss 1.7640\n",
      "Epoch 2 Batch 2077 Loss 1.5358\n",
      "Epoch 2 Batch 2078 Loss 1.9269\n",
      "Epoch 2 Batch 2079 Loss 2.6109\n",
      "Epoch 2 Batch 2080 Loss 1.7897\n",
      "Epoch 2 Batch 2081 Loss 1.9934\n",
      "Epoch 2 Batch 2082 Loss 1.8097\n",
      "Epoch 2 Batch 2083 Loss 1.7936\n",
      "Epoch 2 Batch 2084 Loss 1.7915\n",
      "Epoch 2 Batch 2085 Loss 1.4020\n",
      "Epoch 2 Batch 2086 Loss 1.6243\n",
      "Epoch 2 Batch 2087 Loss 1.7463\n",
      "Epoch 2 Batch 2088 Loss 1.6419\n",
      "Epoch 2 Batch 2089 Loss 1.8437\n",
      "Epoch 2 Batch 2090 Loss 1.7768\n",
      "Epoch 2 Batch 2091 Loss 1.7216\n",
      "Epoch 2 Batch 2092 Loss 1.8674\n",
      "Epoch 2 Batch 2093 Loss 2.2771\n",
      "Epoch 2 Batch 2094 Loss 2.2433\n",
      "Epoch 2 Batch 2095 Loss 1.8279\n",
      "Epoch 2 Batch 2096 Loss 1.9688\n",
      "Epoch 2 Batch 2097 Loss 1.9862\n",
      "Epoch 2 Batch 2098 Loss 2.1860\n",
      "Epoch 2 Batch 2099 Loss 2.2919\n",
      "Epoch 2 Batch 2100 Loss 1.7185\n",
      "Epoch 2 Batch 2101 Loss 1.9266\n",
      "Epoch 2 Batch 2102 Loss 1.8126\n",
      "Epoch 2 Batch 2103 Loss 2.0947\n",
      "Epoch 2 Batch 2104 Loss 1.8558\n",
      "Epoch 2 Batch 2105 Loss 2.0430\n",
      "Epoch 2 Batch 2106 Loss 2.3282\n",
      "Epoch 2 Batch 2107 Loss 2.1852\n",
      "Epoch 2 Batch 2108 Loss 2.1956\n",
      "Epoch 2 Batch 2109 Loss 1.7971\n",
      "Epoch 2 Batch 2110 Loss 1.8637\n",
      "Epoch 2 Batch 2111 Loss 1.5179\n",
      "Epoch 2 Batch 2112 Loss 1.7828\n",
      "Epoch 2 Batch 2113 Loss 1.3809\n",
      "Epoch 2 Batch 2114 Loss 1.2659\n",
      "Epoch 2 Batch 2115 Loss 1.5646\n",
      "Epoch 2 Batch 2116 Loss 2.0219\n",
      "Epoch 2 Batch 2117 Loss 1.7607\n",
      "Epoch 2 Batch 2118 Loss 1.4147\n",
      "Epoch 2 Batch 2119 Loss 1.6045\n",
      "Epoch 2 Batch 2120 Loss 1.8530\n",
      "Epoch 2 Batch 2121 Loss 1.6189\n",
      "Epoch 2 Batch 2122 Loss 2.2802\n",
      "Epoch 2 Batch 2123 Loss 1.9777\n",
      "Epoch 2 Batch 2124 Loss 1.9058\n",
      "Epoch 2 Batch 2125 Loss 1.8124\n",
      "Epoch 2 Batch 2126 Loss 2.6183\n",
      "Epoch 2 Batch 2127 Loss 2.8875\n",
      "Epoch 2 Batch 2128 Loss 2.2791\n",
      "Epoch 2 Batch 2129 Loss 2.6478\n",
      "Epoch 2 Batch 2130 Loss 1.9684\n",
      "Epoch 2 Batch 2131 Loss 1.7596\n",
      "Epoch 2 Batch 2132 Loss 2.2440\n",
      "Epoch 2 Batch 2133 Loss 1.8908\n",
      "Epoch 2 Batch 2134 Loss 1.8580\n",
      "Epoch 2 Batch 2135 Loss 1.5959\n",
      "Epoch 2 Batch 2136 Loss 1.7693\n",
      "Epoch 2 Batch 2137 Loss 1.4331\n",
      "Epoch 2 Batch 2138 Loss 1.8692\n",
      "Epoch 2 Batch 2139 Loss 2.2086\n",
      "Epoch 2 Batch 2140 Loss 2.1280\n",
      "Epoch 2 Batch 2141 Loss 2.5229\n",
      "Epoch 2 Batch 2142 Loss 2.0830\n",
      "Epoch 2 Batch 2143 Loss 2.0928\n",
      "Epoch 2 Batch 2144 Loss 2.2539\n",
      "Epoch 2 Batch 2145 Loss 2.0843\n",
      "Epoch 2 Batch 2146 Loss 1.8477\n",
      "Epoch 2 Batch 2147 Loss 1.6031\n",
      "Epoch 2 Batch 2148 Loss 1.5502\n",
      "Epoch 2 Batch 2149 Loss 2.1068\n",
      "Epoch 2 Batch 2150 Loss 2.1685\n",
      "Epoch 2 Batch 2151 Loss 2.3112\n",
      "Epoch 2 Batch 2152 Loss 2.2252\n",
      "Epoch 2 Batch 2153 Loss 2.1246\n",
      "Epoch 2 Batch 2154 Loss 2.3245\n",
      "Epoch 2 Batch 2155 Loss 2.3376\n",
      "Epoch 2 Batch 2156 Loss 2.6084\n",
      "Epoch 2 Batch 2157 Loss 2.2595\n",
      "Epoch 2 Batch 2158 Loss 2.0109\n",
      "Epoch 2 Batch 2159 Loss 2.0910\n",
      "Epoch 2 Batch 2160 Loss 1.9736\n",
      "Epoch 2 Batch 2161 Loss 1.9741\n",
      "Epoch 2 Batch 2162 Loss 1.6584\n",
      "Epoch 2 Batch 2163 Loss 1.7033\n",
      "Epoch 2 Batch 2164 Loss 2.0269\n",
      "Epoch 2 Batch 2165 Loss 1.8757\n",
      "Epoch 2 Batch 2166 Loss 2.1582\n",
      "Epoch 2 Batch 2167 Loss 1.6151\n",
      "Epoch 2 Batch 2168 Loss 1.3919\n",
      "Epoch 2 Batch 2169 Loss 1.7933\n",
      "Epoch 2 Batch 2170 Loss 2.1780\n",
      "Epoch 2 Batch 2171 Loss 1.7426\n",
      "Epoch 2 Batch 2172 Loss 1.5997\n",
      "Epoch 2 Batch 2173 Loss 1.3310\n",
      "Epoch 2 Batch 2174 Loss 1.6213\n",
      "Epoch 2 Batch 2175 Loss 1.6414\n",
      "Epoch 2 Batch 2176 Loss 2.1013\n",
      "Epoch 2 Batch 2177 Loss 1.6502\n",
      "Epoch 2 Batch 2178 Loss 1.8890\n",
      "Epoch 2 Batch 2179 Loss 2.3232\n",
      "Epoch 2 Batch 2180 Loss 1.6583\n",
      "Epoch 2 Batch 2181 Loss 2.0170\n",
      "Epoch 2 Batch 2182 Loss 2.2938\n",
      "Epoch 2 Batch 2183 Loss 1.9450\n",
      "Epoch 2 Batch 2184 Loss 1.7499\n",
      "Epoch 2 Batch 2185 Loss 1.6230\n",
      "Epoch 2 Batch 2186 Loss 1.9975\n",
      "Epoch 2 Batch 2187 Loss 1.4395\n",
      "Epoch 2 Batch 2188 Loss 1.9018\n",
      "Epoch 2 Batch 2189 Loss 1.9162\n",
      "Epoch 2 Batch 2190 Loss 1.7530\n",
      "Epoch 2 Batch 2191 Loss 2.2957\n",
      "Epoch 2 Batch 2192 Loss 1.8044\n",
      "Epoch 2 Batch 2193 Loss 1.6869\n",
      "Epoch 2 Batch 2194 Loss 1.5913\n",
      "Epoch 2 Batch 2195 Loss 1.5070\n",
      "Epoch 2 Batch 2196 Loss 1.3924\n",
      "Epoch 2 Batch 2197 Loss 1.4551\n",
      "Epoch 2 Batch 2198 Loss 1.5136\n",
      "Epoch 2 Batch 2199 Loss 1.7271\n",
      "Epoch 2 Batch 2200 Loss 1.8944\n",
      "Epoch 2 Batch 2201 Loss 1.7274\n",
      "Epoch 2 Batch 2202 Loss 1.5493\n",
      "Epoch 2 Batch 2203 Loss 1.7388\n",
      "Epoch 2 Batch 2204 Loss 1.9524\n",
      "Epoch 2 Batch 2205 Loss 1.7905\n",
      "Epoch 2 Batch 2206 Loss 1.5273\n",
      "Epoch 2 Batch 2207 Loss 1.7331\n",
      "Epoch 2 Batch 2208 Loss 2.0015\n",
      "Epoch 2 Batch 2209 Loss 2.0408\n",
      "Epoch 2 Batch 2210 Loss 1.9146\n",
      "Epoch 2 Batch 2211 Loss 2.1102\n",
      "Epoch 2 Batch 2212 Loss 2.1704\n",
      "Epoch 2 Batch 2213 Loss 1.8681\n",
      "Epoch 2 Batch 2214 Loss 2.1725\n",
      "Epoch 2 Batch 2215 Loss 1.7083\n",
      "Epoch 2 Batch 2216 Loss 1.8710\n",
      "Epoch 2 Batch 2217 Loss 1.8893\n",
      "Epoch 2 Batch 2218 Loss 1.6136\n",
      "Epoch 2 Batch 2219 Loss 2.2962\n",
      "Epoch 2 Batch 2220 Loss 1.9704\n",
      "Epoch 2 Batch 2221 Loss 1.4432\n",
      "Epoch 2 Batch 2222 Loss 1.2250\n",
      "Epoch 2 Batch 2223 Loss 1.5028\n",
      "Epoch 2 Batch 2224 Loss 1.7066\n",
      "Epoch 2 Batch 2225 Loss 1.6206\n",
      "Epoch 2 Batch 2226 Loss 1.5841\n",
      "Epoch 2 Batch 2227 Loss 1.9054\n",
      "Epoch 2 Batch 2228 Loss 2.1795\n",
      "Epoch 2 Batch 2229 Loss 2.1654\n",
      "Epoch 2 Batch 2230 Loss 1.3275\n",
      "Epoch 2 Batch 2231 Loss 1.7458\n",
      "Epoch 2 Batch 2232 Loss 1.4019\n",
      "Epoch 2 Batch 2233 Loss 1.4250\n",
      "Epoch 2 Batch 2234 Loss 1.9676\n",
      "Epoch 2 Batch 2235 Loss 1.7392\n",
      "Epoch 2 Batch 2236 Loss 1.7707\n",
      "Epoch 2 Batch 2237 Loss 2.2443\n",
      "Epoch 2 Batch 2238 Loss 2.2345\n",
      "Epoch 2 Batch 2239 Loss 2.2113\n",
      "Epoch 2 Batch 2240 Loss 1.7293\n",
      "Epoch 2 Batch 2241 Loss 1.5397\n",
      "Epoch 2 Batch 2242 Loss 1.6742\n",
      "Epoch 2 Batch 2243 Loss 1.4976\n",
      "Epoch 2 Batch 2244 Loss 1.4224\n",
      "Epoch 2 Batch 2245 Loss 1.5455\n",
      "Epoch 2 Batch 2246 Loss 1.3367\n",
      "Epoch 2 Batch 2247 Loss 1.7596\n",
      "Epoch 2 Batch 2248 Loss 1.9406\n",
      "Epoch 2 Batch 2249 Loss 1.2918\n",
      "Epoch 2 Batch 2250 Loss 1.1912\n",
      "Epoch 2 Batch 2251 Loss 1.6615\n",
      "Epoch 2 Batch 2252 Loss 1.8717\n",
      "Epoch 2 Batch 2253 Loss 1.8222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Batch 2254 Loss 1.4061\n",
      "Epoch 2 Batch 2255 Loss 1.6347\n",
      "Epoch 2 Batch 2256 Loss 2.0026\n",
      "Epoch 2 Batch 2257 Loss 1.7123\n",
      "Epoch 2 Batch 2258 Loss 1.4174\n",
      "Epoch 2 Batch 2259 Loss 1.9384\n",
      "Epoch 2 Batch 2260 Loss 1.5503\n",
      "Epoch 2 Batch 2261 Loss 2.2204\n",
      "Epoch 2 Batch 2262 Loss 1.7548\n",
      "Epoch 2 Batch 2263 Loss 1.4974\n",
      "Epoch 2 Batch 2264 Loss 1.4252\n",
      "Epoch 2 Batch 2265 Loss 1.2850\n",
      "Epoch 2 Batch 2266 Loss 1.6368\n",
      "Epoch 2 Batch 2267 Loss 1.8969\n",
      "Epoch 2 Batch 2268 Loss 1.2814\n",
      "Epoch 2 Batch 2269 Loss 1.5869\n",
      "Epoch 2 Batch 2270 Loss 1.6960\n",
      "Epoch 2 Batch 2271 Loss 1.5073\n",
      "Epoch 2 Batch 2272 Loss 1.6698\n",
      "Epoch 2 Batch 2273 Loss 1.6312\n",
      "Epoch 2 Batch 2274 Loss 1.7010\n",
      "Epoch 2 Batch 2275 Loss 2.1595\n",
      "Epoch 2 Batch 2276 Loss 1.7931\n",
      "Epoch 2 Batch 2277 Loss 1.7384\n",
      "Epoch 2 Batch 2278 Loss 1.7631\n",
      "Epoch 2 Batch 2279 Loss 1.8391\n",
      "Epoch 2 Batch 2280 Loss 1.6090\n",
      "Epoch 2 Batch 2281 Loss 1.4549\n",
      "Epoch 2 Batch 2282 Loss 1.5541\n",
      "Epoch 2 Batch 2283 Loss 1.2525\n",
      "Epoch 2 Batch 2284 Loss 1.4868\n",
      "Epoch 2 Batch 2285 Loss 1.2992\n",
      "Epoch 2 Batch 2286 Loss 1.3709\n",
      "Epoch 2 Batch 2287 Loss 1.5774\n",
      "Epoch 2 Batch 2288 Loss 1.5028\n",
      "Epoch 2 Batch 2289 Loss 1.7607\n",
      "Epoch 2 Batch 2290 Loss 1.5133\n",
      "Epoch 2 Batch 2291 Loss 1.8296\n",
      "Epoch 2 Batch 2292 Loss 1.9298\n",
      "Epoch 2 Batch 2293 Loss 1.6699\n",
      "Epoch 2 Batch 2294 Loss 1.4251\n",
      "Epoch 2 Batch 2295 Loss 2.4723\n",
      "Epoch 2 Batch 2296 Loss 2.1022\n",
      "Epoch 2 Batch 2297 Loss 1.5668\n",
      "Epoch 2 Batch 2298 Loss 1.5863\n",
      "Epoch 2 Batch 2299 Loss 1.2528\n",
      "Epoch 2 Batch 2300 Loss 2.0711\n",
      "Epoch 2 Batch 2301 Loss 1.7837\n",
      "Epoch 2 Batch 2302 Loss 1.4142\n",
      "Epoch 2 Batch 2303 Loss 1.5664\n",
      "Epoch 2 Batch 2304 Loss 1.4642\n",
      "Epoch 2 Batch 2305 Loss 1.8761\n",
      "Epoch 2 Batch 2306 Loss 2.1671\n",
      "Epoch 2 Batch 2307 Loss 1.5329\n",
      "Epoch 2 Batch 2308 Loss 1.5185\n",
      "Epoch 2 Batch 2309 Loss 1.4772\n",
      "Epoch 2 Batch 2310 Loss 1.8090\n",
      "Epoch 2 Batch 2311 Loss 2.0850\n",
      "Epoch 2 Batch 2312 Loss 2.0072\n",
      "Epoch 2 Batch 2313 Loss 1.7801\n",
      "Epoch 2 Batch 2314 Loss 1.6221\n",
      "Epoch 2 Batch 2315 Loss 2.0453\n",
      "Epoch 2 Batch 2316 Loss 1.9135\n",
      "Epoch 2 Batch 2317 Loss 2.0590\n",
      "Epoch 2 Batch 2318 Loss 1.9757\n",
      "Epoch 2 Batch 2319 Loss 1.7692\n",
      "Epoch 2 Batch 2320 Loss 1.7314\n",
      "Epoch 2 Batch 2321 Loss 1.4056\n",
      "Epoch 2 Batch 2322 Loss 1.8457\n",
      "Epoch 2 Batch 2323 Loss 1.6948\n",
      "Epoch 2 Batch 2324 Loss 1.5506\n",
      "Epoch 2 Batch 2325 Loss 1.7997\n",
      "Epoch 2 Batch 2326 Loss 1.5831\n",
      "Epoch 2 Batch 2327 Loss 1.3776\n",
      "Epoch 2 Batch 2328 Loss 1.3884\n",
      "Epoch 2 Batch 2329 Loss 1.3749\n",
      "Epoch 2 Batch 2330 Loss 1.4296\n",
      "Epoch 2 Batch 2331 Loss 1.1585\n",
      "Epoch 2 Batch 2332 Loss 1.3210\n",
      "Epoch 2 Batch 2333 Loss 1.1716\n",
      "Epoch 2 Batch 2334 Loss 1.3532\n",
      "Epoch 2 Batch 2335 Loss 1.6580\n",
      "Epoch 2 Batch 2336 Loss 1.6320\n",
      "Epoch 2 Batch 2337 Loss 1.4432\n",
      "Epoch 2 Batch 2338 Loss 1.4729\n",
      "Epoch 2 Batch 2339 Loss 1.6480\n",
      "Epoch 2 Batch 2340 Loss 1.6469\n",
      "Epoch 2 Batch 2341 Loss 1.3855\n",
      "Epoch 2 Batch 2342 Loss 1.7560\n",
      "Epoch 2 Batch 2343 Loss 1.5120\n",
      "Epoch 2 Batch 2344 Loss 1.6027\n",
      "Epoch 2 Batch 2345 Loss 1.4531\n",
      "Epoch 2 Batch 2346 Loss 2.1124\n",
      "Epoch 2 Batch 2347 Loss 1.9568\n",
      "Epoch 2 Batch 2348 Loss 1.3207\n",
      "Epoch 2 Batch 2349 Loss 1.5188\n",
      "Epoch 2 Batch 2350 Loss 1.8290\n",
      "Epoch 2 Batch 2351 Loss 1.7415\n",
      "Epoch 2 Batch 2352 Loss 1.8356\n",
      "Epoch 2 Batch 2353 Loss 1.3865\n",
      "Epoch 2 Batch 2354 Loss 1.5727\n",
      "Epoch 2 Batch 2355 Loss 1.4669\n",
      "Epoch 2 Batch 2356 Loss 1.3686\n",
      "Epoch 2 Batch 2357 Loss 1.5154\n",
      "Epoch 2 Batch 2358 Loss 1.2677\n",
      "Epoch 2 Batch 2359 Loss 1.3087\n",
      "Epoch 2 Batch 2360 Loss 1.3493\n",
      "Epoch 2 Batch 2361 Loss 1.5110\n",
      "Epoch 2 Batch 2362 Loss 1.4292\n",
      "Epoch 2 Batch 2363 Loss 1.8060\n",
      "Epoch 2 Batch 2364 Loss 1.6667\n",
      "Epoch 2 Batch 2365 Loss 1.5647\n",
      "Epoch 2 Batch 2366 Loss 1.5776\n",
      "Epoch 2 Batch 2367 Loss 1.3563\n",
      "Epoch 2 Batch 2368 Loss 1.4151\n",
      "Epoch 2 Batch 2369 Loss 1.6482\n",
      "Epoch 2 Batch 2370 Loss 1.4739\n",
      "Epoch 2 Batch 2371 Loss 1.4560\n",
      "Epoch 2 Batch 2372 Loss 1.3323\n",
      "Epoch 2 Batch 2373 Loss 1.4561\n",
      "Epoch 2 Batch 2374 Loss 1.4000\n",
      "Epoch 2 Batch 2375 Loss 1.5755\n",
      "Epoch 2 Batch 2376 Loss 1.1870\n",
      "Epoch 2 Batch 2377 Loss 1.1934\n",
      "Epoch 2 Batch 2378 Loss 1.6264\n",
      "Epoch 2 Batch 2379 Loss 1.7090\n",
      "Epoch 2 Batch 2380 Loss 1.6743\n",
      "Epoch 2 Batch 2381 Loss 1.5621\n",
      "Epoch 2 Batch 2382 Loss 1.5050\n",
      "Epoch 2 Batch 2383 Loss 2.0320\n",
      "Epoch 2 Batch 2384 Loss 1.8687\n",
      "Epoch 2 Batch 2385 Loss 1.6988\n",
      "Epoch 2 Batch 2386 Loss 1.5960\n",
      "Epoch 2 Batch 2387 Loss 1.9084\n",
      "Epoch 2 Batch 2388 Loss 1.8375\n",
      "Epoch 2 Batch 2389 Loss 1.7693\n",
      "Epoch 2 Batch 2390 Loss 1.9413\n",
      "Epoch 2 Batch 2391 Loss 1.7414\n",
      "Epoch 2 Batch 2392 Loss 1.6847\n",
      "Epoch 2 Batch 2393 Loss 1.3380\n",
      "Epoch 2 Batch 2394 Loss 1.2083\n",
      "Epoch 2 Batch 2395 Loss 1.5958\n",
      "Epoch 2 Batch 2396 Loss 1.9487\n",
      "Epoch 2 Batch 2397 Loss 1.3489\n",
      "Epoch 2 Batch 2398 Loss 1.5283\n",
      "Epoch 2 Batch 2399 Loss 1.1432\n",
      "Epoch 2 Batch 2400 Loss 1.2320\n",
      "Epoch 2 Batch 2401 Loss 1.5858\n",
      "Epoch 2 Batch 2402 Loss 1.6484\n",
      "Epoch 2 Batch 2403 Loss 1.3823\n",
      "Epoch 2 Batch 2404 Loss 1.9012\n",
      "Epoch 2 Batch 2405 Loss 1.5553\n",
      "Epoch 2 Batch 2406 Loss 1.6630\n",
      "Epoch 2 Batch 2407 Loss 1.9431\n",
      "Epoch 2 Batch 2408 Loss 1.8892\n",
      "Epoch 2 Batch 2409 Loss 2.1271\n",
      "Epoch 2 Batch 2410 Loss 1.5055\n",
      "Epoch 2 Batch 2411 Loss 1.6863\n",
      "Epoch 2 Batch 2412 Loss 1.4091\n",
      "Epoch 2 Batch 2413 Loss 1.4445\n",
      "Epoch 2 Batch 2414 Loss 1.2734\n",
      "Epoch 2 Batch 2415 Loss 1.6095\n",
      "Epoch 2 Batch 2416 Loss 2.0094\n",
      "Epoch 2 Batch 2417 Loss 1.4647\n",
      "Epoch 2 Batch 2418 Loss 1.2210\n",
      "Epoch 2 Batch 2419 Loss 1.1489\n",
      "Epoch 2 Batch 2420 Loss 1.2279\n",
      "Epoch 2 Batch 2421 Loss 1.2353\n",
      "Epoch 2 Batch 2422 Loss 1.2375\n",
      "Epoch 2 Batch 2423 Loss 1.2491\n",
      "Epoch 2 Batch 2424 Loss 1.4004\n",
      "Epoch 2 Batch 2425 Loss 1.6509\n",
      "Epoch 2 Batch 2426 Loss 2.0532\n",
      "Epoch 2 Batch 2427 Loss 1.5844\n",
      "Epoch 2 Batch 2428 Loss 1.7385\n",
      "Epoch 2 Batch 2429 Loss 1.3749\n",
      "Epoch 2 Batch 2430 Loss 1.2643\n",
      "Epoch 2 Batch 2431 Loss 1.7197\n",
      "Epoch 2 Batch 2432 Loss 1.2793\n",
      "Epoch 2 Batch 2433 Loss 1.3135\n",
      "Epoch 2 Batch 2434 Loss 1.4332\n",
      "Epoch 2 Batch 2435 Loss 1.4435\n",
      "Epoch 2 Batch 2436 Loss 1.4874\n",
      "Epoch 2 Batch 2437 Loss 1.3387\n",
      "Epoch 2 Batch 2438 Loss 1.3875\n",
      "Epoch 2 Batch 2439 Loss 1.4029\n",
      "Epoch 2 Batch 2440 Loss 1.4466\n",
      "Epoch 2 Batch 2441 Loss 1.5051\n",
      "Epoch 2 Batch 2442 Loss 1.5031\n",
      "Epoch 2 Batch 2443 Loss 1.2300\n",
      "Epoch 2 Batch 2444 Loss 1.4534\n",
      "Epoch 2 Batch 2445 Loss 1.3361\n",
      "Epoch 2 Batch 2446 Loss 1.2660\n",
      "Epoch 2 Batch 2447 Loss 1.5653\n",
      "Epoch 2 Batch 2448 Loss 1.3121\n",
      "Epoch 2 Batch 2449 Loss 1.3241\n",
      "Epoch 2 Batch 2450 Loss 1.7387\n",
      "Epoch 2 Batch 2451 Loss 1.9361\n",
      "Epoch 2 Batch 2452 Loss 1.7635\n",
      "Epoch 2 Batch 2453 Loss 2.0924\n",
      "Epoch 2 Batch 2454 Loss 1.7333\n",
      "Epoch 2 Batch 2455 Loss 1.6970\n",
      "Epoch 2 Batch 2456 Loss 1.6928\n",
      "Epoch 2 Batch 2457 Loss 2.1017\n",
      "Epoch 2 Batch 2458 Loss 1.6859\n",
      "Epoch 2 Batch 2459 Loss 1.7396\n",
      "Epoch 2 Batch 2460 Loss 1.8092\n",
      "Epoch 2 Batch 2461 Loss 1.3528\n",
      "Epoch 2 Batch 2462 Loss 1.6438\n",
      "Epoch 2 Batch 2463 Loss 1.5852\n",
      "Epoch 2 Batch 2464 Loss 1.2995\n",
      "Epoch 2 Batch 2465 Loss 1.2431\n",
      "Epoch 2 Batch 2466 Loss 1.5500\n",
      "Epoch 2 Batch 2467 Loss 1.1494\n",
      "Epoch 2 Batch 2468 Loss 1.5557\n",
      "Epoch 2 Batch 2469 Loss 1.6097\n",
      "Epoch 2 Batch 2470 Loss 1.6381\n",
      "Epoch 2 Batch 2471 Loss 2.1499\n",
      "Epoch 2 Batch 2472 Loss 1.8971\n",
      "Epoch 2 Batch 2473 Loss 1.8680\n",
      "Epoch 2 Batch 2474 Loss 1.5860\n",
      "Epoch 2 Batch 2475 Loss 1.4741\n",
      "Epoch 2 Batch 2476 Loss 1.1628\n",
      "Epoch 2 Batch 2477 Loss 1.2498\n",
      "Epoch 2 Batch 2478 Loss 1.1080\n",
      "Epoch 2 Batch 2479 Loss 1.0963\n",
      "Epoch 2 Batch 2480 Loss 1.2653\n",
      "Epoch 2 Batch 2481 Loss 1.2190\n",
      "Epoch 2 Batch 2482 Loss 1.7810\n",
      "Epoch 2 Batch 2483 Loss 1.5478\n",
      "Epoch 2 Batch 2484 Loss 1.4041\n",
      "Epoch 2 Batch 2485 Loss 1.6953\n",
      "Epoch 2 Batch 2486 Loss 1.5206\n",
      "Epoch 2 Batch 2487 Loss 1.4266\n",
      "Epoch 2 Batch 2488 Loss 1.7289\n",
      "Epoch 2 Batch 2489 Loss 1.4528\n",
      "Epoch 2 Batch 2490 Loss 1.6655\n",
      "Epoch 2 Batch 2491 Loss 1.5679\n",
      "Epoch 2 Batch 2492 Loss 2.1963\n",
      "Epoch 2 Batch 2493 Loss 1.8410\n",
      "Epoch 2 Batch 2494 Loss 1.4872\n",
      "Epoch 2 Batch 2495 Loss 1.3533\n",
      "Epoch 2 Batch 2496 Loss 1.6859\n",
      "Epoch 2 Batch 2497 Loss 1.9805\n",
      "Epoch 2 Batch 2498 Loss 1.4721\n",
      "Epoch 2 Batch 2499 Loss 1.9196\n",
      "Epoch 2 Batch 2500 Loss 1.2823\n",
      "Epoch 2 Batch 2501 Loss 1.3776\n",
      "Epoch 2 Batch 2502 Loss 1.3859\n",
      "Epoch 2 Batch 2503 Loss 0.9938\n",
      "Epoch 2 Batch 2504 Loss 1.5318\n",
      "Epoch 2 Batch 2505 Loss 1.1424\n",
      "Epoch 2 Batch 2506 Loss 1.7976\n",
      "Epoch 2 Batch 2507 Loss 1.6764\n",
      "Epoch 2 Batch 2508 Loss 1.6923\n",
      "Epoch 2 Batch 2509 Loss 1.3956\n",
      "Epoch 2 Batch 2510 Loss 1.3573\n",
      "Epoch 2 Batch 2511 Loss 1.4505\n",
      "Epoch 2 Batch 2512 Loss 1.7204\n",
      "Epoch 2 Batch 2513 Loss 1.8538\n",
      "Epoch 2 Batch 2514 Loss 1.8219\n",
      "Epoch 2 Batch 2515 Loss 1.4758\n",
      "Epoch 2 Batch 2516 Loss 1.5609\n",
      "Epoch 2 Batch 2517 Loss 1.2395\n",
      "Epoch 2 Batch 2518 Loss 1.4484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Batch 2519 Loss 1.7397\n",
      "Epoch 2 Batch 2520 Loss 1.6997\n",
      "Epoch 2 Batch 2521 Loss 1.6617\n",
      "Epoch 2 Batch 2522 Loss 1.6959\n",
      "Epoch 2 Batch 2523 Loss 1.8311\n",
      "Epoch 2 Batch 2524 Loss 1.9121\n",
      "Epoch 2 Batch 2525 Loss 1.9057\n",
      "Epoch 2 Batch 2526 Loss 1.5911\n",
      "Epoch 2 Batch 2527 Loss 2.1734\n",
      "Epoch 2 Batch 2528 Loss 1.8195\n",
      "Epoch 2 Batch 2529 Loss 2.0398\n",
      "Epoch 2 Batch 2530 Loss 1.9383\n",
      "Epoch 2 Batch 2531 Loss 1.6489\n",
      "Epoch 2 Batch 2532 Loss 1.3723\n",
      "Epoch 2 Batch 2533 Loss 1.5133\n",
      "Epoch 2 Batch 2534 Loss 1.6022\n",
      "Epoch 2 Batch 2535 Loss 1.6528\n",
      "Epoch 2 Batch 2536 Loss 1.6114\n",
      "Epoch 2 Batch 2537 Loss 1.5408\n",
      "Epoch 2 Batch 2538 Loss 1.3055\n",
      "Epoch 2 Batch 2539 Loss 1.5252\n",
      "Epoch 2 Batch 2540 Loss 1.3568\n",
      "Epoch 2 Batch 2541 Loss 1.1993\n",
      "Epoch 2 Batch 2542 Loss 1.3111\n",
      "Epoch 2 Batch 2543 Loss 1.5026\n",
      "Epoch 2 Batch 2544 Loss 1.4508\n",
      "Epoch 2 Batch 2545 Loss 1.4789\n",
      "Epoch 2 Batch 2546 Loss 1.3374\n",
      "Epoch 2 Batch 2547 Loss 1.3933\n",
      "Epoch 2 Batch 2548 Loss 1.7687\n",
      "Epoch 2 Batch 2549 Loss 2.0118\n",
      "Epoch 2 Batch 2550 Loss 2.2131\n",
      "Epoch 2 Batch 2551 Loss 1.6092\n",
      "Epoch 2 Batch 2552 Loss 1.3417\n",
      "Epoch 2 Batch 2553 Loss 1.7633\n",
      "Epoch 2 Batch 2554 Loss 1.9268\n",
      "Epoch 2 Batch 2555 Loss 1.8053\n",
      "Epoch 2 Batch 2556 Loss 1.6595\n",
      "Epoch 2 Batch 2557 Loss 1.5809\n",
      "Epoch 2 Batch 2558 Loss 1.7311\n",
      "Epoch 2 Batch 2559 Loss 1.3010\n",
      "Epoch 2 Batch 2560 Loss 1.2991\n",
      "Epoch 2 Batch 2561 Loss 1.3447\n",
      "Epoch 2 Batch 2562 Loss 1.2815\n",
      "Epoch 2 Batch 2563 Loss 1.2204\n",
      "Epoch 2 Batch 2564 Loss 1.3519\n",
      "Epoch 2 Batch 2565 Loss 1.0704\n",
      "Epoch 2 Batch 2566 Loss 1.3831\n",
      "Epoch 2 Batch 2567 Loss 1.3405\n",
      "Epoch 2 Batch 2568 Loss 1.4966\n",
      "Epoch 2 Batch 2569 Loss 1.5440\n",
      "Epoch 2 Batch 2570 Loss 1.7755\n",
      "Epoch 2 Batch 2571 Loss 1.6716\n",
      "Epoch 2 Batch 2572 Loss 1.3294\n",
      "Epoch 2 Batch 2573 Loss 1.9667\n",
      "Epoch 2 Batch 2574 Loss 1.8659\n",
      "Epoch 2 Batch 2575 Loss 1.4529\n",
      "Epoch 2 Batch 2576 Loss 1.5871\n",
      "Epoch 2 Batch 2577 Loss 1.5107\n",
      "Epoch 2 Batch 2578 Loss 1.3503\n",
      "Epoch 2 Batch 2579 Loss 1.1789\n",
      "Epoch 2 Batch 2580 Loss 1.2335\n",
      "Epoch 2 Batch 2581 Loss 1.7252\n",
      "Epoch 2 Batch 2582 Loss 1.7346\n",
      "Epoch 2 Batch 2583 Loss 1.9394\n",
      "Epoch 2 Batch 2584 Loss 1.5222\n",
      "Epoch 2 Batch 2585 Loss 1.9456\n",
      "Epoch 2 Batch 2586 Loss 1.5895\n",
      "Epoch 2 Batch 2587 Loss 1.4538\n",
      "Epoch 2 Batch 2588 Loss 1.3192\n",
      "Epoch 2 Batch 2589 Loss 1.3147\n",
      "Epoch 2 Batch 2590 Loss 1.3806\n",
      "Saving checkpoint for epoch 2 at E:\\GitHub\\QA-abstract-and-reasoning\\data\\checkpoints\\training_checkpoints\\ckpt-1\n",
      "Epoch 2 Loss 1.7500\n",
      "Time taken for 1 epoch 461.44007658958435 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = params[\"epochs\"]\n",
    "# 如果检查点存在，则恢复最新的检查点。\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print ('Latest checkpoint restored!!')\n",
    "    \n",
    "for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch+1)):\n",
    "        batch_loss = train_step(inp, targ)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        if batch % 1 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                         batch,\n",
    "                                                         batch_loss.numpy()))\n",
    "    # saving (checkpoint) the model every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        ckpt_save_path = ckpt_manager.save()\n",
    "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
    "                                                             ckpt_save_path))\n",
    "\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 载入模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of preprocess failed: Traceback (most recent call last):\n",
      "  File \"E:\\software\\Anaconda3\\envs\\tf2.0\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"E:\\software\\Anaconda3\\envs\\tf2.0\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 450, in superreload\n",
      "    update_generic(old_obj, new_obj)\n",
      "  File \"E:\\software\\Anaconda3\\envs\\tf2.0\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 387, in update_generic\n",
      "    update(a, b)\n",
      "  File \"E:\\software\\Anaconda3\\envs\\tf2.0\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 357, in update_class\n",
      "    update_instances(old, new)\n",
      "  File \"E:\\software\\Anaconda3\\envs\\tf2.0\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 312, in update_instances\n",
      "    update_instances(old, new, obj.__dict__, visited)\n",
      "  File \"E:\\software\\Anaconda3\\envs\\tf2.0\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 317, in update_instances\n",
      "    update_instances(old, new, obj, visited)\n",
      "  File \"E:\\software\\Anaconda3\\envs\\tf2.0\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 317, in update_instances\n",
      "    update_instances(old, new, obj, visited)\n",
      "  File \"E:\\software\\Anaconda3\\envs\\tf2.0\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 302, in update_instances\n",
      "    visited.update({id(obj):obj})\n",
      "MemoryError\n",
      "]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model restored\n"
     ]
    }
   ],
   "source": [
    "# 如果检查点存在，则恢复最新的检查点。\n",
    "ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "print(\"Model restored\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model,inputs):\n",
    "    attention_plot = np.zeros((params[\"max_dec_len\"], params[\"max_enc_len\"]))\n",
    "    \n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "    result = ''\n",
    "    \n",
    "    hidden = [tf.zeros((1, params[\"enc_units\"]))]\n",
    "    enc_output, enc_hidden = model.encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    \n",
    "    dec_input = tf.expand_dims([vocab['<START>']], 0)\n",
    "    \n",
    "    context_vector, _ = model.attention(dec_hidden, enc_output)\n",
    "\n",
    "    for t in range(params[\"max_dec_len\"]):\n",
    "        \n",
    "        context_vector, attention_weights = model.attention(dec_hidden, enc_output)\n",
    "        \n",
    "        predictions, dec_hidden = model.decoder(dec_input,\n",
    "                                         dec_hidden,\n",
    "                                         enc_output,\n",
    "                                         context_vector)\n",
    "\n",
    "        # storing the attention weights to plot later on\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        \n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += vocab_reversed[predicted_id] + ' '\n",
    "        if vocab_reversed[predicted_id] == '<STOP>':\n",
    "            return result, attention_plot\n",
    "\n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    st = preproc.sentence_proc(sentence)\n",
    "    sentence = preproc.sentence_proc_eval(sentence,params[\"max_enc_len\"]-2,vocab)\n",
    "    result, attention_plot = evaluate(model,sentence)\n",
    "\n",
    "    print('Input: %s' % (st))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "\n",
    "    attention_plot = attention_plot[:len(result.split(' ')), :len(st.split(' '))]\n",
    "    plot_attention(attention_plot, st.split(' '), result.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\Light\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.757 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'方向机 重 助力 泵 方向机 都 换'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = '方向机重，助力泵，方向机都换了还是一样'\n",
    "preproc = Preprocess()\n",
    "preproc.sentence_proc(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib as mpl\n",
    "# mpl.rcParams['font.family'] = 'STSong'  # 显示中文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 方向机 重 助力 泵 方向机 都 换\n",
      "Predicted translation: 方向机 助力 泵 方向机 助力 泵 方向机 助力 泵 方向机 助力 泵 方向机 助力 泵 方向机 助力 泵 方向机 助力 泵 方向机 助力 泵 方向机 助力 泵 方向机 助力 泵 方向机 助力 泵 \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1440 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "translate(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-cff7cc3d7d7b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 下半部分\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32massert\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 下半部分\n",
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_predict(inps):\n",
    "    # 判断输入长度\n",
    "    batch_size=len(inps)\n",
    "    # 开辟结果存储list\n",
    "    preidicts=[''] * batch_size\n",
    "    \n",
    "    inps = tf.convert_to_tensor(inps)\n",
    "    # 0. 初始化隐藏层输入\n",
    "    hidden = [tf.zeros((batch_size, params[\"enc_units\"]))]\n",
    "    # 1. 构建encoder\n",
    "    enc_output, enc_hidden = model.encoder(inps, hidden)\n",
    "    # 2. 复制\n",
    "    dec_hidden = enc_hidden\n",
    "    # 3. <START> * BATCH_SIZE \n",
    "    dec_input = tf.expand_dims([vocab['<START>']] * batch_size, 1)\n",
    "    \n",
    "    context_vector, _ = model.attention(dec_hidden, enc_output)\n",
    "    # Teacher forcing - feeding the target as the next input\n",
    "    for t in range(params[\"max_dec_len\"]):\n",
    "        # 计算上下文\n",
    "        context_vector, attention_weights = model.attention(dec_hidden, enc_output)\n",
    "        # 单步预测\n",
    "        predictions, dec_hidden = model.decoder(dec_input,\n",
    "                                         dec_hidden,\n",
    "                                         enc_output,\n",
    "                                         context_vector)\n",
    "        \n",
    "        # id转换 贪婪搜索\n",
    "        predicted_ids = tf.argmax(predictions,axis=1).numpy()\n",
    "        \n",
    "        \n",
    "        for index,predicted_id in enumerate(predicted_ids):\n",
    "            preidicts[index]+= vocab_reversed[predicted_id] + ' '\n",
    "        \n",
    "        # using teacher forcing\n",
    "        dec_input = tf.expand_dims(predicted_ids, 1)\n",
    "\n",
    "    results=[]\n",
    "    for preidict in preidicts:\n",
    "        # 去掉句子前后空格\n",
    "        preidict=preidict.strip()\n",
    "        # 句子小于max len就结束了 截断\n",
    "        if '<STOP>' in preidict:\n",
    "            # 截断stop\n",
    "            preidict=preidict[:preidict.index('<STOP>')]\n",
    "        # 保存结果\n",
    "        results.append(preidict)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试代码\n",
    "# ds = iter(dataset)\n",
    "# x,y = ds.next()\n",
    "# batch_predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict(data_X, batch_size):\n",
    "    # 存储结果\n",
    "    results=[]\n",
    "    # 样本数量\n",
    "    sample_size=len(data_X)\n",
    "    # batch 操作轮数 math.ceil向上取整 小数 +1\n",
    "    # 因为最后一个batch可能不足一个batch size 大小 ,但是依然需要计算  \n",
    "    steps_epoch = math.ceil(sample_size/batch_size)\n",
    "    # [0,steps_epoch)\n",
    "    for i in tqdm(range(steps_epoch)):\n",
    "        batch_data = data_X[i*batch_size:(i+1)*batch_size]\n",
    "        results+=batch_predict(batch_data)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|██▌                                                                         | 689/20736 [04:13<2:14:52,  2.48it/s]"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results=model_predict(train_x[:sample_num+1],batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf2.0]",
   "language": "python",
   "name": "conda-env-tf2.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
