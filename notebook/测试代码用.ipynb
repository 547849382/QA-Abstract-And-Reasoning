{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\software\\Anaconda3\\envs\\tf2.0\\lib\\site-packages\\ipykernel\\parentpoller.py:116: UserWarning: Parent poll failed.  If the frontend dies,\n",
      "                the kernel may be left running.  Please let us know\n",
      "                about your system (bitness, Python, etc.) at\n",
      "                ipython-dev@scipy.org\n",
      "  ipython-dev@scipy.org\"\"\")\n"
     ]
    }
   ],
   "source": [
    "# 自动加载修改过的文件\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "sys.path.append('../')  # |返回notebook的上一级目录\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "from utils.config_gpu import config_gpu\n",
    "config_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.config import *\n",
    "from utils.saveLoader import load_text, get_text, load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_seg, test_seg = load_dataset(TRAIN_SEG, TEST_SEG)\n",
    "# proc_text = load_text(PROC_TEXT)\n",
    "# raw_text = load_text(RAW_TEXT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1.损失函数的改进**\n",
    "- 原本return tf.reduce_mean(loss_)改成tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "- 改成了coverage loss\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.config import *\n",
    "from utils.saveLoader import load_train_dataset, load_vocab\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 载入数据\n",
    "train_x,train_y,test_x = load_train_dataset()  # 数据集\n",
    "vocab,vocab_reversed = load_vocab(VOCAB_PAD)  # vocab\n",
    "embedding_matrix = np.loadtxt(EMBEDDING_MATRIX_PAD)  # 预训练层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.1原损失函数**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(name='Adam',learning_rate=0.001)\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "pad_index=vocab['<PAD>']\n",
    "unk_index=vocab['<UNK>']\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    pad_mask = tf.math.equal(real, pad_index)\n",
    "    unk_mask = tf.math.equal(real, unk_index)\n",
    "    # <PAD> 和 <UNK> 的损失都不算\n",
    "    mask = tf.math.logical_not(tf.math.logical_or(pad_mask,unk_mask))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    #return tf.reduce_mean(loss_)\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.2模型测试**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'vocab_size': 32247, 'max_enc_len': 460, 'max_dec_len': 52, 'embed_size': 300, 'enc_units': 256, 'attn_units': 10, 'dec_units': 256, 'batch_size': 32, 'epochs': 2}\n"
     ]
    }
   ],
   "source": [
    "params = {}\n",
    "params[\"vocab_size\"] = len(vocab)\n",
    "params[\"max_enc_len\"] = train_x.shape[1]  # 260\n",
    "params[\"max_dec_len\"] = train_y.shape[1]  # 33\n",
    "params[\"embed_size\"] = embedding_matrix.shape[1]\n",
    "params[\"enc_units\"] = 256\n",
    "params[\"attn_units\"] = 10\n",
    "params[\"dec_units\"] = params[\"enc_units\"]\n",
    "params[\"batch_size\"] = 32\n",
    "params[\"epochs\"] = 2\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_num = train_x.shape[0]\n",
    "dataset = tf.data.Dataset.from_tensor_slices((train_x[:sample_num], train_y[:sample_num])).shuffle(params[\"batch_size\"]*2+1)\n",
    "dataset = dataset.batch(params[\"batch_size\"], drop_remainder=True)\n",
    "steps_per_epoch = sample_num//params[\"batch_size\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp, targ = next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([32, 460]), TensorShape([32, 52]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input shape (batch_size, enc_len)\n",
    "# targ shape (batch_size, dec_len)\n",
    "inp.shape, targ.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 进行encoder的运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seq2seq import Seq2Seq\n",
    "model = Seq2Seq(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([32, 460, 256]), TensorShape([32, 256]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_output, enc_hidden = model.call_encoder(inp)\n",
    "# enc_output shape (batch_size, enc_len, enc_unit) 所有时间步的输出\n",
    "# enc_hidden shape (batch_size, enc_unit) 最后一个时间步的输出\n",
    "enc_output.shape, enc_hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_hidden = enc_hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成第一个decoder的输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([32, 1])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dec_input shape (batch_size, 1) 一个词一个词输入decoder\n",
    "dec_input = tf.expand_dims([vocab['<START>']] * params[\"batch_size\"], 1)\n",
    "dec_input.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([32, 51, 32247]),\n",
       " TensorShape([32, 256]),\n",
       " 51,\n",
       " TensorShape([32, 460, 1]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predictions shape (batch_size, dec_len - 1, vocab_size)\n",
    "# dec_hidden shape (batch_size, dec_unit) same as enc_hidden\n",
    "# attentions 51个元素的列表 每个元素shape[32, 460, 1]\n",
    "predictions, dec_hidden, attentions = model(dec_input, dec_hidden, enc_output, targ)\n",
    "predictions.shape, dec_hidden.shape, len(attentions), attentions[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ps:模拟下model.call()关于注意力的操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=9541, shape=(), dtype=float32, numpy=10.380057>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_loss = loss_function(targ[:, 1:], predictions)\n",
    "batch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **测试下损失函数加深理解**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "real = targ[:, 1:]  # shape (32, 51) (batch_size, dec_len - 1)\n",
    "pred = predictions  # shape (32, 51, 32247) (batch_size, dec_len - 1, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=27830, shape=(32, 51), dtype=bool, numpy=\n",
       "array([[ True,  True,  True, ..., False, False, False],\n",
       "       [ True,  True,  True, ..., False, False, False],\n",
       "       [ True,  True,  True, ..., False, False, False],\n",
       "       ...,\n",
       "       [ True,  True,  True, ..., False, False, False],\n",
       "       [ True,  True,  True, ..., False, False, False],\n",
       "       [ True,  True,  True, ..., False, False, False]])>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_index=vocab['<PAD>']\n",
    "unk_index=vocab['<UNK>']\n",
    "pad_mask = tf.math.equal(real, pad_index)\n",
    "unk_mask = tf.math.equal(real, unk_index)\n",
    "# shape (32, 51) (batch_size, dec_len - 1)\n",
    "mask = tf.math.logical_not(tf.math.logical_or(pad_mask,unk_mask))\n",
    "mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=27863, shape=(32, 51), dtype=float32, numpy=\n",
       "array([[10.381944, 10.381646, 10.386802, ...,  0.      ,  0.      ,\n",
       "         0.      ],\n",
       "       [10.382557, 10.391157, 10.363414, ...,  0.      ,  0.      ,\n",
       "         0.      ],\n",
       "       [10.385499, 10.38046 , 10.399067, ...,  0.      ,  0.      ,\n",
       "         0.      ],\n",
       "       ...,\n",
       "       [10.366598, 10.37426 , 10.393599, ...,  0.      ,  0.      ,\n",
       "         0.      ],\n",
       "       [10.368627, 10.373091, 10.378149, ...,  0.      ,  0.      ,\n",
       "         0.      ],\n",
       "       [10.366828, 10.375762, 10.384366, ...,  0.      ,  0.      ,\n",
       "         0.      ]], dtype=float32)>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_ = loss_object(real, pred)  # shape (32, 51) (batch_size, dec_len - 1)\n",
    "mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "loss_ *= mask # 对应位置相乘\n",
    "loss_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 返回值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=9589, shape=(), dtype=float32, numpy=10.380057>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **加入coverage loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(name='Adam',learning_rate=0.001)\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "pad_index=vocab['<PAD>']\n",
    "unk_index=vocab['<UNK>']\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    pad_mask = tf.math.equal(real, pad_index)\n",
    "    unk_mask = tf.math.equal(real, unk_index)\n",
    "    # <PAD> 和 <UNK> 的损失都不算\n",
    "    mask = tf.math.logical_not(tf.math.logical_or(pad_mask,unk_mask))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    #return tf.reduce_mean(loss_)\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "\n",
    "def coverage_loss_function(real, pred, attn_dists):\n",
    "    # 先计算原本的损失\n",
    "    pad_mask = tf.math.equal(real, pad_index)\n",
    "    unk_mask = tf.math.equal(real, unk_index)\n",
    "    # <PAD> 和 <UNK> 的损失都不算\n",
    "    mask = tf.math.logical_not(tf.math.logical_or(pad_mask,unk_mask))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "        \n",
    "    coverage = tf.zeros_like(attn_dists[0]) # shape (batch_size, attn_length). Initial coverage is zero.\n",
    "    #covlosses = [] # Coverage loss per decoder timestep. Will be list length max_dec_steps containing shape (batch_size).\n",
    "    covlosses = []\n",
    "    for i,a in enumerate(attn_dists):\n",
    "        covloss = tf.reduce_sum(tf.minimum(a, coverage), [1]) # calculate the coverage loss for this step\n",
    "        covlosses.append(covloss)\n",
    "        coverage += a # update the coverage vector\n",
    "    # coverage_loss = _mask_and_avg(covlosses)\n",
    "    \n",
    "    coverage_loss = 0\n",
    "    for i,_ in enumerate(covlosses):\n",
    "        covlosses[i] = covlosses[i] * tf.expand_dims(mask[:, i],1)\n",
    "        coverage_loss += tf.reduce_sum(covlosses[i])\n",
    "\n",
    "    # print(\"coverage loss\", (coverage_loss/ tf.reduce_sum(mask)).numpy())\n",
    "    \n",
    "    return (tf.reduce_sum(loss_)+coverage_loss) / tf.reduce_sum(mask)\n",
    "\n",
    "@tf.function\n",
    "def _coverage_loss(attn_dists):\n",
    "    \"\"\"\n",
    "    Calculates the coverage loss from the attention distributions.\n",
    "    \n",
    "    attn_dists shape (enc_len, batch_size, dec_len)\n",
    "    padding_mask 掩码操作\n",
    "\n",
    "    return: coverage_loss shape \n",
    "    \"\"\"\n",
    "    coverage = tf.zeros_like(attn_dists[0]) # shape (batch_size, attn_length). Initial coverage is zero.\n",
    "    #covlosses = [] # Coverage loss per decoder timestep. Will be list length max_dec_steps containing shape (batch_size).\n",
    "    covlosses = []\n",
    "    for i,a in enumerate(attn_dists):\n",
    "        covloss = tf.reduce_sum(tf.minimum(a, coverage), [1]) # calculate the coverage loss for this step\n",
    "        covlosses.append(covloss)\n",
    "        coverage += a # update the coverage vector\n",
    "    # coverage_loss = _mask_and_avg(covlosses)\n",
    "    \n",
    "    coverage_loss = 0\n",
    "    for i,_ in enumerate(covlosses):\n",
    "        covlosses[i] = covlosses[i] * tf.expand_dims(mask[:, i],1)\n",
    "        coverage_loss += tf.reduce_sum(covlosses[i])\n",
    "    \n",
    "    return coverage_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=28198, shape=(), dtype=float32, numpy=10.381315>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_function(targ[:,1:], predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=28963, shape=(), dtype=float32, numpy=11.344197>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coverage_loss_function(targ[:,1:], predictions, attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_dists = attentions\n",
    "coverage = tf.zeros_like(attn_dists[0])\n",
    "# coverage_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "covlosses = []\n",
    "for i,a in enumerate(attn_dists):\n",
    "    covloss = tf.reduce_sum(tf.minimum(a, coverage), [1]) # calculate the coverage loss for this step\n",
    "    covlosses.append(covloss)\n",
    "    coverage += a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "covlosses[0] = covlosses[0] * tf.expand_dims(mask[:, 0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始loss 10.380057\n",
      "coverage loss 0.9570429\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=10516, shape=(), dtype=float32, numpy=11.3371>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 460) (32, 460)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=10952, shape=(32,), dtype=float32, numpy=\n",
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coverage = tf.zeros_like(attentions[0])\n",
    "a = attentions[0]\n",
    "covloss = tf.reduce_sum(tf.minimum(a, coverage), [1]) \n",
    "print(a.shape, coverage.shape)\n",
    "covloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "covlosses = []\n",
    "for a in attentions:\n",
    "    covloss = tf.reduce_sum(tf.minimum(a, coverage), [1]) # calculate the coverage loss for this step\n",
    "    covlosses.append(covloss)\n",
    "    coverage += a # update the coverage vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i,loss in enumerate(covlosses):\n",
    "#     covlosses[i] = covlosses[i] + 1\n",
    "#     #print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = tf.constant([1,2,3,4,5,6,7,8], shape=(4,2,1))\n",
    "tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reshape(tt, tt.shape[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt[0] *= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'accumulate_n'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-8d8071a642f2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhelp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccumulate_n\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'accumulate_n'"
     ]
    }
   ],
   "source": [
    "help(tf.accumulate_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.params import get_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "%run tt.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mode': 'train',\n",
       " 'max_enc_len': 200,\n",
       " 'max_dec_len': 41,\n",
       " 'batch_size': 64,\n",
       " 'epochs': 10,\n",
       " 'vocab_path': 'E:\\\\GitHub\\\\QA-abstract-and-reasoning\\\\data\\\\wv\\\\vocab_index_pad.txt',\n",
       " 'learning_rate': 100000.0,\n",
       " 'adagrad_init_acc': 0.1,\n",
       " 'max_grad_norm': 0.8,\n",
       " 'vocab_size': 31820,\n",
       " 'beam_size': 3,\n",
       " 'embed_size': 500,\n",
       " 'enc_units': 512,\n",
       " 'dec_units': 512,\n",
       " 'attn_units': 256,\n",
       " 'train_seg_x_dir': 'E:\\\\GitHub\\\\QA-abstract-and-reasoning\\\\data\\\\train_x.npy',\n",
       " 'train_seg_y_dir': 'E:\\\\GitHub\\\\QA-abstract-and-reasoning\\\\data\\\\train_y.npy',\n",
       " 'test_seg_x_dir': 'E:\\\\GitHub\\\\QA-abstract-and-reasoning\\\\data\\\\test_x.npy',\n",
       " 'checkpoints_save_steps': 5,\n",
       " 'min_dec_steps': 4,\n",
       " 'max_train_steps': 1250,\n",
       " 'train_pickle_dir': '/opt/kaikeba/dataset/',\n",
       " 'save_batch_train_data': False,\n",
       " 'load_batch_train_data': False}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf2.0]",
   "language": "python",
   "name": "conda-env-tf2.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
