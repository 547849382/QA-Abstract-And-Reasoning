{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\software\\Anaconda3\\envs\\tf2.0\\lib\\site-packages\\ipykernel\\parentpoller.py:116: UserWarning: Parent poll failed.  If the frontend dies,\n",
      "                the kernel may be left running.  Please let us know\n",
      "                about your system (bitness, Python, etc.) at\n",
      "                ipython-dev@scipy.org\n",
      "  ipython-dev@scipy.org\"\"\")\n"
     ]
    }
   ],
   "source": [
    "# 自动加载修改过的文件\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "sys.path.append('../')  # |返回notebook的上一级目录\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "from utils.config_gpu import config_gpu\n",
    "config_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.config import *\n",
    "from utils.saveLoader import load_text, get_text, load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_seg, test_seg = load_dataset(TRAIN_SEG, TEST_SEG)\n",
    "# proc_text = load_text(PROC_TEXT)\n",
    "# raw_text = load_text(RAW_TEXT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1.损失函数的改进**\n",
    "- 原本return tf.reduce_mean(loss_)改成tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "- 改成了coverage loss\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.config import *\n",
    "from utils.saveLoader import load_train_dataset, load_vocab\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 载入数据\n",
    "train_x,train_y,test_x = load_train_dataset()  # 数据集\n",
    "vocab,vocab_reversed = load_vocab(VOCAB_PAD)  # vocab\n",
    "embedding_matrix = np.loadtxt(EMBEDDING_MATRIX_PAD)  # 预训练层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.1原损失函数**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(name='Adam',learning_rate=0.001)\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "pad_index=vocab['<PAD>']\n",
    "unk_index=vocab['<UNK>']\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    pad_mask = tf.math.equal(real, pad_index)\n",
    "    unk_mask = tf.math.equal(real, unk_index)\n",
    "    # <PAD> 和 <UNK> 的损失都不算\n",
    "    mask = tf.math.logical_not(tf.math.logical_or(pad_mask,unk_mask))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    #return tf.reduce_mean(loss_)\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.2模型测试**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'vocab_size': 32247, 'max_enc_len': 460, 'max_dec_len': 52, 'embed_size': 300, 'enc_units': 256, 'attn_units': 10, 'dec_units': 256, 'batch_size': 32, 'epochs': 2}\n"
     ]
    }
   ],
   "source": [
    "params = {}\n",
    "params[\"vocab_size\"] = len(vocab)\n",
    "params[\"max_enc_len\"] = train_x.shape[1]  # 260\n",
    "params[\"max_dec_len\"] = train_y.shape[1]  # 33\n",
    "params[\"embed_size\"] = embedding_matrix.shape[1]\n",
    "params[\"enc_units\"] = 256\n",
    "params[\"attn_units\"] = 10\n",
    "params[\"dec_units\"] = params[\"enc_units\"]\n",
    "params[\"batch_size\"] = 32\n",
    "params[\"epochs\"] = 2\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_num = train_x.shape[0]\n",
    "dataset = tf.data.Dataset.from_tensor_slices((train_x[:sample_num], train_y[:sample_num])).shuffle(params[\"batch_size\"]*2+1)\n",
    "dataset = dataset.batch(params[\"batch_size\"], drop_remainder=True)\n",
    "steps_per_epoch = sample_num//params[\"batch_size\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp, targ = next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([32, 460]), TensorShape([32, 52]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input shape (batch_size, enc_len)\n",
    "# targ shape (batch_size, dec_len)\n",
    "inp.shape, targ.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 进行encoder的运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seq2seq import Seq2Seq\n",
    "model = Seq2Seq(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([32, 460, 256]), TensorShape([32, 256]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_output, enc_hidden = model.call_encoder(inp)\n",
    "# enc_output shape (batch_size, enc_len, enc_unit) 所有时间步的输出\n",
    "# enc_hidden shape (batch_size, enc_unit) 最后一个时间步的输出\n",
    "enc_output.shape, enc_hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_hidden = enc_hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成第一个decoder的输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([32, 1])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dec_input shape (batch_size, 1) 一个词一个词输入decoder\n",
    "dec_input = tf.expand_dims([vocab['<START>']] * params[\"batch_size\"], 1)\n",
    "dec_input.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([32, 51, 32247]),\n",
       " TensorShape([32, 256]),\n",
       " 51,\n",
       " TensorShape([32, 460, 1]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predictions shape (batch_size, dec_len - 1, vocab_size)\n",
    "# dec_hidden shape (batch_size, dec_unit) same as enc_hidden\n",
    "# attentions 51个元素的列表 每个元素shape[32, 460, 1]\n",
    "predictions, dec_hidden, attentions = model(dec_input, dec_hidden, enc_output, targ)\n",
    "predictions.shape, dec_hidden.shape, len(attentions), attentions[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ps:模拟下model.call()关于注意力的操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=9541, shape=(), dtype=float32, numpy=10.380057>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_loss = loss_function(targ[:, 1:], predictions)\n",
    "batch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **测试下损失函数加深理解**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "real = targ[:, 1:]  # shape (32, 51) (batch_size, dec_len - 1)\n",
    "pred = predictions  # shape (32, 51, 32247) (batch_size, dec_len - 1, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=27830, shape=(32, 51), dtype=bool, numpy=\n",
       "array([[ True,  True,  True, ..., False, False, False],\n",
       "       [ True,  True,  True, ..., False, False, False],\n",
       "       [ True,  True,  True, ..., False, False, False],\n",
       "       ...,\n",
       "       [ True,  True,  True, ..., False, False, False],\n",
       "       [ True,  True,  True, ..., False, False, False],\n",
       "       [ True,  True,  True, ..., False, False, False]])>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_index=vocab['<PAD>']\n",
    "unk_index=vocab['<UNK>']\n",
    "pad_mask = tf.math.equal(real, pad_index)\n",
    "unk_mask = tf.math.equal(real, unk_index)\n",
    "# shape (32, 51) (batch_size, dec_len - 1)\n",
    "mask = tf.math.logical_not(tf.math.logical_or(pad_mask,unk_mask))\n",
    "mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=27863, shape=(32, 51), dtype=float32, numpy=\n",
       "array([[10.381944, 10.381646, 10.386802, ...,  0.      ,  0.      ,\n",
       "         0.      ],\n",
       "       [10.382557, 10.391157, 10.363414, ...,  0.      ,  0.      ,\n",
       "         0.      ],\n",
       "       [10.385499, 10.38046 , 10.399067, ...,  0.      ,  0.      ,\n",
       "         0.      ],\n",
       "       ...,\n",
       "       [10.366598, 10.37426 , 10.393599, ...,  0.      ,  0.      ,\n",
       "         0.      ],\n",
       "       [10.368627, 10.373091, 10.378149, ...,  0.      ,  0.      ,\n",
       "         0.      ],\n",
       "       [10.366828, 10.375762, 10.384366, ...,  0.      ,  0.      ,\n",
       "         0.      ]], dtype=float32)>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_ = loss_object(real, pred)  # shape (32, 51) (batch_size, dec_len - 1)\n",
    "mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "loss_ *= mask # 对应位置相乘\n",
    "loss_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 返回值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=9589, shape=(), dtype=float32, numpy=10.380057>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **加入coverage loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(name='Adam',learning_rate=0.001)\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "pad_index=vocab['<PAD>']\n",
    "unk_index=vocab['<UNK>']\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    pad_mask = tf.math.equal(real, pad_index)\n",
    "    unk_mask = tf.math.equal(real, unk_index)\n",
    "    # <PAD> 和 <UNK> 的损失都不算\n",
    "    mask = tf.math.logical_not(tf.math.logical_or(pad_mask,unk_mask))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    #return tf.reduce_mean(loss_)\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "\n",
    "def coverage_loss_function(real, pred, attn_dists):\n",
    "    # 先计算原本的损失\n",
    "    pad_mask = tf.math.equal(real, pad_index)\n",
    "    unk_mask = tf.math.equal(real, unk_index)\n",
    "    # <PAD> 和 <UNK> 的损失都不算\n",
    "    mask = tf.math.logical_not(tf.math.logical_or(pad_mask,unk_mask))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "        \n",
    "    coverage = tf.zeros_like(attn_dists[0]) # shape (batch_size, attn_length). Initial coverage is zero.\n",
    "    #covlosses = [] # Coverage loss per decoder timestep. Will be list length max_dec_steps containing shape (batch_size).\n",
    "    covlosses = []\n",
    "    for i,a in enumerate(attn_dists):\n",
    "        covloss = tf.reduce_sum(tf.minimum(a, coverage), [1]) # calculate the coverage loss for this step\n",
    "        covlosses.append(covloss)\n",
    "        coverage += a # update the coverage vector\n",
    "    # coverage_loss = _mask_and_avg(covlosses)\n",
    "    \n",
    "    coverage_loss = 0\n",
    "    for i,_ in enumerate(covlosses):\n",
    "        covlosses[i] = covlosses[i] * tf.expand_dims(mask[:, i],1)\n",
    "        coverage_loss += tf.reduce_sum(covlosses[i])\n",
    "\n",
    "    # print(\"coverage loss\", (coverage_loss/ tf.reduce_sum(mask)).numpy())\n",
    "    \n",
    "    return (tf.reduce_sum(loss_)+coverage_loss) / tf.reduce_sum(mask)\n",
    "\n",
    "@tf.function\n",
    "def _coverage_loss(attn_dists):\n",
    "    \"\"\"\n",
    "    Calculates the coverage loss from the attention distributions.\n",
    "    \n",
    "    attn_dists shape (enc_len, batch_size, dec_len)\n",
    "    padding_mask 掩码操作\n",
    "\n",
    "    return: coverage_loss shape \n",
    "    \"\"\"\n",
    "    coverage = tf.zeros_like(attn_dists[0]) # shape (batch_size, attn_length). Initial coverage is zero.\n",
    "    #covlosses = [] # Coverage loss per decoder timestep. Will be list length max_dec_steps containing shape (batch_size).\n",
    "    covlosses = []\n",
    "    for i,a in enumerate(attn_dists):\n",
    "        covloss = tf.reduce_sum(tf.minimum(a, coverage), [1]) # calculate the coverage loss for this step\n",
    "        covlosses.append(covloss)\n",
    "        coverage += a # update the coverage vector\n",
    "    # coverage_loss = _mask_and_avg(covlosses)\n",
    "    \n",
    "    coverage_loss = 0\n",
    "    for i,_ in enumerate(covlosses):\n",
    "        covlosses[i] = covlosses[i] * tf.expand_dims(mask[:, i],1)\n",
    "        coverage_loss += tf.reduce_sum(covlosses[i])\n",
    "    \n",
    "    return coverage_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=28198, shape=(), dtype=float32, numpy=10.381315>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_function(targ[:,1:], predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=28963, shape=(), dtype=float32, numpy=11.344197>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coverage_loss_function(targ[:,1:], predictions, attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_dists = attentions\n",
    "coverage = tf.zeros_like(attn_dists[0])\n",
    "# coverage_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "covlosses = []\n",
    "for i,a in enumerate(attn_dists):\n",
    "    covloss = tf.reduce_sum(tf.minimum(a, coverage), [1]) # calculate the coverage loss for this step\n",
    "    covlosses.append(covloss)\n",
    "    coverage += a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "covlosses[0] = covlosses[0] * tf.expand_dims(mask[:, 0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始loss 10.380057\n",
      "coverage loss 0.9570429\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=10516, shape=(), dtype=float32, numpy=11.3371>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 460) (32, 460)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=10952, shape=(32,), dtype=float32, numpy=\n",
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coverage = tf.zeros_like(attentions[0])\n",
    "a = attentions[0]\n",
    "covloss = tf.reduce_sum(tf.minimum(a, coverage), [1]) \n",
    "print(a.shape, coverage.shape)\n",
    "covloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "covlosses = []\n",
    "for a in attentions:\n",
    "    covloss = tf.reduce_sum(tf.minimum(a, coverage), [1]) # calculate the coverage loss for this step\n",
    "    covlosses.append(covloss)\n",
    "    coverage += a # update the coverage vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i,loss in enumerate(covlosses):\n",
    "#     covlosses[i] = covlosses[i] + 1\n",
    "#     #print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = tf.constant([1,2,3,4,5,6,7,8], shape=(4,2,1))\n",
    "tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reshape(tt, tt.shape[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt[0] *= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'accumulate_n'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-8d8071a642f2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhelp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccumulate_n\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'accumulate_n'"
     ]
    }
   ],
   "source": [
    "help(tf.accumulate_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.params import get_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--mode MODE] [--max_enc_len MAX_ENC_LEN]\n",
      "                             [--max_dec_len MAX_DEC_LEN]\n",
      "                             [--batch_size BATCH_SIZE] [--epochs EPOCHS]\n",
      "                             [--vocab_path VOCAB_PATH]\n",
      "                             [--learning_rate LEARNING_RATE]\n",
      "                             [--adagrad_init_acc ADAGRAD_INIT_ACC]\n",
      "                             [--max_grad_norm MAX_GRAD_NORM]\n",
      "                             [--vocab_size VOCAB_SIZE] [--beam_size BEAM_SIZE]\n",
      "                             [--embed_size EMBED_SIZE] [--enc_units ENC_UNITS]\n",
      "                             [--dec_units DEC_UNITS] [--attn_units ATTN_UNITS]\n",
      "                             [--train_seg_x_dir TRAIN_SEG_X_DIR]\n",
      "                             [--train_seg_y_dir TRAIN_SEG_Y_DIR]\n",
      "                             [--test_seg_x_dir TEST_SEG_X_DIR]\n",
      "                             [--checkpoints_save_steps CHECKPOINTS_SAVE_STEPS]\n",
      "                             [--min_dec_steps MIN_DEC_STEPS]\n",
      "                             [--max_train_steps MAX_TRAIN_STEPS]\n",
      "                             [--train_pickle_dir TRAIN_PICKLE_DIR]\n",
      "                             [--save_batch_train_data SAVE_BATCH_TRAIN_DATA]\n",
      "                             [--load_batch_train_data LOAD_BATCH_TRAIN_DATA]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\Light\\AppData\\Roaming\\jupyter\\runtime\\kernel-357e3af2-5252-4dfb-b8a1-9a61ead9462c.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "from utils.params import get_params\n",
    "params = get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "%run tt.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-8071c7fa4824>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mE:\\GitHub\\QA-abstract-and-reasoning\\utils\\params.py\u001b[0m in \u001b[0;36mget_params\u001b[1;34m()\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_argument\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"--load_batch_train_data\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhelp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"load batch train data from pickle\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m     \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m     \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvars\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\software\\Anaconda3\\envs\\tf2.0\\lib\\argparse.py\u001b[0m in \u001b[0;36mparse_args\u001b[1;34m(self, args, namespace)\u001b[0m\n\u001b[0;32m   1750\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0margv\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1751\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'unrecognized arguments: %s'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1752\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1753\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1754\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\software\\Anaconda3\\envs\\tf2.0\\lib\\argparse.py\u001b[0m in \u001b[0;36merror\u001b[1;34m(self, message)\u001b[0m\n\u001b[0;32m   2499\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_usage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_sys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2500\u001b[0m         \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'prog'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprog\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'message'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2501\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'%(prog)s: error: %(message)s\\n'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mE:\\software\\Anaconda3\\envs\\tf2.0\\lib\\argparse.py\u001b[0m in \u001b[0;36mexit\u001b[1;34m(self, status, message)\u001b[0m\n\u001b[0;32m   2486\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2487\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_print_message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_sys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2488\u001b[1;33m         \u001b[0m_sys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2489\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2490\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mSystemExit\u001b[0m: 2"
     ]
    }
   ],
   "source": [
    "%tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'type'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-f19383e873bf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'dict' object has no attribute 'type'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'params' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-bfd368200e6b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mparams\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'params' is not defined"
     ]
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mode': 'train',\n",
       " 'max_enc_len': 200,\n",
       " 'max_dec_len': 41,\n",
       " 'batch_size': 64,\n",
       " 'epochs': 10,\n",
       " 'vocab_path': 'E:\\\\GitHub\\\\QA-abstract-and-reasoning\\\\data\\\\wv\\\\vocab_index_pad.txt',\n",
       " 'learning_rate': 100000.0,\n",
       " 'adagrad_init_acc': 0.1,\n",
       " 'max_grad_norm': 0.8,\n",
       " 'vocab_size': 31820,\n",
       " 'beam_size': 3,\n",
       " 'embed_size': 500,\n",
       " 'enc_units': 512,\n",
       " 'dec_units': 512,\n",
       " 'attn_units': 256,\n",
       " 'train_seg_x_dir': 'E:\\\\GitHub\\\\QA-abstract-and-reasoning\\\\data\\\\train_x.npy',\n",
       " 'train_seg_y_dir': 'E:\\\\GitHub\\\\QA-abstract-and-reasoning\\\\data\\\\train_y.npy',\n",
       " 'test_seg_x_dir': 'E:\\\\GitHub\\\\QA-abstract-and-reasoning\\\\data\\\\test_x.npy',\n",
       " 'checkpoints_save_steps': 5,\n",
       " 'min_dec_steps': 4,\n",
       " 'max_train_steps': 1250,\n",
       " 'train_pickle_dir': '/opt/kaikeba/dataset/',\n",
       " 'save_batch_train_data': False,\n",
       " 'load_batch_train_data': False}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.config import *\n",
    "from utils.saveLoader import load_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "train_x = np.savetxt(TRAIN_X)\n",
    "train_y = np.savetxt(TRAIN_Y)\n",
    "test_x = np.savetxt(TEST_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(TRAIN_X, train_x, fmt=\"%d\", delimiter=\",\")\n",
    "np.savetxt(TRAIN_Y, train_y, fmt=\"%d\", delimiter=\",\")\n",
    "np.savetxt(TEST_X, test_x, fmt=\"%d\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '32243,510,1053,0,338,316,0,510,20,28,3,42,118,149,1,20,28,3,338,316,0,510,28,3,510,574,1,6,6,338,29,1053,0,622,46,183,47,11,21,12,62,4,30,1,697,6,10,96,562,3,1825,271,0,9,490,1053,1,102,2,4,752,1053,14,4,1,0,26375,7,523,17,4,0,772,413,4,292,39,458,1053,0,1933,47,4969,316,28,3,169,0,132,51,475,685,5459,3,0,56,90,98,9,11,123,1113,34,413,2230,2,32244,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-7ea3c2d68a2a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_train_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mE:\\GitHub\\QA-abstract-and-reasoning\\utils\\saveLoader.py\u001b[0m in \u001b[0;36mload_train_dataset\u001b[1;34m()\u001b[0m\n\u001b[0;32m    156\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[1;32mreturn\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0m加载处理好的数据集\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m     \"\"\"\n\u001b[1;32m--> 158\u001b[1;33m     \u001b[0mtrain_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTRAIN_X\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    159\u001b[0m     \u001b[0mtrain_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTRAIN_Y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m     \u001b[0mtest_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTEST_X\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\software\\Anaconda3\\envs\\tf2.0\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mloadtxt\u001b[1;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows)\u001b[0m\n\u001b[0;32m   1144\u001b[0m         \u001b[1;31m# converting the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1145\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1146\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mread_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_loadtxt_chunksize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1147\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1148\u001b[0m                 \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\software\\Anaconda3\\envs\\tf2.0\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mread_data\u001b[1;34m(chunk_size)\u001b[0m\n\u001b[0;32m   1072\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1073\u001b[0m             \u001b[1;31m# Convert each value according to its column and store\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1074\u001b[1;33m             \u001b[0mitems\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mconv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mconv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconverters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1075\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1076\u001b[0m             \u001b[1;31m# Then pack it according to the dtype's nesting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\software\\Anaconda3\\envs\\tf2.0\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1072\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1073\u001b[0m             \u001b[1;31m# Convert each value according to its column and store\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1074\u001b[1;33m             \u001b[0mitems\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mconv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mconv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconverters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1075\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1076\u001b[0m             \u001b[1;31m# Then pack it according to the dtype's nesting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\software\\Anaconda3\\envs\\tf2.0\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mfloatconv\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'0x'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfromhex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 781\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    782\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    783\u001b[0m     \u001b[0mtyp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: '32243,510,1053,0,338,316,0,510,20,28,3,42,118,149,1,20,28,3,338,316,0,510,28,3,510,574,1,6,6,338,29,1053,0,622,46,183,47,11,21,12,62,4,30,1,697,6,10,96,562,3,1825,271,0,9,490,1053,1,102,2,4,752,1053,14,4,1,0,26375,7,523,17,4,0,772,413,4,292,39,458,1053,0,1933,47,4969,316,28,3,169,0,132,51,475,685,5459,3,0,56,90,98,9,11,123,1113,34,413,2230,2,32244,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245,32245'"
     ]
    }
   ],
   "source": [
    "x,y,z = load_train_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[32243,   411,   483, ..., 32245, 32245, 32245],\n",
       "       [32243,   411,   483, ..., 32245, 32245, 32245],\n",
       "       [32243,   122,    15, ..., 32245, 32245, 32245],\n",
       "       ...,\n",
       "       [32243,    55,    36, ..., 32245, 32245, 32245],\n",
       "       [32243,    55,    36, ..., 32245, 32245, 32245],\n",
       "       [32243,    55,    36, ..., 32245, 32245, 32245]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf2.0]",
   "language": "python",
   "name": "conda-env-tf2.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
